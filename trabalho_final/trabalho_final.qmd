---
title: "Trabalho Final"
author: "Roberto Jorge Dummar Filho (232103587)"
prefer-html: true
format: html
editor: visual
---

# PPCA/UnB : Métodos Computacionais Intensivos em Mineração de Dados

## Professor: Donald Matthew Pianto

## Introdução

O presente trabalho final buscará aplicar o Latent Dirichlet Allocation (LDA), um modelo estatístico generativo para coleções de dados discretos tais como corpora de texto \[1\], na base de dados de perguntas de acesso à informação inquiridas ao Senado Federal, entre janeiro de 2022 e junho de 2024.

## Lei de Acesso à Informação

A Lei nº 12.527, de 2011 (Lei de Acesso à Informação - LAI) \[2\] regulamentou o direito de acesso à informação previsto na Constituição Federal, vinculando as entidades públicas e as entidades privadas que, de forma geral, recebem recursos públicos, a uma gestão transparente e a proteção da informação pessoal.

A partir dela, são realizados pelos órgãos públicos dois tipos de transparência: a transparência ativa, na qual disponibilizam informações de interesse geral em seus sítios de internet, tais como a agenda de autoridades e a remuneração de seus servidores; e a transparência passiva, na qual recebem, tratam e respondem a pedidos de acesso à informação solicitados por qualquer pessoal.

Ainda, os órgãos públicos de cada Poder publicam em seus sítios de internet relatórios estatísticos acerca das demandas recebidas e do perfil dos solicitantes, bem com a relação de perguntas e respostas, e o estado do seu atendimento (ex: respondido, negativo, respondido parcialmente etc).

## Fonte dos Dados

O Senado Federal \[3\] publica mensalmente relatórios estatísticos dos pedidos e dos assuntos tratados, bem como uma tabela com as perguntas e o estado do seu atendimento, sem incluir o texto das respostas fornecidas.

Foram obtidos os relatórios em formato Office Open XML/Open Document e realizadas as seguintes operações para tratamento dos dados:

1.  Remoção de quebras de linha e marcações de parágrafos desnecessários.
2.  Cada tabela foi salva em uma aba da planilha, sendo incluído o mês/ano da informação.

## Latent Dirichlet Allocation (LDA)

O Latent Dirichlet Allocation (LDA) é um modelo generativo probabilístico, sendo especificamente um modelo bayesiano hierárquico de três níveis, para uma coleção de dados discretos (como um corpora de texto).

Ele é uma técnica amplamente adotada para ajustar modelos de tópicos. Opera tratando cada documento como uma mistura de tópicos, e cada tópico como uma mistura de palavras. Consequentemente, os documentos podem exibir sobreposições de conteúdo, semelhantes à fluidez observada no uso da linguagem natural, em vez de serem estritamente segregados em grupos distintos.

![](images/clipboard-379997878.png)

Pelo diagrama acima, temos que:

-   $\alpha$ e $\beta$ são distribuições *a priori* de Dirichlet.

    -   $\alpha$ é um parâmetro da distribuição de probabilidades de tópicos presentes em documentos.

        $$f(x_1, x_2, \ldots, x_k; \alpha_1, \alpha_2, \ldots, \alpha_k) = \frac{1}{B(\alpha)} \prod{i=1}^k x_i^{\alpha_i - 1}$$

        onde

        $$B(\alpha) = \frac{\prod_{i=1}^k \Gamma(\alpha_i)}{\Gamma\left(\sum_{i=1}^k \alpha_i\right)}$$

        sendo $\Gamma$ a função Gamma.

        -   Para um valor de $\alpha$ maior do que 1, a maioria dos documentos conterá uma mistura de quase todos os tópicos, com proporções relativamente equilibradas entre eles.
        -   Para um valor de $\alpha$ igual a 1, haverá uma mistura média de tópicos nos documentos.
        -   Para um valor de $\alpha$ menor do que 1, cada documento será dominado por poucos tópicos ou até um tópico, em casos extremos.

    -   $\beta$ é um parâmetro da distribuição de probabilidades de palavras presentes em tópicos.

$\theta$ e $\phi$ são distribuições multinomiais.

-   $\theta_m$ é a distribuição de tópicos para cada documento $m$ dentro dos $M$ documentos, a partir do $\alpha$ inicial.

-   $\phi_k$ é a distribuição de palavras para cada tópico $k$, dentre os vários tópicos $K$, a partir do $\beta$ inicial.

$z_{mn}$ é a atribuição de tópicos para a $n-ésima$ palavra do document $m$, obtida a partir da distribuição de $\theta$.

Ao combinarmos a lista de tópicos presentes em $z_{mn}$ com a distribuição de palavras para cada tópico presente em $\phi_k$ , obtemos $w_{mn}$, que representa a $n-ésima$ palavra observada para cada documento $m$.

O conjunto de palavras observadas $w$ com os tópicos associados $z$ representam o total de palavras em um documento. Adicionando-se a isso a distribuição de tópicos presentes em $\theta$, obtemos os documentos presentes no corpus.

## Gibbs Sampling

Gibbs sampling é uma técnica de amostragem dentro do grupo de algoritmos de Métodos de Monte Carlo por meio da cadeia de Markov, no qual são realizadas diversas interações.

Suponha que você deseja amostrar de uma distribuição conjunta $(P(X_1, X_2, \ldots, X_n))$, onde $(X_1, X_2, \ldots, X_n$ são variáveis aleatórias. O Gibbs sampling funciona da seguinte maneira:

1.  Inicialização: Começa-se com um valor inicial para cada uma das variáveis $(X_1, X_2, \ldots, X_n)$, digamos $(X_1^{(0)}, X_2^{(0)}, \ldots, X_n^{(0)})$.
2.  Amostragem Condicional: Em cada iteração do algoritmo, cada variável é amostrada condicionalmente às outras variáveis. Isso significa que, para a $(k)-ésima$ variável $(X_k)$, amostra-se de sua distribuição condicional, dada as outras variáveis fixadas em seus valores mais recentes:

$$
X_k^{(t+1)} \sim P(X_k \mid X_1^{(t+1)}, X_2^{(t+1)}, \ldots, X_{k-1}^{(t+1)}, X_{k+1}^{(t)}, \ldots, X_n^{(t)})
$$

3.  Repetição: O processo continua iterativamente, alternando entre as variáveis, até que a cadeia de Markov converja para a distribuição estacionária desejada.
4.  Coleta de Amostras: Após um número suficiente de iterações (geralmente depois de descartar um número inicial de amostras como "burn-in"), as amostras subsequentes podem ser consideradas como provenientes da distribuição conjunta original.

No contexto da LDA, a Gibbs sampling funciona com dois princípios:

1.  Os documentos tem que ser os mais monocromáticos possíveis, isto é, tem que pertencer a menor quantidade de tópicos possíveis.
2.  As palavras tem que ser as mais monocromáticas possíveis, isto é, tem que pertencer a menor quantidade de de tópicos possíveis.

## Execução

```{r carregarbibliotecas, warning=FALSE, message=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
library(dplyr)
library(flextable)
library(ggplot2)
library(lda)
library(ldatuning)
#install.packages("microbenchmark")
library(microbenchmark)
library(quanteda)
library(RColorBrewer)
#install.packages("readxl")
library(readxl)
library(reshape2)
library(slam)
library(stringr)
#install.packages("textstem")
library(textstem)
library(tidyr)
library(tidytext)
library(tm)
library(topicmodels)
library(wordcloud)
#install.packages("writexl")
library(writexl)
# activate klippy for copy-to-clipboard button
#klippy::klippy()
#install.packages("remotes")
#remotes::install_github("rlesur/klippy")
```

A partir dos dados consolidados de pedidos de acesso à informação de 2022 a junho de 2024, foram realizados os seguintes procedimentos:

1.  Texto foi convertido para letras minúsculas.
2.  Como a parte principal de uma pergunta começa após alguns verbos característicos (ex: solicito, requeiro etc), foi realizada a remoção da parte anterior.
3.  Foram removidas sinais específicos (ex: aspas duplas, aspas curvas, nº).
4.  Foram removidas algunas palavras de encerramento de e-mail (ex: por favor, atenciosamente), de marcação de imprensa (sic)).
5.  Foram removidas palavras com 3 ou mais letras repetidas, por se tratarem de ocultação de informações pessoais.

Ao usar o pacote `quanteda`, foram removidas:

1.  Pontuações, símbolos e números.
2.  "Stopwords" em português.
3.  Realizar o stemming das palavras

```{r carregardados}
dir_bases <- paste0(getwd(), "/bases/")
caminho_arquivo <- paste0(dir_bases, "dados_consolidados.xlsx")

dados <- readxl::read_excel(caminho_arquivo)

salvar_df_excel <- function(dados, nm_arquivo) {
  writexl::write_xlsx(dados, paste0(dir_bases, nm_arquivo))
}

processar_texto <- function(texto, padrao) {
  match <- stringr::str_match(texto, padrao)
  if (!is.na(match[1])) {
    partes_texto <- stringr::str_split_fixed(texto, match[1], n = 2)
    partes_texto <- stringr::str_trim(partes_texto)
    resultado <- partes_texto[2]
  } else {
    resultado <- ""
  }
  return(resultado)
}

padroes <- list(
  "\\b[Ss]olicit\\w*\\b",  # Captura palavras que começam com "solicit"
  "\\b[Oo]bter\\b",        # Captura a palavra "obter"
  "\\b[Rr]equei\\w*\\b",     # Captura palavras que começam com "reque"
  "\\b[Ss]aber\\b",        # Captura a palavra "saber"
  "\\b[Dd]isponibilizar\\b", # Captura a palavra "disponibilizar"
  "\\b[Rr]eceb\\w*\\b",       # Captura a palavra "receber"
  "\\b[Gg]ostaria\\s+de\\s+\\w+\\b", # Captura "gostaria de" seguido de uma palavra
  "\\b[Gg]ostaria\\s+que\\b", # Captura a expressão "gostaria de"
  "\\b[Nn]ecessito\\s+de\\b", # Captura a expressão "necessito de"
  "\\b[Nn]ecessito\\s+da\\b", # Captura a expressão "necessito da"
  "\\b[Nn]ecessito\\s+do\\b" # Captura a expressão "necessito do"
)

# Remoção dos dois tipos de aspas duplas
dados <- dados |> 
  dplyr::mutate(text = tolower(text)) |> 
  dplyr::mutate(text = gsub('"', '', text)) |> 
  dplyr::mutate(text = gsub('[\u201C\u201D"]', '', text)) |> 
  dplyr::mutate(text = gsub('/', ' ', text)) |> 
  dplyr::mutate(text = gsub("\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b", "", text))
         

dados <- dados |> 
  dplyr::mutate(text2 = "")

# Remoção da parte anterior da pergunta
for (padrao in padroes) {
  linhas_vazias <- which(dados$text2 == "")
  dados$text2[linhas_vazias] <- sapply(dados$text[linhas_vazias], processar_texto, padrao = padrao)
}

dados <- dados |> 
  dplyr::mutate(text2 = dplyr::if_else(text2 == "", text, text2))

# Remover a substring "nº" da coluna 'text2'
dados$text2 <- gsub("nº", "", dados$text2)

# remover expressões de polidez
dados$text2 <- gsub("lei de acesso à informação", "", dados$text2)
dados$text2 <- gsub("por favor", "", dados$text2)
dados$text2 <- gsub("por gentileza", "", dados$text2)
dados$text2 <- gsub("atenciosamente", "", dados$text2)
dados$text2 <- gsub("respeitosamente", "", dados$text2)
dados$text2 <- gsub("att.", "", dados$text2)
dados$text2 <- gsub("atte.", "", dados$text2)
dados$text2 <- gsub("obrigado", "", dados$text2)
dados$text2 <- gsub("por favor", "", dados$text2)


# Remove a marcação (sic) dos pedidos
dados$text2 <- gsub("\\(sic\\)", "", dados$text2, ignore.case = TRUE)

# Remove palavras com 3 ou mais letras repetidas (ex: XXX, YYY, ZZZ, muito usadas para mascaramento de informações pessoais)
dados$text2 <- gsub("\\b([A-Z])\\1{2,}\\b", "", dados$text2, ignore.case = TRUE)

dados$text <- dados$text2
dados <- dados |> 
  dplyr::select(-text2)
```

```{r processardadosquanteda}
dados <- as.data.frame(dados)

dados$text |> 
  quanteda::tokens(remove_punct = TRUE,       # remove punctuation 
                   remove_symbols = TRUE,     # remove symbols 
                   remove_number = TRUE) |>  # remove numbers
  quanteda::tokens_select(pattern = quanteda::stopwords("pt"), selection = "remove") |> 

  quanteda::tokens_wordstem(language="portuguese")  |> 
  quanteda::dfm(tolower = T) -> ctxt

# add docvars
quanteda::docvars(ctxt, "status") <- dados$st_pedido
quanteda::docvars(ctxt, "periodo") <- dados$dt_periodo
# clean data
ctxt <- quanteda::dfm_subset(ctxt, quanteda::ntoken(ctxt) > 0)
dim(ctxt)

# inspect data
ctxt[1:5, 1:5]
```

Após o processamento, verificamos o total de 1.762 documentos com 5.218 termos.

Inicialmente, iremos executar um LDA não supervisionado para identificação dos tópicos presentes nos dados. A partir dessa informação, iremos executar um LDA semi-supervisionado.

Como não há uma quantidade ideal de tópicos, é necessário análise com diferentes números. Após algumas análises, verificou-se que o ideal seria utilizar 10 tópicos com as 8 palavras em cada um.

A matriz presente na variável *ddlda_topics* representa os 10 tópicos com as 8 palavras com maior probabilidade de aparecem em cada tópico. Os valores da probabilidade da matriz $\beta$ associados aparecem entre parênteses, que é a probabilidade da palavra aparecer no tópico.

```{r lda_orientado_dados}
topicmodels::LDA(ctxt, k = 10, control = list(seed = 1)) -> ddlda
```

```{r limpar_topicos_beta}
# define number of topics
ntopics = 10
# define number of terms
nterms = 8
# generate table

#If matrix == "beta" (default), returns a table with one row per topic and term, with columns
#topic    Topic, as an integer
#term     Term
#beta     Probability of a term generated from a topic according to the multinomial model
#If matrix == "gamma", returns a table with one row per topic and document, with columns
#topic Topic, as an integer
#document Document name or ID
#gamma Probability of topic given document
tidytext::tidy(ddlda, matrix = "beta") %>%
  dplyr::group_by(topic) %>%
  dplyr::slice_max(beta, n = nterms) %>% 
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta) %>%
  dplyr::mutate(term = paste(term, " (", round(beta, 3), ")", sep = ""),
                topic = paste("topic", topic),
                topic = factor(topic, levels = c(paste("topic", 1:ntopics))),
                top = rep(paste("top", 1:nterms), nrow(.)/nterms),
                top = factor(top, levels = c(paste("top", 1:nterms)))) %>%
  dplyr::select(-beta) %>%
  tidyr::spread(topic, term) -> ddlda_top_terms
ddlda_top_terms


```

Abaixo, a matriz $\beta$ completa, com as palavras associadas com suas probabilidades em cada um dos 10 tópicos.

```{r extrair_valores_beta}
ddlda
# extract topics
ddlda_topics <- tidy(ddlda, matrix = "beta")
print(ddlda_topics)

```

### Modelo de tópicos semi-supervisionado

Em seguida, há um modelo de tópicos semi-supervisionado, no qual é definido um dicionário contendo as palavras mais comuns de cada tópico, para guiar o LDA a separar os termos.

```{r lda_com_dicionario}
# semisupervised LDA
dict <- dictionary(list(sessao = c("requer", "inform", "plena"),
                        ceaps = c("gast", "combust", "viag", "alug"),
                        concurso = c("comiss", "concurs", "servid", "poss", "nomeaç", "analis", "vag"),
                        pl = c("projet", "lei", "motiv", "andam"),
                        cn = c("senad", "feder", "congres", "nacion", "palác"),
                        covid = c("pandem", "covid", "mort", "infect", "posit"),
                        gastos = c("gabinet", "parlamen", "pesso", "verba"),
                        fumigeno = c("fumígen", "cigarr", "eletrôn", "fum", "tabag", "tabac"),
                        pais = c("pact", "federaç", "federat", "estad"),
                        tribunal = c("julgam", "impeach", "golp", "testemunh")
                        
                        ))
#residual = TRUE: Este argumento indica que termos que não se encaixam nos tópicos definidos no dicionário devem ser incluídos em um tópico residual.
tmod_slda <- seededlda::textmodel_seededlda(ctxt, 
                                            dict, 
                                            residual = TRUE, 
                                            min_termfreq = 2)


# inspect
seededlda::terms(tmod_slda)
```

Para verificarmos como os tópicos são tratados ao longo do tempo, vamos plotar o seguinte gráfico:

```{r mostrar_topicos_por_perioodo}

# generate data frame

data.frame(tmod_slda$data$periodo, seededlda::topics(tmod_slda)) %>%
  dplyr::rename(Periodo = 1,
                Tópico = 2) %>%
  #dplyr::mutate(Periodo = stringr::str_remove_all(Periodo, "-.*"),
  #              Periodo = stringr::str_replace_all(Periodo, ".$", "0")) %>%
  dplyr::mutate_if(is.character, factor) -> topic_df
# inspect
topic_df
```

```{r mostrar_grafico_topicos_mesano}
topic_df %>%
  dplyr::group_by(Periodo, Tópico) %>%
  dplyr::summarise(freq = n()) %>%
  ggplot(aes(x = Periodo, y = freq, fill = Tópico)) +
  geom_bar(stat="identity", position="fill", color = "black") + 
  theme_bw() +
  labs(x = "Mês/ano") +
  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, "RdBu"))(ntopics+1))) +
  scale_y_continuous(name ="% de pedidos", labels = seq(0, 100, 25))
```

No gráfico acima, vale notar que os pedidos de LAI relacionados a 'concurso' reduziram após a realização do concurso (nov/2022) e as consequentes nomeações (durante o ano de 2023).

### Utilização de modelo orientado a dados

Ao usar o modelo orientado a dados, não definimos inicialmente a quantidade de tópicos que queremos separar as palavras do corpus, mas utilizamos duas métricas, CaoJuan2009\[4\] e Deveaud2014\[5\].

O algoritmo de CaoJuan2009 introduz a métrica de densidade dos tópicos, penalizando conjuntos de tópicos que sejam muito semelhantes entre si. Quanto menor a densidade, maior a diversidade de tópicos.

O algoritmo de Deveaud2014 leva em conta a coerência dos tópicos, ou seja, a semelhança semântica entre as palavras que compõem um tópico. Quanto maior a coerência dos tópicos, mais os tópicos são semanticamente ricos e úteis.

Ao contrário do modelo não-supervisionado, foi preferido utilizar aqui o *stemming* no lugar da *lematização*.

```{r modelo_orientado_dados_mod, message=FALSE, warning=FALSE}
# load data
#textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))

dados <- as.data.frame(dados)

#textdata
#dados
# create corpus object
tm::Corpus(DataframeSource(dados)) %>%
  # convert to lower case
  #tm::tm_map(content_transformer(tolower))  %>%
  # remove stop words
  tm::tm_map(removeWords, quanteda::stopwords(language = "pt", source="stopwords-iso")) %>% 
  # remove punctuation
  tm::tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %>%
  # remove numbers
  tm::tm_map(removeNumbers) %>% 
  # stemming
  tm::tm_map(stripWhitespace)  %>%
  tm::tm_map(stemDocument, language = "pt") -> corpustexto
# inspect data

str(corpustexto)
#inspect(corpustexto)
```

Em razão dos pedidos de acesso à informação tratarem de assuntos repetidos, optou-se por utilizar um valor mínimo de termos igual a 7.

```{r mod_2}
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 7
DTM <- tm::DocumentTermMatrix(corpustexto, 
                              control = list(bounds = list(global = c(minimumFrequency, Inf))))
# inspect the number of documents and terms in the DTM
dim(DTM)

```

```{r mod_3}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
dados <- dados[sel_idx, ]
# inspect the number of documents and terms in the DTM
dim(DTM)
```

```{r mod_4, message=FALSE, warning=FALSE}
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 25, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 1),
  verbose = TRUE
)
```

```{r mod_5, message=FALSE, warning=FALSE}
#print(result)
FindTopicsNumber_plot(result)
```

Pela análise dos gráficos acima, como a própria coluna de etiquetas da direita informa, o número de tópicos no qual os valores de CaoJuan2009 são mínimos e os valores de Deveaud2014 são máximos é quando o número de tópicos é igual a 8.

Abaixo, iremos executar 10.000 interações de amostragem de Gibbs para visualização dos tópicos.

```{r mod_6}
# number of topics
K <- 8
# set random number generator seed
set.seed(1)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- topicmodels::LDA(DTM, K, method="Gibbs", control=list(iter = 10000, verbose = 500))

topicModel
```

Após a execução da amostragem de Gibbs, são obtidos os valores de $\alpha$ , $\beta$ e $\theta$ , sendo:

$a$ : parâmetro de Dirichlet relativo a distribuição dos tópicos. No caso abaixo, o valor do parâmetro calculado foi de 6.25, informando que cada documento possui uma distribuição média de tópicos.

```{r mod_7}
tmResult <- posterior(topicModel)

attr(topicModel, "alpha")
```

$\theta$ : matriz que contém as distribuições de tópicos para cada documento

```{r mod_8}
theta <- tmResult$topics
head(theta)
```

$\phi$ : matriz que contém as distribuições de termos para cada tópico

```{r mod_9}
phi <- tmResult$terms
head(phi)
```

Abaixo, podemos verificar como fica a distribuição dos termos para cada tópico:

```{r mod_10}
# reset topic names
topicNames <- apply(terms(topicModel, 8), 2, paste, collapse = " ")
topicNames
```

Abaixo, temos os 8 termos com maiores valores de $\beta$ que aparecem em cada tópico.

```{r mod_11}

# create a data frame from the topic model data
tidytext::tidy(topicModel, matrix = "beta") %>% 
  # ensure topics are factors with specific levels
  dplyr::mutate(topic = paste0("topic", as.character(topic)),
                topic = factor(topic, levels = paste0("topic", 1:20))) %>%
  # group the data by topic
  dplyr::group_by(topic) %>%
  # arrange terms within each topic by beta value (ascending)
  dplyr::arrange(topic, -beta) %>% 
  # select the top 10 terms with the highest beta values for each topic
  dplyr::top_n(10) %>%
  # add beta to term
  dplyr::mutate(term = paste0(term, " (", round(beta, 3), ")")) %>%
  # remove the beta column as it is now part of the term string
  dplyr::select(-beta) %>%  
  # ungroup the data frame
  dplyr::ungroup() %>%
  # create an id column for each term's position within the topic
  dplyr::mutate(id = rep(1:10, 8)) %>%  
  # pivot the data to a wider format with topics as columns
  tidyr::pivot_wider(names_from = topic, values_from = term) -> topterms  
# inspect
topterms
```

Ainda, ilustrativamente, temos as palavras mais associadas com 'concurso'. Durante o período analisado, verificou-se que houve vários pedidos de perguntas associadas ao cargo de 'consultor legislativo', bem como outras palavras associadas a 'concurso' como 'vagas', 'questões', 'cargos' etc.

```{r mod_12, fig.width=4, fig.height=4, fig.align='center', message=FALSE, warning=F}
# visualize topics as word cloud
# choose topic of interest by a term contained in its name

# Encontra o tópico dentro da lista de tópicos. 
# Se encontrar em mais de uma coluna, retorna a primeira
topicToViz <- grep('concurs', topicNames)[1]
# select to 50 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top50terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
top50terms
# Extrai apenas o nome 
words <- names(top50terms)
words
# extract the probabilities of each of the 50 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
probabilities
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

Abaixo, temos a distribuição dos tópicos em cada documento de acordo com a cor representada na legenda. Como o valor de $\alpha$ é igual a 6.25, verifica-se que há uma distribuição menos concentrada de um só tópico para cada documento.

```{r mod_13}
exampleIds <- c(2, 100, 200, 300, 400)
N <- length(exampleIds)  # Number of example documents

# Get topic proportions from example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames

# Reshape data for visualization
reshape2::melt(cbind(data.frame(topicProportionExamples), 
                                     document = factor(1:N)),
                               variable.name = "topic", 
                               id.vars = "document") %>%  
  # create bar plot using ggplot2
  ggplot(aes(topic, value, fill = document), ylab = "Proportion") +
  # plot bars
  geom_bar(stat="identity") +  
  # rotate x-axis labels
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  # flip coordinates to create horizontal bar plot
  coord_flip() +  
  # facet by document
  facet_wrap(~ document, ncol = N) +
  labs(y = "Valor", fill = "Documento")
```

### LDA com $\alpha$ pré-determinado

Ao contrário do experimento anterior, iremos executar o LDA determinando um valor de $\alpha$ menor do que 1, para verificar se há a concentração de um tópico para cada documento, conforme preconiza a teoria.

No caso, iremos utilizar o valor de $\alpha$ = 0.4.

```{r mod_alfa_pre}
# generate new LDA model with low alpha
topicModel2 <- LDA(DTM, K, method="Gibbs", 
                   control=list(iter = 10000, verbose = 1000, alpha = 0.4))
# save results
tmResult2 <- posterior(topicModel2)
#tmResult
# save theta values
theta2 <- tmResult2$topics
#theta
# save beta values
beta2 <- tmResult2$terms
#beta
# reset topic names
topicNames2 <- apply(terms(topicModel2, 5), 2, paste, collapse = " ")
#topicNames
```

```{r mod_alfa_pre_2, results="hide", echo=T, warning=FALSE, message=FALSE, fig.width=10, fig.height=6, fig.align='center'}
# get topic proportions form example documents
topicProportionExamples2 <- theta2[exampleIds,]
colnames(topicProportionExamples2) <- topicNames2
vizDataFrame <- reshape2::melt(cbind(data.frame(topicProportionExamples2),
                                     document = factor(1:5)), 
                               variable.name = "topic", 
                               id.vars = "document") 
# plot alpha distribution 
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = 5)
```

Pelo acima, verificamos que para um valor de $\alpha$ = 0.4, há, de fato, uma maior definição de documentos relacionados a tópicos específicos, indicando uma associação maior de tópicos relacionados a documentos.

```{r mod_alfa_pre_3}
# mean probabilities over all paragraphs
#print(theta)
#print(colSums(theta))
#print(nDocs(DTM))
topicProportions <- colSums(theta) / nDocs(DTM)
#topicProportions
# assign the topic names we created before
names(topicProportions) <- topicNames
names(topicProportions)
# show summed proportions in decreased order
soP <- sort(topicProportions, decreasing = TRUE)
#soP
# inspect ordering
paste(round(soP, 5), ":", names(soP))
```

```{r mod_alfa_pre_4}
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
# sort by primary topic
so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
so
# show ordering
paste(so, ":", names(so))
```

```{r mod_alfa_pre_5}
# selected by a term in the topic name (e.g. 'militari')
topicToFilter <- grep('legisl', topicNames)[1] 
topicThreshold <- 0.2
selectedDocumentIndexes <- which(theta[, topicToFilter] >= topicThreshold)
filteredCorpus <- dados$text[selectedDocumentIndexes]
# show length of filtered corpus
length(filteredCorpus)
# show first 5 paragraphs
head(filteredCorpus, 5)
```

## Considerações Finais

Neste presente trabalho, no qual analisamos os pedidos de acesso à informação realizadas junto ao Senado Federal entre janeiro de 2022 e junho de 2024, a utilização do LDA para a identificação dos tópicos presentes nos pedidos (=documentos), bem como das palavras mais associadas aos tópicos descobertos se mostra bastante interessante para análise dos padrões subjacentes às requisições dos cidadãos.

Procurou-se realizar dois modelos: um orientado ao conhecedor do negócio, nos quais foram analisadas os melhores números para divisão da quantidade de tópicos e da quantidade de palavras; e um orientado aos dados, nos quais foram aplicados algoritmos de CaoJuan2004 e Deveaud2009 para identificação da quantidade de tópicos presentes nos documentos. Ainda, foram realizadas observações visuais em nuvens de palavras e da distribuição de tópicos para cada documento, com o valor de $\alpha$ calculado pelo algoritmo, no caso, 6.25, e com valor de $\alpha$ pré-determinado, no caso, 0.4.

## Referências

\[1\] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." *Journal of machine Learning research* 3.Jan (2003): 993-1022. <https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>

\[2\] Lei nº 12.527, de 2011. <https://www.planalto.gov.br/ccivil_03/_ato2011-2014/2011/lei/l12527.htm>

\[3\] Relatórios LAI. Senado Federal. <https://www12.senado.leg.br/transparencia/lai/relatorios-lai-1>

\[4\] Cao et al. 2009. <https://ladal.edu.au/topic.html#ref-cao2009density>

\[5\] Deveaud, SanJuan, and Bellot 2014. <https://ladal.edu.au/topic.html#ref-deveaud2014accurate>

## Vídeos sobre LDA que consolidaram o conhecimento

\[a\] Latent Dirichlet Allocation (Part 1 of 2): [https://www.youtube.com/watch?v=T05t-SqKArY](https://www.youtube.com/watch?v=T05t-SqKArY&t=11s)

\[b\] Latent Dirichlet Allocation (Part 2 of 2): <https://www.youtube.com/watch?v=BaM1uiCpj_E>

\[c\] Modelagem de tópicos com LDA, parte 1. <https://www.youtube.com/watch?v=7g-mgBa1F5k>

\[d\] Modelagem de tópicos com LDA, parte 2. <https://www.youtube.com/watch?v=f01VXxK9DVs>

## Código no GitHub

Os arquivos deste trabalho estão disponíveis no Github, no endereço:
