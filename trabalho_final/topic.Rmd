---
title: "Topic Modeling with R"
author: "Martin Schweinberger"
date: "2024-05-17"
output:
  bookdown::html_document2
bibliography: bibliography.bib
link-citations: yes
---




```{r}
#install.packages("readxl")
#install.packages("writexl")

# Função para ler e ajustar as planilhas
ler_e_ajustar_planilha <- function(nome_planilha) {
  #print(paste0("Planilha: ", nome_planilha))
  dados <- read_excel(caminho_arquivo, sheet = nome_planilha)
  # Ajuste os dados para ter o mesmo cabeçalho da primeira planilha
  dados <- dados %>%
    select(all_of(cabecalho)) %>%
    mutate(sheet_name = nome_planilha)  # Adicione o nome da planilha como coluna, se desejar
  return(dados)
}

# Defina o caminho relativo para o seu arquivo CSV
dir_bases <- paste0(getwd(), "/bases/")
caminho_arquivo <- paste0(dir_bases, "base_dados_lai_senado.xlsx")

# Ler o conjunto de abas dentro da planilha
nomes_planilhas <- excel_sheets(caminho_arquivo)

# Obtém o cabeçalho da primeira planilha
dados_primeira_planilha <- read_excel(caminho_arquivo, sheet = 1)
cabecalho <- names(dados_primeira_planilha)

# Leia e ajuste todas as planilhas
dados_consolidados <- lapply(nomes_planilhas, ler_e_ajustar_planilha) %>%
  bind_rows()

colnames(dados_consolidados) <- c("doc_id", "text", "st_pedido", "dt_periodo", "nm_aba")
dados_consolidados$id <- seq_len(nrow(dados_consolidados))

print(paste0("Linhas: ",nrow(dados_consolidados)))
print(paste0("Colunas: ",ncol(dados_consolidados)))

# Veja as primeiras linhas do dataframe consolidado
dados_consolidados$dt_periodo <- as.Date(dados_consolidados$dt_periodo , format = "%Y-%m-%d")

# Ordenar por mês/ano da base e ID do pedido
dados_consolidados <- dados_consolidados %>% arrange(dt_periodo, id)

# Agora que está ordenado, gerar novamente a ordem dos IDs
dados_consolidados$id <- seq_len(nrow(dados_consolidados))

# Gravar a planilha
write_xlsx(dados_consolidados, paste0(dir_bases, "dados_consolidados.xlsx"))
```

```{r prep1, echo=T, eval = F, message=FALSE, warning=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
library(dplyr)
library(flextable)
library(ggplot2)
library(lda)
library(ldatuning)
#install.packages("microbenchmark")
library(microbenchmark)
library(quanteda)
library(RColorBrewer)
#install.packages("readxl")
library(readxl)
library(reshape2)
library(slam)
library(stringr)
#install.packages("textstem")
library(textstem)
library(tidyr)
library(tidytext)
library(tm)
library(topicmodels)
library(wordcloud)
#install.packages("writexl")
library(writexl)
# activate klippy for copy-to-clipboard button
#klippy::klippy()
#install.packages("remotes")
#remotes::install_github("rlesur/klippy")
```

A partir dos dados consolidados de pedidos de acesso à informação de 2022 a junho de 2024, foram realizados os seguintes procedimentos:

1. Texto foi convertido em minúscula
2. Como a parte principal de uma pergunta começa após alguns verbos característicos (ex: solicito, requeiro etc), foi realizada a remoção da parte anterior.
3. Foram removidas sinais específicos (ex: aspas duplas, aspas curvas, nº)
4. Foram removidas algunas palavras de encerramento de e-mail (ex: por favor, atenciosamente), de marcação de imprensa (sic)
5. Foram removidas palavras com 3 ou mais letras repetidas, por se tratarem de ocultação de informações pessoais.


```{r}
dir_bases <- paste0(getwd(), "/bases/")
caminho_arquivo <- paste0(dir_bases, "dados_consolidados.xlsx")

dados <- read_excel(caminho_arquivo)

salvar_df_excel <- function(dados, nm_arquivo) {
  write_xlsx(dados, paste0(dir_bases, nm_arquivo))
}

processar_texto <- function(texto, padrao) {
  match <- str_match(texto, padrao)
  if (!is.na(match[1])) {
    partes_texto <- str_split_fixed(texto, match[1], n = 2)
    partes_texto <- str_trim(partes_texto)
    resultado <- partes_texto[2]
  } else {
    resultado <- ""
  }
  return(resultado)
}

padroes <- list(
  "\\b[Ss]olicit\\w*\\b",  # Captura palavras que começam com "solicit"
  "\\b[Oo]bter\\b",        # Captura a palavra "obter"
  "\\b[Rr]equei\\w*\\b",     # Captura palavras que começam com "reque"
  "\\b[Ss]aber\\b",        # Captura a palavra "saber"
  "\\b[Dd]isponibilizar\\b", # Captura a palavra "disponibilizar"
  "\\b[Rr]eceb\\w*\\b",       # Captura a palavra "receber"
  "\\b[Gg]ostaria\\s+de\\s+\\w+\\b", # Captura "gostaria de" seguido de uma palavra
  "\\b[Gg]ostaria\\s+que\\b", # Captura a expressão "gostaria de"
  "\\b[Nn]ecessito\\s+de\\b", # Captura a expressão "necessito de"
  "\\b[Nn]ecessito\\s+da\\b", # Captura a expressão "necessito da"
  "\\b[Nn]ecessito\\s+do\\b" # Captura a expressão "necessito do"
)

# Remoção dos dois tipos de aspas duplas
dados <- dados |> 
  mutate(text = tolower(text)) |> 
  mutate(text = gsub('"', '', text)) |> 
  mutate(text = gsub('[\u201C\u201D"]', '', text)) |> 
  mutate(text = gsub('/', ' ', text))
         
#print(dados)         

dados <- dados %>%
  mutate(text2 = "")

# Remoção da parte anterior da pergunta
for (padrao in padroes) {
  linhas_vazias <- which(dados$text2 == "")
  dados$text2[linhas_vazias] <- sapply(dados$text[linhas_vazias], processar_texto, padrao = padrao)
}

dados <- dados %>%
  mutate(text2 = if_else(text2 == "", text, text2))

# Remover a substring "nº" da coluna 'text2'
dados$text2 <- gsub("nº", "", dados$text2)

# remover expressões de polidez
dados$text2 <- gsub("lei de acesso à informação", "", dados$text2)
dados$text2 <- gsub("por favor", "", dados$text2)
dados$text2 <- gsub("por gentileza", "", dados$text2)
dados$text2 <- gsub("atenciosamente", "", dados$text2)
dados$text2 <- gsub("respeitosamente", "", dados$text2)
dados$text2 <- gsub("att.", "", dados$text2)
dados$text2 <- gsub("atte.", "", dados$text2)
dados$text2 <- gsub("obrigado", "", dados$text2)


#dados$ds_pedido2 <- dados$ds_pedido

# Remove a marcação (sic) dos pedidos
dados$text2 <- gsub("\\(sic\\)", "", dados$text2, ignore.case = TRUE)

# Remove palavras com 3 ou mais letras repetidas (ex: XXX, YYY, ZZZ, muito usadas para mascaramento
# de informações pessoais)
dados$text2 <- gsub("\\b([A-Z])\\1{2,}\\b", "", dados$text2, ignore.case = TRUE)

dados$text <- dados$text2
dados <- dados %>%
  select(-text2)

dados

#salvar_df_excel(dados, "dados_consolidados2.xlsx")

```

Inicialmente, será executado um LDA não supervisionado para identificar os tópicos presentes nos pedidos de acesso à informação. Para isso, será utilizado o pacote `topicmodels` para calcular o modelo LDA e o pacote `ldatuning` para determinar o número ideal de tópicos.



```{r loaddata}
dados <- as.data.frame(dados)

dados$text |> 
  quanteda::tokens(remove_punct = TRUE,       # remove punctuation 
                   remove_symbols = TRUE,     # remove symbols 
                   remove_number = TRUE) |>  # remove numbers
  quanteda::tokens_select(pattern = stopwords("pt"), selection = "remove") |> 

  quanteda::tokens_wordstem(language="portuguese")  |> 
  quanteda::dfm(tolower = T) -> ctxt
head(ctxt, n = 100)

# add docvars
docvars(ctxt, "status") <- ctxt$status
docvars(ctxt, "periodo") <- ctxt$periodo
# clean data
ctxt <- dfm_subset(ctxt, ntoken(ctxt) > 0)
dim(ctxt)

# inspect data
ctxt[1:20, 1:20]
```




```{r datadrivenlda}
topicmodels::LDA(ctxt, k = 10, control = list(seed = 1234)) -> ddlda
```




```{r tm2, message=FALSE, warning=FALSE}
# load data
#textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))

dados <- as.data.frame(dados)


#textdata
#dados
# create corpus object
tm::Corpus(DataframeSource(dados)) %>%
  # convert to lower case
  #tm::tm_map(content_transformer(tolower))  %>%
  # remove stop words
  tm::tm_map(removeWords, quanteda::stopwords(language = "pt", source="stopwords-iso")) %>% 
  # remove punctuation
  tm::tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %>%
  # remove numbers
  tm::tm_map(removeNumbers) %>% 
  # stemming
  #tm_map(content_transformer(lemmatize_words)) -> corpustexto
  # remove superfluous white spaces
  tm::tm_map(stripWhitespace)  %>%
  tm::tm_map(stemDocument, language = "pt") -> corpustexto
# inspect data

print(corpustexto)
#inspect(corpustexto)
```


### Model calculation {.unnumbered}

Here's the improved and expanded version of the paragraph:

After preprocessing, we have a clean corpus object called `textcorpus`, which we use to calculate the unsupervised Latent Dirichlet Allocation (LDA) topic model [@blei2003lda]. To perform this calculation, we first create a Document-Term Matrix (DTM) from the `textcorpus`. In this step, we ensure that only terms with a certain minimum frequency in the corpus are included (we set the minimum frequency to 5). This selection process not only speeds up the model calculation but also helps improve the model's accuracy by focusing on more relevant and frequently occurring terms. By filtering out less common terms, we reduce noise and enhance the coherence of the topics identified by the LDA model.

```{r tm3a}
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- tm::DocumentTermMatrix(corpustexto, 
                              control = list(bounds = list(global = c(minimumFrequency, Inf))))
# inspect the number of documents and terms in the DTM
dim(DTM)
```

Due to vocabulary pruning, some rows in our Document-Term Matrix (DTM) may end up being empty. Latent Dirichlet Allocation (LDA) cannot handle empty rows, so we must remove these documents from both the DTM and the corresponding metadata. This step ensures that the topic modeling process runs smoothly without encountering errors caused by empty documents. Additionally, removing these empty rows helps maintain the integrity of our analysis by focusing only on documents that contain meaningful content.

```{r tm3a2}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
# inspect the number of documents and terms in the DTM
dim(DTM)
```

The output shows that we have removed 22 documents (8833 - 8811) from the DTM.

As an unsupervised machine learning method, topic models are well-suited for exploring data. The primary goal of calculating topic models is to determine the proportionate composition of a fixed number of topics within the documents of a collection. Experimenting with different parameters is essential to identify the most suitable settings for your analysis needs.

For parameterized models such as Latent Dirichlet Allocation (LDA), the number of topics `K` is the most critical parameter to define in advance. Selecting the optimal `K` depends on various factors. If `K` is too small, the collection is divided into a few very general semantic contexts. Conversely, if `K` is too large, the collection is divided into too many topics, leading to overlaps and some topics being barely interpretable. Finding the right balance is key to achieving meaningful and coherent topics in your analysis.

An alternative to deciding on a set number of topics is to extract parameters form a models using a rage of number of topics. This approach can be useful when the number of topics is not theoretically motivated or based on closer, qualitative inspection of the data. In the example below, the determination of the optimal number of topics follows @murzintcev2020idealtopics, but we only use two metrics (`CaoJuan2009` and `Deveaud2014`) - it is highly recommendable to inspect the results of the four metrics available for the `FindTopicsNumber` function which are `Griffiths2004` [see @griffiths2004integrating], `CaoJuan2009` [see @cao2009density], `Arun2010` [see @arun2010finding], and `Deveaud2014` [see @deveaud2014accurate].

```{r tm3b, message=FALSE, warning=FALSE}
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 30, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 1),
  verbose = TRUE
)
```

We can now plot the results. In this case, we have only use two methods `CaoJuan2009` and `Griffith2004`. The best number of topics shows low values for `CaoJuan2009` and high values for `Griffith2004` (optimally, several methods should converge and show peaks and dips respectively for a certain number of topics).

```{r tm3c, message=FALSE, warning=FALSE}
#print(result)
FindTopicsNumber_plot(result)
```

For our first analysis, however, we choose a thematic "resolution" of `K = 20` topics. In contrast to a resolution of 100 or more, this number of topics can be evaluated qualitatively very easy.

```{r tm4}
# number of topics
K <- 8
# set random number generator seed
set.seed(1)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- topicmodels::LDA(DTM, K, method="Gibbs", control=list(iter = 2000, verbose = 200))

topicModel
```

```{r}
# save results
tmResult <- posterior(topicModel)
#tmResult
# save theta values
theta <- tmResult$topics
# save beta values
beta <- tmResult$terms
#beta
# reset topic names
topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
topicNames
```

Depending on the size of the vocabulary, the collection size and the number K, the inference of topic models can take a very long time. This calculation may take several minutes. If it takes too long, reduce the vocabulary in the DTM by increasing the minimum frequency in the previous step.

Let's take a look at the 10 most likely terms within the term probabilities `beta` of the inferred topics.

```{r tm6}
# create a data frame from the topic model data
tidytext::tidy(topicModel, matrix = "beta") %>% 
  # ensure topics are factors with specific levels
  dplyr::mutate(topic = paste0("topic", as.character(topic)),
                topic = factor(topic, levels = paste0("topic", 1:20))) %>%
  # group the data by topic
  dplyr::group_by(topic) %>%
  # arrange terms within each topic by beta value (ascending)
  dplyr::arrange(topic, -beta) %>% 
  # select the top 10 terms with the highest beta values for each topic
  dplyr::top_n(10) %>%
  # add beta to term
  dplyr::mutate(term = paste0(term, " (", round(beta, 3), ")")) %>%
  # remove the beta column as it is now part of the term string
  dplyr::select(-beta) %>%  
  # ungroup the data frame
  dplyr::ungroup() %>%
  # create an id column for each term's position within the topic
  dplyr::mutate(id = rep(1:10, 20)) %>%  
  # pivot the data to a wider format with topics as columns
  tidyr::pivot_wider(names_from = topic, values_from = term) -> topterms  
# inspect
topterms
```

For the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.

```{r tm8}
topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
# inspect first 3 topic names
topicNames[1:3]
```

### Visualization of Words and Topics {.unnumbered}

Although wordclouds may not be optimal for scientific purposes they can provide a quick visual overview of a set of terms. Let's look at some topics as wordcloud.

In the following code, you can change the variable **topicToViz** with values between 1 and 20 to display other topics.

```{r wordcloud, fig.width=4, fig.height=4, fig.align='center', message=FALSE, warning=F}
# visualize topics as word cloud
# choose topic of interest by a term contained in its name

# Encontra o tópico dentro da lista de tópicos. 
# Se encontrar em mais de uma coluna, retorna a primeira
topicToViz <- grep('mexico', topicNames)[1]
# select to 50 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top50terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
top50terms
# Extrai apenas o nome 
words <- names(top50terms)
words
# extract the probabilities of each of the 50 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
probabilities
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

Let us now look more closely at the distribution of topics within individual documents. To this end, we visualize the distribution in 3 sample documents.

Let us first take a look at the contents of three sample documents:

```{r tm9}
exampleIds <- c(2, 100, 200)
# first 400 characters of file 2
stringr::str_sub(txts$text[2], 1, 400)
# first 400 characters of file 100
stringr::str_sub(txts$text[100], 1, 400)
# first 400 characters of file 200
stringr::str_sub(txts$text[200], 1, 400)
```

After looking into the documents, we visualize the topic distributions within the documents.

```{r vis2, results="hide", warning=FALSE, message=FALSE, fig.width=10, fig.height=6, fig.align='center'}
N <- length(exampleIds)  # Number of example documents

# Get topic proportions from example documents

# Estou obtendo todas as colunas das linhas especificadas em "exampleIds"
topicProportionExamples <- theta[exampleIds,]
topicProportionExamples
topicNames
# Atribuo os nomes dos tópicos às colunas
colnames(topicProportionExamples) <- topicNames
topicProportionExamples
topicNames
# Reshape data for visualization
reshape2::melt(cbind(data.frame(topicProportionExamples), 
                                     document = factor(1:N)),
                               variable.name = "topic", 
                               id.vars = "document") %>%  
  # create bar plot using ggplot2
  ggplot(aes(topic, value, fill = document), ylab = "Proportion") +
  # plot bars
  geom_bar(stat="identity") +  
  # rotate x-axis labels
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  # flip coordinates to create horizontal bar plot
  coord_flip() +  
  # facet by document
  facet_wrap(~ document, ncol = N)  

```

### Topic distributions {.unnumbered}

The figure above illustrates how topics are distributed within a document according to the model. In the current model, all three documents exhibit at least a small percentage of each topic.

The topic distribution within a document can be controlled using the *alpha* parameter of the model. Higher alpha priors result in an even distribution of topics within a document, while lower alpha priors ensure that the inference process concentrates the probability mass on a few topics for each document.

In the previous model calculation, the alpha prior was automatically estimated to fit the data, achieving the highest overall probability for the model. However, this automatic estimate may not align with the results that an analyst desires. Depending on our analysis goals, we might prefer a more concentrated (peaky) or more evenly distributed set of topics in the model.

Next, let us change the alpha prior to a lower value to observe how this adjustment affects the topic distributions in the model. To do this, we first extarct the alpha value of teh previous model.

```{r tm11}
# see alpha from previous model
attr(topicModel, "alpha") 
```

The alpha value of the previous model was `attr(topicModel, "alpha")`. So now, we set a much lower value (0.2) when we generate a new model.

```{r tm12}
# generate new LDA model with low alpha
topicModel2 <- LDA(DTM, K, method="Gibbs", 
                   control=list(iter = 500, verbose = 25, alpha = 0.2))
# save results
tmResult <- posterior(topicModel2)
tmResult
# save theta values
theta <- tmResult$topics
theta
# save beta values
beta <- tmResult$terms
beta
# reset topic names
topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
topicNames
```

```{r}
#topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
#topicNames
```

Now visualize the topic distributions in the three documents again. What are the differences in the distribution structure?

```{r vis3, results="hide", echo=T, warning=FALSE, message=FALSE, fig.width=10, fig.height=6, fig.align='center'}
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- reshape2::melt(cbind(data.frame(topicProportionExamples),
                                     document = factor(1:N)), 
                               variable.name = "topic", 
                               id.vars = "document") 
# plot alpha distribution 
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```

The figure above now shows that the documents are more clearly assigned to specific topics. The difference in the probability of a document belonging to a particular topic is much more distinct, indicating a stronger association between documents and their respective dominant topics.

By adjusting the alpha parameter to a lower value, we have concentrated the probability mass on fewer topics for each document. This change makes the topic distribution within documents less even and more peaked, resulting in documents being more distinctly associated with specific topics.

This adjustment can be particularly useful when analyzing data sets where we expect documents to focus on a few key themes rather than covering a broad range of topics. It allows for a clearer interpretation of the primary topics discussed in each document, enhancing the overall clarity and interpretability of the topic model.

### Topic ranking {.unnumbered}

Determining the defining topics within a collection is a crucial step in topic modeling, as it helps to organize and interpret the underlying themes effectively. There are several approaches to uncover these topics and arrange them in a meaningful order. Here, we present two different methods: **Ordering Topics by Probability** and **Counting Primary Topic Appearances**. These two approaches complement each other and, when used together, can provide a comprehensive understanding of the defining topics within a collection. By combining the probabilistic ranking with the frequency count of primary topics, we can achieve a more nuanced and accurate interpretation of the underlying themes in the data.

#### Approach 1: Ordering Topics by Probability {.unnumbered}

This approach involves ranking topics based on their overall probability within the given collection. By examining the distribution of words across topics and documents, we can identify which topics are more dominant and relevant. This method helps to highlight the most significant themes within the data.

```{r tm14}
# mean probabilities over all paragraphs
print(theta)
print(colSums(theta))
print(nDocs(DTM))
topicProportions <- colSums(theta) / nDocs(DTM)
topicProportions
# assign the topic names we created before
names(topicProportions) <- topicNames
names(topicProportions)
# show summed proportions in decreased order
soP <- sort(topicProportions, decreasing = TRUE)
soP
# inspect ordering
paste(round(soP, 5), ":", names(soP))
```

We recognize some topics that are way more likely to occur in the corpus than others. These describe rather general thematic coherence. Other topics correspond more to specific contents.

#### Approach 2: Counting Primary Topic Appearances {.unnumbered}

Another method is to count how often a topic appears as the primary topic within individual paragraphs or documents. This approach focuses on the frequency with which each topic takes precedence in the text, providing insight into which topics are most commonly addressed and therefore, potentially more important.

```{r tm16}
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
# sort by primary topic
so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
so
# show ordering
paste(so, ":", names(so))
```

Sorting topics by the Rank-1 method highlights topics with specific thematic coherences, placing them at the upper ranks of the list. This sorting approach is valuable for several subsequent analysis steps:

-   *Semantic Interpretation of Topics*: By examining topics ranked higher in the list, researchers can gain insights into the most salient and distinctive themes present in the collection. Understanding these topics facilitates their semantic interpretation and allows for deeper exploration of the underlying content.

-   *Analysis of Time Series*: Examining the temporal evolution of the most important topics over time can reveal trends, patterns, and shifts in discourse. Researchers can track how the prominence of certain topics fluctuates over different time periods, providing valuable context for understanding changes in the subject matter.

-   *Filtering Based on Sub-Topics*: The sorted list of topics can serve as a basis for filtering the original collection to focus on specific sub-topics of interest. Researchers can selectively extract documents or passages related to particular themes, enabling targeted analysis and investigation of niche areas within the broader context.

By leveraging the Rank-1 method to sort topics, researchers can enhance their understanding of the thematic landscape within the collection and facilitate subsequent analytical tasks aimed at extracting meaningful insights and knowledge.

### Filtering documents {.unnumbered}

The inclusion of topic probabilities for each document or paragraph in a topic model enables its application for thematic filtering of a collection. This filtering process involves selecting only those documents that surpass a predetermined threshold of probability for specific topics. For instance, we may choose to retain documents containing a particular topic, such as topic 'X', with a probability exceeding 20 percent.

In the subsequent steps, we will implement this filtering approach to select documents based on their topical content and visualize the resulting document distribution over time. This analysis will provide insights into the prevalence and distribution of specific themes within the collection, allowing for a more targeted exploration of relevant topics across different temporal intervals.

```{r tm18}
# selected by a term in the topic name (e.g. 'militari')
topicToFilter <- grep('militari', topicNames)[1] 
topicThreshold <- 0.2
selectedDocumentIndexes <- which(theta[, topicToFilter] >= topicThreshold)
filteredCorpus <- txts$text[selectedDocumentIndexes]
# show length of filtered corpus
length(filteredCorpus)
# show first 5 paragraphs
head(filteredCorpus, 5)
```

Our filtered corpus contains `r length(filteredCorpus)` documents related to the topic `r topicToFilter` to at least 20 %.

### Topic proportions over time {.unnumbered}

In the final step, we offer a comprehensive overview of the topics present in the data across different time periods. To achieve this, we aggregate the mean topic proportions per decade for all State of the Union (SOTU) speeches. These aggregated topic proportions provide a distilled representation of the prevalent themes over time and can be effectively visualized, such as through a bar plot. This visualization offers valuable insights into the evolving discourse captured within the SOTU speeches, highlighting overarching trends and shifts in thematic emphasis across decades.

```{r vis4, fig.width=9, fig.height=6, fig.align='center', warning=F, message=F}
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$date, 0, 3), "0")
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
topic_proportion_per_decade
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame and generate plot
reshape2::melt(topic_proportion_per_decade, id.vars = "decade") %>%
  ggplot(aes(x=decade, y=value, fill=variable)) +
  geom_bar(stat = "identity") + 
  labs(y = "Proportion", x = "Decade")  +
  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, "RdBu"))(20))) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```






### Loading and preparing data {.unnumbered}

When preparing the data for analysis, we employ several preprocessing steps to ensure its cleanliness and readiness for analysis. Initially, we load the data and then remove punctuation, symbols, and numerical characters. Additionally, we eliminate common stop words, such as *the* and *and*, which can introduce noise and hinder the topic modeling process. To standardize the text, we convert it to lowercase and, lastly, we apply stemming to reduce words to their base form.

```{r loaddata}
# load data
#txts <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb")) 

salvar_df_excel <- function(dados, nm_arquivo) {
  write_xlsx(dados, paste0(dir_bases, nm_arquivo))
}

extrair_ultimos_20 <- function(x) {
  substr(x, max(1, nchar(x) - 19), nchar(x))
}

# Aplicar a função à coluna ds_pedido
#dados$ultimos_20_caracteres_r <- sapply(dados$ds_pedido, extrair_ultimos_20)

# Exibir o dataframe atualizado

dados$text |> 
  quanteda::tokens(remove_punct = TRUE,       # remove punctuation 
                   remove_symbols = TRUE,     # remove symbols 
                   remove_number = TRUE) |>  # remove numbers
  quanteda::tokens_select(pattern = stopwords("pt"), selection = "remove") |> 

  quanteda::tokens_wordstem(language="portuguese") |> 
  
  # convert to document-frequency matrix
  quanteda::dfm(tolower = T) -> ctxt
head(ctxt, n = 100)

# add docvars
docvars(ctxt, "status") <- ctxt$status
docvars(ctxt, "periodo") <- ctxt$periodo
# clean data
ctxt <- dfm_subset(ctxt, ntoken(ctxt) > 0)
dim(ctxt)

# inspect data
ctxt[1:20, 1:20]
```

### Initial unsupervised topic model {.unnumbered}

Now that we have loaded and prepared the data for analysis, we will follow a two-step approach.

1.  First, we perform an unsupervised topic model using Latent Dirichlet Allocation (LDA) to identify the topics present in our data. This initial step helps us understand the broad themes and structure within the data set.

2.  Then, based on the results of the unsupervised topic model, we conduct a supervised topic model using LDA to refine and identify more meaningful topics in our data.

This combined approach allows us to leverage both data-driven insights and expert supervision to enhance the accuracy and interpretability of the topics.

In the initial step that implements a unsupervised, data-driven topic model, we vary the number of topics the LDA algorithm looks for until we identify coherent topics in the data. We use the `LDA` function from the `topicmodels` package instead of the `textmodel_lda` function from the `seededlda` package because the former allows us to include a seed. Including a seed ensures that the results of this unsupervised topic model are reproducible, which is not the case if we do not seed the model, as each model will produce different results without setting a seed.

```{r datadrivenlda}
# generate model: change k to different numbers, e.g. 10 or 20 and look for consistencies in the keywords for the topics below.
topicmodels::LDA(ctxt, k = 15, control = list(seed = 1)) -> ddlda
#topicmodels::LDA(ctxts, k = 10, control = list(seed = 1234)) -> ddlda10
#topicmodels::LDA(ctxts, k = 20, control = list(seed = 1234)) -> ddlda20
```

Now that we have generated an initial data-driven model, the next step is to inspect it to evaluate its performance and understand the topics it has identified. To do this, we need to examine the terms associated with each detected topic. By analyzing these terms, we can gain insights into the themes represented by each topic and assess the coherence and relevance of the model's output.

```{r cleanbetatopics}
# define number of topics
ntopics = 15
# define number of terms
nterms = 8
# generate table


#If matrix == "beta" (default), returns a table with one row per topic and term, with columns
#topic    Topic, as an integer
#term     Term
#beta     Probability of a term generated from a topic according to the multinomial model
#If matrix == "gamma", returns a table with one row per topic and document, with columns
#topic Topic, as an integer
#document Document name or ID
#gamma Probability of topic given document
tidytext::tidy(ddlda, matrix = "beta") %>%
  dplyr::group_by(topic) %>%
  dplyr::slice_max(beta, n = nterms) %>% 
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta) %>%
  # Cria várias colunas
  dplyr::mutate(term = paste(term, " (", round(beta, 3), ")", sep = ""),
                topic = paste("topic", topic),
                topic = factor(topic, levels = c(paste("topic", 1:ntopics))),
                top = rep(paste("top", 1:nterms), nrow(.)/nterms),
                top = factor(top, levels = c(paste("top", 1:nterms)))) %>%
  # Seleciona tudo menos a coluna beta
  dplyr::select(-beta) %>%
  tidyr::spread(topic, term) -> ddlda_top_terms
ddlda_top_terms
```

In a real analysis, we would re-run the unsupervised model multiple times, adjusting the number of topics that the Latent Dirichlet Allocation (LDA) algorithm "looks for." For each iteration, we would inspect the key terms associated with the identified topics to check their thematic consistency. This evaluation helps us determine whether the results of the topic model make sense and accurately reflect the themes present in the data. By varying the number of topics and examining the corresponding key terms, we can identify the optimal number of topics that best represent the underlying themes in our data set. However, we will skip re-running the model here, as this is just a tutorial intended to showcase the process rather than a comprehensive analysis.

To obtain a comprehensive table of terms and their association strengths with topics (the beta values), follow the steps outlined below. This table can help verify if the data contains thematically distinct topics. Additionally, visualizations and statistical modeling can be employed to compare the distinctness of topics and determine the ideal number of topics. However, I strongly recommend not solely relying on statistical measures when identifying the optimal number of topics. In my experience, human intuition is still essential for evaluating topic coherence and consistency.

```{r extractbeta}
ddlda
# extract topics
ddlda_topics <- tidy(ddlda, matrix = "beta")
# inspect
head(ddlda_topics, 20)
```

The purpose of this initial step, in which we generate data-driven unsupervised topic models, is to identify the number of coherent topics present in the data and to determine the key terms associated with these topics. These key terms will then be used as seed terms in the next step: the supervised, seeded topic model. This approach ensures that the supervised model is grounded in the actual thematic structure of the data set, enhancing the accuracy and relevance of the identified topics.

### Supervised, seeded topic model {.unnumbered}

To implement the supervised, seeded topic model, we start by creating a dictionary containing the seed terms we have identified in the first step.

To check terms (to see if ), you can use the following code chunk:

```{r checkterms}
select(ddlda_topics, term) |> unique() |> filter(str_detect(term, "agri"))

ddlda_topics %>%  select(term) %>% unique() %>% filter(str_detect(term, "agri"))
```

```{r seededlda}
# semisupervised LDA
dict <- dictionary(list(military = c("armi", "war", "militari", "conflict"),
                        liberty = c("freedom", "liberti", "free"),
                        nation = c("nation", "countri", "citizen"),
                        law = c("law", "court", "prison"),
                        treaty = c("claim", "treati", "negoti"),
                        indian = c("indian", "tribe", "territori"),
                        labor = c("labor", "work", "condit"),
                        money = c("bank", "silver", "gold", "currenc", "money"),
                        finance = c("debt", "invest", "financ"),
                        wealth = c("prosper", "peac", "wealth"),
                        industry = c("produc", "industri", "manufactur"),
                        navy = c("navi", "ship", "vessel", "naval"),
                        consitution = c("constitut", "power", "state"),
                        agriculture = c("agricultur", "grow", "land"),
                        office = c("office", "serv", "duti")))
#residual = TRUE: Este argumento indica que termos que não se encaixam nos tópicos definidos no dicionário devem ser incluídos em um tópico residual.
tmod_slda <- seededlda::textmodel_seededlda(ctxts, 
                                            dict, 
                                            residual = TRUE, 
                                            min_termfreq = 2)

# inspect
seededlda::terms(tmod_slda)
```

Now, we extract files and create a data frame of topics and documents. This shows what topic is dominant in which file in tabular form.

```{r dataframe}
# generate data frame
data.frame(tmod_slda$data$date, tmod_slda$data$president, seededlda::topics(tmod_slda)) %>%
  dplyr::rename(Date = 1,
                President = 2,
                Topic = 3) %>%
  dplyr::mutate(Date = stringr::str_remove_all(Date, "-.*"),
                Date = stringr::str_replace_all(Date, ".$", "0")) %>%
  dplyr::mutate_if(is.character, factor) -> topic_df
# inspect
head(topic_df)
```

Using the table (or data frame) we have just created, we can visualize the use of topics over time.

```{r vis}
topic_df %>%
  dplyr::group_by(Date, Topic) %>%
  dplyr::summarise(freq = n()) %>%
  ggplot(aes(x = Date, y = freq, fill = Topic)) +
  geom_bar(stat="identity", position="fill", color = "black") + 
  theme_bw() +
  labs(x = "Decade") +
  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, "RdBu"))(ntopics+1))) +
  scale_y_continuous(name ="Percent of paragraphs", labels = seq(0, 100, 25))
```

The figure illustrates the relative frequency of topics over time in the State of the Union (SOTU) texts. Notably, paragraphs discussing the topic of "office," characterized by key terms such as *office*, *serv*, and *duti*, have become less prominent over time. This trend suggests a decreasing emphasis on this particular theme, as evidenced by the diminishing number of paragraphs dedicated to it.

## Data-driven Topic Modelling {.unnumbered}

In this part of the tutorial, we show an alternative approaches for performing data-driven topic modelling using LDA.

### Loading and preparing data {.unnumbered}

When readying the data for analysis, we follow consistent pre-processing steps, employing the `tm` package [@tm2024package; @tm2008pub] for efficient data preparation and cleaning. First, we load the data and convert it into a corpus object. Next, we convert the text to lowercase, eliminating superfluous white spaces, and removing stop words. Subsequently, we proceed to strip the data of punctuation, symbols, and numerical characters. Finally, we apply stemming to standardize words to their base form, ensuring uniformity throughout the data set.

```{r tm2, message=FALSE, warning=FALSE}
# load data
textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))
# create corpus object
tm::Corpus(DataframeSource(textdata)) %>%
  # convert to lower case
  tm::tm_map(content_transformer(tolower))  %>%
  # remove superfluous white spaces
  tm::tm_map(stripWhitespace)  %>%
  # remove stop words
  tm::tm_map(removeWords, quanteda::stopwords()) %>% 
  # remove punctuation
  tm::tm_map(removePunctuation, preserve_intra_word_dashes = TRUE) %>%
  # remove numbers
  tm::tm_map(removeNumbers) %>% 
  # stemming
  tm::tm_map(stemDocument, language = "en") -> textcorpus
# inspect data
str(textcorpus)
```

### Model calculation {.unnumbered}

Here's the improved and expanded version of the paragraph:

After preprocessing, we have a clean corpus object called `textcorpus`, which we use to calculate the unsupervised Latent Dirichlet Allocation (LDA) topic model [@blei2003lda]. To perform this calculation, we first create a Document-Term Matrix (DTM) from the `textcorpus`. In this step, we ensure that only terms with a certain minimum frequency in the corpus are included (we set the minimum frequency to 5). This selection process not only speeds up the model calculation but also helps improve the model's accuracy by focusing on more relevant and frequently occurring terms. By filtering out less common terms, we reduce noise and enhance the coherence of the topics identified by the LDA model.

```{r tm3a}
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- tm::DocumentTermMatrix(textcorpus, 
                              control = list(bounds = list(global = c(minimumFrequency, Inf))))
# inspect the number of documents and terms in the DTM
dim(DTM)
```

Due to vocabulary pruning, some rows in our Document-Term Matrix (DTM) may end up being empty. Latent Dirichlet Allocation (LDA) cannot handle empty rows, so we must remove these documents from both the DTM and the corresponding metadata. This step ensures that the topic modeling process runs smoothly without encountering errors caused by empty documents. Additionally, removing these empty rows helps maintain the integrity of our analysis by focusing only on documents that contain meaningful content.

```{r tm3a2}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
# inspect the number of documents and terms in the DTM
dim(DTM)
```

The output shows that we have removed 22 documents (8833 - 8811) from the DTM.

As an unsupervised machine learning method, topic models are well-suited for exploring data. The primary goal of calculating topic models is to determine the proportionate composition of a fixed number of topics within the documents of a collection. Experimenting with different parameters is essential to identify the most suitable settings for your analysis needs.

For parameterized models such as Latent Dirichlet Allocation (LDA), the number of topics `K` is the most critical parameter to define in advance. Selecting the optimal `K` depends on various factors. If `K` is too small, the collection is divided into a few very general semantic contexts. Conversely, if `K` is too large, the collection is divided into too many topics, leading to overlaps and some topics being barely interpretable. Finding the right balance is key to achieving meaningful and coherent topics in your analysis.

An alternative to deciding on a set number of topics is to extract parameters form a models using a rage of number of topics. This approach can be useful when the number of topics is not theoretically motivated or based on closer, qualitative inspection of the data. In the example below, the determination of the optimal number of topics follows @murzintcev2020idealtopics, but we only use two metrics (`CaoJuan2009` and `Deveaud2014`) - it is highly recommendable to inspect the results of the four metrics available for the `FindTopicsNumber` function which are `Griffiths2004` [see @griffiths2004integrating], `CaoJuan2009` [see @cao2009density], `Arun2010` [see @arun2010finding], and `Deveaud2014` [see @deveaud2014accurate].

```{r tm3b, message=FALSE, warning=FALSE}
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
```

We can now plot the results. In this case, we have only use two methods `CaoJuan2009` and `Griffith2004`. The best number of topics shows low values for `CaoJuan2009` and high values for `Griffith2004` (optimally, several methods should converge and show peaks and dips respectively for a certain number of topics).

```{r tm3c, message=FALSE, warning=FALSE}
FindTopicsNumber_plot(result)
```

For our first analysis, however, we choose a thematic "resolution" of `K = 20` topics. In contrast to a resolution of 100 or more, this number of topics can be evaluated qualitatively very easy.

```{r tm4}
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- topicmodels::LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))

topicModel
```

```{r}
# save results
tmResult <- posterior(topicModel)
tmResult
# save theta values
theta <- tmResult$topics
# save beta values
beta <- tmResult$terms
beta
# reset topic names
topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
topicNames
```

Depending on the size of the vocabulary, the collection size and the number K, the inference of topic models can take a very long time. This calculation may take several minutes. If it takes too long, reduce the vocabulary in the DTM by increasing the minimum frequency in the previous step.

Let's take a look at the 10 most likely terms within the term probabilities `beta` of the inferred topics.

```{r tm6}
# create a data frame from the topic model data
tidytext::tidy(topicModel, matrix = "beta") %>% 
  # ensure topics are factors with specific levels
  dplyr::mutate(topic = paste0("topic", as.character(topic)),
                topic = factor(topic, levels = paste0("topic", 1:20))) %>%
  # group the data by topic
  dplyr::group_by(topic) %>%
  # arrange terms within each topic by beta value (ascending)
  dplyr::arrange(topic, -beta) %>% 
  # select the top 10 terms with the highest beta values for each topic
  dplyr::top_n(10) %>%
  # add beta to term
  dplyr::mutate(term = paste0(term, " (", round(beta, 3), ")")) %>%
  # remove the beta column as it is now part of the term string
  dplyr::select(-beta) %>%  
  # ungroup the data frame
  dplyr::ungroup() %>%
  # create an id column for each term's position within the topic
  dplyr::mutate(id = rep(1:10, 20)) %>%  
  # pivot the data to a wider format with topics as columns
  tidyr::pivot_wider(names_from = topic, values_from = term) -> topterms  
# inspect
topterms
```

For the next steps, we want to give the topics more descriptive names than just numbers. Therefore, we simply concatenate the five most likely terms of each topic to a string that represents a pseudo-name for each topic.

```{r tm8}
topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
# inspect first 3 topic names
topicNames[1:3]
```

### Visualization of Words and Topics {.unnumbered}

Although wordclouds may not be optimal for scientific purposes they can provide a quick visual overview of a set of terms. Let's look at some topics as wordcloud.

In the following code, you can change the variable **topicToViz** with values between 1 and 20 to display other topics.

```{r wordcloud, fig.width=4, fig.height=4, fig.align='center', message=FALSE, warning=F}
# visualize topics as word cloud
# choose topic of interest by a term contained in its name

# Encontra o tópico dentro da lista de tópicos. 
# Se encontrar em mais de uma coluna, retorna a primeira
topicToViz <- grep('mexico', topicNames)[1]
# select to 50 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top50terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
top50terms
# Extrai apenas o nome 
words <- names(top50terms)
words
# extract the probabilities of each of the 50 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:50]
probabilities
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

Let us now look more closely at the distribution of topics within individual documents. To this end, we visualize the distribution in 3 sample documents.

Let us first take a look at the contents of three sample documents:

```{r tm9}
exampleIds <- c(2, 100, 200)
# first 400 characters of file 2
stringr::str_sub(txts$text[2], 1, 400)
# first 400 characters of file 100
stringr::str_sub(txts$text[100], 1, 400)
# first 400 characters of file 200
stringr::str_sub(txts$text[200], 1, 400)
```

After looking into the documents, we visualize the topic distributions within the documents.

```{r vis2, results="hide", warning=FALSE, message=FALSE, fig.width=10, fig.height=6, fig.align='center'}
N <- length(exampleIds)  # Number of example documents

# Get topic proportions from example documents

# Estou obtendo todas as colunas das linhas especificadas em "exampleIds"
topicProportionExamples <- theta[exampleIds,]
topicProportionExamples
topicNames
# Atribuo os nomes dos tópicos às colunas
colnames(topicProportionExamples) <- topicNames
topicProportionExamples
topicNames
# Reshape data for visualization
reshape2::melt(cbind(data.frame(topicProportionExamples), 
                                     document = factor(1:N)),
                               variable.name = "topic", 
                               id.vars = "document") %>%  
  # create bar plot using ggplot2
  ggplot(aes(topic, value, fill = document), ylab = "Proportion") +
  # plot bars
  geom_bar(stat="identity") +  
  # rotate x-axis labels
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  # flip coordinates to create horizontal bar plot
  coord_flip() +  
  # facet by document
  facet_wrap(~ document, ncol = N)  

```

### Topic distributions {.unnumbered}

The figure above illustrates how topics are distributed within a document according to the model. In the current model, all three documents exhibit at least a small percentage of each topic.

The topic distribution within a document can be controlled using the *alpha* parameter of the model. Higher alpha priors result in an even distribution of topics within a document, while lower alpha priors ensure that the inference process concentrates the probability mass on a few topics for each document.

In the previous model calculation, the alpha prior was automatically estimated to fit the data, achieving the highest overall probability for the model. However, this automatic estimate may not align with the results that an analyst desires. Depending on our analysis goals, we might prefer a more concentrated (peaky) or more evenly distributed set of topics in the model.

Next, let us change the alpha prior to a lower value to observe how this adjustment affects the topic distributions in the model. To do this, we first extarct the alpha value of teh previous model.

```{r tm11}
# see alpha from previous model
attr(topicModel, "alpha") 
```

The alpha value of the previous model was `attr(topicModel, "alpha")`. So now, we set a much lower value (0.2) when we generate a new model.

```{r tm12}
# generate new LDA model with low alpha
topicModel2 <- LDA(DTM, K, method="Gibbs", 
                   control=list(iter = 500, verbose = 25, alpha = 0.2))
# save results
tmResult <- posterior(topicModel2)
tmResult
# save theta values
theta <- tmResult$topics
theta
# save beta values
beta <- tmResult$terms
beta
# reset topic names
topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
topicNames
```

```{r}
#topicNames <- apply(terms(topicModel, 5), 2, paste, collapse = " ")
#topicNames
```

Now visualize the topic distributions in the three documents again. What are the differences in the distribution structure?

```{r vis3, results="hide", echo=T, warning=FALSE, message=FALSE, fig.width=10, fig.height=6, fig.align='center'}
# get topic proportions form example documents
topicProportionExamples <- theta[exampleIds,]
colnames(topicProportionExamples) <- topicNames
vizDataFrame <- reshape2::melt(cbind(data.frame(topicProportionExamples),
                                     document = factor(1:N)), 
                               variable.name = "topic", 
                               id.vars = "document") 
# plot alpha distribution 
ggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = "proportion") + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  coord_flip() +
  facet_wrap(~ document, ncol = N)
```

The figure above now shows that the documents are more clearly assigned to specific topics. The difference in the probability of a document belonging to a particular topic is much more distinct, indicating a stronger association between documents and their respective dominant topics.

By adjusting the alpha parameter to a lower value, we have concentrated the probability mass on fewer topics for each document. This change makes the topic distribution within documents less even and more peaked, resulting in documents being more distinctly associated with specific topics.

This adjustment can be particularly useful when analyzing data sets where we expect documents to focus on a few key themes rather than covering a broad range of topics. It allows for a clearer interpretation of the primary topics discussed in each document, enhancing the overall clarity and interpretability of the topic model.

### Topic ranking {.unnumbered}

Determining the defining topics within a collection is a crucial step in topic modeling, as it helps to organize and interpret the underlying themes effectively. There are several approaches to uncover these topics and arrange them in a meaningful order. Here, we present two different methods: **Ordering Topics by Probability** and **Counting Primary Topic Appearances**. These two approaches complement each other and, when used together, can provide a comprehensive understanding of the defining topics within a collection. By combining the probabilistic ranking with the frequency count of primary topics, we can achieve a more nuanced and accurate interpretation of the underlying themes in the data.

#### Approach 1: Ordering Topics by Probability {.unnumbered}

This approach involves ranking topics based on their overall probability within the given collection. By examining the distribution of words across topics and documents, we can identify which topics are more dominant and relevant. This method helps to highlight the most significant themes within the data.

```{r tm14}
# mean probabilities over all paragraphs
print(theta)
print(colSums(theta))
print(nDocs(DTM))
topicProportions <- colSums(theta) / nDocs(DTM)
topicProportions
# assign the topic names we created before
names(topicProportions) <- topicNames
names(topicProportions)
# show summed proportions in decreased order
soP <- sort(topicProportions, decreasing = TRUE)
soP
# inspect ordering
paste(round(soP, 5), ":", names(soP))
```

We recognize some topics that are way more likely to occur in the corpus than others. These describe rather general thematic coherence. Other topics correspond more to specific contents.

#### Approach 2: Counting Primary Topic Appearances {.unnumbered}

Another method is to count how often a topic appears as the primary topic within individual paragraphs or documents. This approach focuses on the frequency with which each topic takes precedence in the text, providing insight into which topics are most commonly addressed and therefore, potentially more important.

```{r tm16}
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
# sort by primary topic
so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
so
# show ordering
paste(so, ":", names(so))
```

Sorting topics by the Rank-1 method highlights topics with specific thematic coherences, placing them at the upper ranks of the list. This sorting approach is valuable for several subsequent analysis steps:

-   *Semantic Interpretation of Topics*: By examining topics ranked higher in the list, researchers can gain insights into the most salient and distinctive themes present in the collection. Understanding these topics facilitates their semantic interpretation and allows for deeper exploration of the underlying content.

-   *Analysis of Time Series*: Examining the temporal evolution of the most important topics over time can reveal trends, patterns, and shifts in discourse. Researchers can track how the prominence of certain topics fluctuates over different time periods, providing valuable context for understanding changes in the subject matter.

-   *Filtering Based on Sub-Topics*: The sorted list of topics can serve as a basis for filtering the original collection to focus on specific sub-topics of interest. Researchers can selectively extract documents or passages related to particular themes, enabling targeted analysis and investigation of niche areas within the broader context.

By leveraging the Rank-1 method to sort topics, researchers can enhance their understanding of the thematic landscape within the collection and facilitate subsequent analytical tasks aimed at extracting meaningful insights and knowledge.

### Filtering documents {.unnumbered}

The inclusion of topic probabilities for each document or paragraph in a topic model enables its application for thematic filtering of a collection. This filtering process involves selecting only those documents that surpass a predetermined threshold of probability for specific topics. For instance, we may choose to retain documents containing a particular topic, such as topic 'X', with a probability exceeding 20 percent.

In the subsequent steps, we will implement this filtering approach to select documents based on their topical content and visualize the resulting document distribution over time. This analysis will provide insights into the prevalence and distribution of specific themes within the collection, allowing for a more targeted exploration of relevant topics across different temporal intervals.

```{r tm18}
# selected by a term in the topic name (e.g. 'militari')
topicToFilter <- grep('militari', topicNames)[1] 
topicThreshold <- 0.2
selectedDocumentIndexes <- which(theta[, topicToFilter] >= topicThreshold)
filteredCorpus <- txts$text[selectedDocumentIndexes]
# show length of filtered corpus
length(filteredCorpus)
# show first 5 paragraphs
head(filteredCorpus, 5)
```

Our filtered corpus contains `r length(filteredCorpus)` documents related to the topic `r topicToFilter` to at least 20 %.

### Topic proportions over time {.unnumbered}

In the final step, we offer a comprehensive overview of the topics present in the data across different time periods. To achieve this, we aggregate the mean topic proportions per decade for all State of the Union (SOTU) speeches. These aggregated topic proportions provide a distilled representation of the prevalent themes over time and can be effectively visualized, such as through a bar plot. This visualization offers valuable insights into the evolving discourse captured within the SOTU speeches, highlighting overarching trends and shifts in thematic emphasis across decades.

```{r vis4, fig.width=9, fig.height=6, fig.align='center', warning=F, message=F}
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$date, 0, 3), "0")
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
topic_proportion_per_decade
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame and generate plot
reshape2::melt(topic_proportion_per_decade, id.vars = "decade") %>%
  ggplot(aes(x=decade, y=value, fill=variable)) +
  geom_bar(stat = "identity") + 
  labs(y = "Proportion", x = "Decade")  +
  scale_fill_manual(values = rev(colorRampPalette(brewer.pal(8, "RdBu"))(20))) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The visualization shows that topics around the relation between the federal government and the states as well as inner conflicts clearly dominate the first decades. Security issues and the economy are the most important topics of recent SOTU addresses.

# Citation & Session Info {.unnumbered}

Schweinberger, Martin. 2024. *Topic Modeling with R*. Brisbane: The University of Queensland. url: <https://slcladal.github.io/topic.html> (Version 2024.05.17).

```         
@manual{schweinberger2024topic,
  author = {Schweinberger, Martin},
  title = {Topic Modeling with R},
  note = {https://ladal.edu.au/topic.html},
  year = {2024},
  organization = "The University of Queensland, Australia. School of Languages and Cultures},
  address = {Brisbane},
  edition = {2024.05.17}
}
```

```{r fin}
sessionInfo()
```

------------------------------------------------------------------------

[Back to top](#introduction)

[Back to HOME](https://ladal.edu.au)

------------------------------------------------------------------------

# References {.unnumbered}
