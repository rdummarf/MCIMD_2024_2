---
title: "Lista02"
author: "Andreia, Fernando e Roberto"
format: html
editor: visual
---

## 1) Gera√ß√£o de N√∫meros Aleat√≥rios

<!-- -->

##### a. O m√©todo Inverse Transform - Exerc√≠cio 3.3 do livro da Rizzo.

*3.2 The Pareto(a, b) distribution has cdf*

$$F(x) = 1 - (\frac{b}{x})^a, x\geq b\geq 0, a\geq 0$$

*Derive the probability inverse transformation* $F^{-1}(U)$ *and use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution. Graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison.*

##### [Resposta]{.underline}

Para c√°lculo da transforma√ß√£o inversa, temos que:

$$
u = F(x) = 1 - (\frac{b}{x})^a \Rightarrow 1-u = (\frac{b}{x})^a \Rightarrow {(1-u)}^\frac{1}{a} = \frac{b}{x} \Rightarrow x = \frac{b}{{(1-u)}^\frac{1}{a}} \Rightarrow F^{-1}_x(u) = {b} * {{(1-u)}^\frac{-1}{a}}
$$

Como explicado no Exemplo 3.3 (p.51), $U$ e $1-U$ possuem a mesma distribui√ß√£o $Uniform(0,1)$, logo √© mais simples utilizar ${b} * {{(u)}^\frac{-1}{a}}$.

Por se tratar de fun√ß√£o de distribui√ß√£o acumulada, √© necess√°ria deriv√°-la para encontrar a fun√ß√£o densidade de probabilidade, logo:

$$F(x) = 1 - (\frac{b}{x})^a, x\geq b\geq 0, a\geq 0$$

$$
f(x) = F'(x) = 0 - (a)*(b^a*(-1)*x^{-a-1}) \Rightarrow a*b^a*x^{-({a+1})}, x \geq b
$$

```{r q1a}
set.seed(1)
# Valores iniciais dados pelo problema
a <- 2
b <- 2
n <- 2000
u <- runif(n)
x <- b * (u)^(-1/a)
print(summary(x))

hist(x, breaks = 100, prob = TRUE, col = 'lightgreen'
     , main = "Histograma de Amostra Simulada com Dens. Te√≥rica Pareto(2, 2)"
     , xlab = 'x', ylab = 'Densidade')
y <- sort(x)
fy <- a * b^a * y^(- (a + 1))
lines (y, fy, col = 'red', lwd = 1)

```

##### b. O m√©todo Acceptance-Rejection - Exerc√≠cio 3.7 do livro da Rizzo. (Exemplo 3.7 faz isso para Beta(2,2). Para simplificar voc√™s podem resolver a quest√£o somente para Œ± e Œ≤ maiores que 1.)

*3.7 Write a function to generate a random sample of size n from the Beta(a, b) distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta(3,2) distribution. Graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.*

##### [Resposta]{.underline}

Vamos calcular a fun√ß√£o densidade de probabilidade (PDF) para a distribui√ß√£o Beta com par√¢metros $\alpha = 3$ e $\beta = 2.$

#### Defini√ß√£o Geral da PDF da Distribui√ß√£o Beta

A fun√ß√£o densidade de probabilidade (PDF) da distribui√ß√£o Beta √© dada por: $$f(x; \alpha, \beta) = \frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)}$$

onde $B(\alpha, \beta)$ √© a fun√ß√£o beta: $B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}$.

Para $\alpha = 3$ e $\beta = 2$ , a PDF se torna: $$ f(x; 3, 2) = \frac{x^{3 - 1} (1 - x)^{2 - 1}}{B(3, 2)} = \frac{x^2 (1 - x)}{B(3, 2)} $$

###### C√°lculo da Fun√ß√£o Beta B(3, 2)

A fun√ß√£o beta para $\alpha = 3$ e $\beta = 2$ √©: $B(3, 2) = \frac{\Gamma(3) \Gamma(2)}{\Gamma(3 + 2)}$

Usando as propriedades da fun√ß√£o gama: $\Gamma(n) = (n - 1)!$, ent√£o, $\Gamma(3) = 2! = 2$, $\Gamma(2) = 1! = 1$, $\Gamma(5) = 4! = 24$

Portanto, $B(3, 2) = \frac{2! \cdot 1!}{4!} = \frac{2 \cdot 1}{24} = \frac{2}{24} = \frac{1}{12}$

Substituindo $B(3, 2) = \frac{1}{12}$ na express√£o da PDF: $$
f(x; 3, 2) = \frac{x^2 (1 - x)}{\frac{1}{12}} = x^2 (1 - x) \cdot 12 = 12x^2 (1 - x)
$$

No exerc√≠cio, para os valores de $\alpha = 3$ e $\beta = 2$, temos uma distribui√ß√£o unimodal com um pico, assim√©trica e levemente inclinada para a direita.

```{r q1b}
function_Beta <- function(n, a, b) 
{ 
  k <- 0
  y <- numeric(n) 
  count <- 0
  while (k < n) 
  {
    u <- runif(1)
    x <- runif(1)
    if (x^(a - 1) * (1 - x)^(b - 1) > u) 
    {
      k <- k + 1
      y[k] <- x
    }
    count <- count + 1
  }
  print(paste("Quantidade de itera√ß√µes: ", count))
  return(y)
}

y <- function_Beta(1000, a = 3, b = 2)
hist(y, breaks = 50, prob = TRUE, ylim = c(0, 2.5), col='lightgreen')
z <- seq (0, 1, .01)
f.z <- 12 * z^2 * (1-z)
lines (z, f.z, col = 'red', lwd=2)
```

## 2) Integra√ß√£o por Monte Carlo

<!-- -->

##### a. Exerc√≠cio 5.3 do livro da Rizzo. (Opcional: gera da exponencial truncada ao intervalo \[0;0,5\] e compara a vari√¢ncia.)

*Compute a Monte Carlo estimate* $\hat{\theta}$ of $$
\theta = \int_0^{0.5} e^{-x} \, dx
$$*by sampling from Uniform(0, 0.5), and estimate the variance of* $\hat{\theta}$*. Find another Monte Carlo estimator* $\hat{\theta}^*$ *by sampling from the exponential distribution. Which of the variances (of* $\hat{\theta}$ *and* $\hat{\theta}^*$*) is smaller, and why?*

##### [Resposta]{.underline}

O valor exato da integral √© dado por:

$$ \theta = -e^{-0.5} -(-e^{0}) = 1 - e^{-0.5} = 1 - 0.606531 = 0.393469  $$

O estimador de Monte Carlo √© dado pela express√£o:

$$
\hat{\theta} = (b - a) \int_{a}^{b} g(x) \, dx = \frac{1}{2} \left( \frac{1}{m} \sum_{i=1}^{m} e^{-u} \right),$$

sendo que $u$ √© gerado a partir de uma distribui√ß√£o $Uniforme (0, 0.5)$.

```{r q2a1}
n <- 20000
# Gera√ß√£o da distribui√ß√†o uniforme entre os intervalor 0 e 0.5
u <- runif(n, 0, 0.5)
theta <- 0.5 * mean(exp(-u))
theta
```

```{r q2a2}
# Para cada 20.000 amostras da distribui√ß√£o uniforme no intervalo [0, 0.5], vamos 
# calcular o estimador de Monte Carlo usando a fun√ß√£o 0.5 * exp(-u), fazendo isso
# 2.000 vezes
estimator <- replicate(2000, expr = {
  u <- runif(n, 0, 0.5)
  theta <- 0.5 * mean(exp(-u))
  theta
})
print(paste("M√©dia do estimador: ", mean(estimator)))
print(paste("Vari√¢ncia do estimador: ", var(estimator)))
print(paste("DP do estimador: ", sd(estimator)))
```

Para estimarmos a vari√¢ncia de $\hat{\theta}^*$, vamos utilizar a seguinte express√£o:

$$\hat{\theta}^* = \frac{1}{m} \sum_{i=1}^{m} I(v < 0.5)$$, sendo $v$ gerado a partir de uma distribui√ß√£o $Exponential(1)$.

```{r q2a3}
n <- 20000
v <- rexp(n, 1)
theta <- mean(v <= 0.5)
print(theta)
```

```{r q2a4}
estimator_1 <- replicate(2000, expr = {
  v <- rexp(n, 1)
  theta <- mean(v <= 0.5)
  theta
})
print(paste("M√©dia do estimador", mean(estimator_1)))
print(paste("Vari√¢ncia do estimador: ", var(estimator_1)))
print(paste("Desvio-padr√£o do estimador: ", sd(estimator_1)))

print(paste("Raz√£o entre os estimadores: ", var(estimator)/var(estimator_1)))


```

A vari√¢ncia do estimador $\hat{\theta}$ (baseado na amostragem uniforme) √© maior do que a vari√¢ncia do estimador $\hat{\theta}^*$ (baseado na amostragem exponencial) porque a transforma√ß√£o usada na amostragem exponencial pode reduzir a variabilidade dos valores amostrados, uma vez que √© melhor ajustada √† forma da fun√ß√£o de densidade $e^{-x}$.

Assim, o estimador $\hat{\theta}^*$ ter√° uma vari√¢ncia menor devido √† melhor adequa√ß√£o da amostragem √† distribui√ß√£o exponencial.

##### b. Vari√°veis Antit√©ticos: Exerc√≠cio 5.10 do livro da Rizzo. (Opcional: Trocar esse exerc√≠cio para 5.9, mas saiba que a fun√ß√£o n√£o √© monot√¥nica \[pode quebrar em peda√ßos monot√¥nicos\] e o limite superior √© infinito \[precisa fazer alguma transforma√ß√£o de vari√°veis x-\> 1/x funcionaria\])

*Use Monte Carlo integration with antithetic variables to estimate*

$$ \int_{0}^{1} \frac{e^{-x}}{1 + x^2} \, dx
$$

*and find the approximate reduction in variance as a percentage of the variance without variance reduction.*

##### [Resposta]{.underline}

O uso de vari√°veis anti√©ticas ajudam a reduzir a variabilidade de um m√©todo e estima√ß√£o.

Como regra geral, temos que:

$$
Var(\frac{U_1 + U_2}{2}) = \frac{1}{4} * (Var(U_1) + Var(U_2) + 2*Cov(U_1,U_2))
$$

sendo que a vari√¢ncia de $\frac{(U_1+U_2)}{2}$ √© menor se $U_1$ e $U_2$ s√£o negativamente correlacionadas do que quando as vari√°veis s√£o independentes.

Neste caso, iremos gerar uma distribui√ß√£o $Uniforme (0,1)$ e aplicar o estimador de Monte Carlo para o valor de $u$.

Ainda, para calcularmos com o uso de vari√°veis anti√©ticas, iremos aplicar a express√£o abaixo:

$$
\hat{\theta} = \frac{1}{m} \left\{ Y_1 + Y'_1 + Y_2 + Y'_2 + \cdots + Y_{m/2} + Y'_{m/2} \right\} 
$$

```{r q2b}
set.seed(10)
m <- 20000
monte_carlo <- replicate (2000, expr = 
                   { 
                     u <- runif (m) 
                     mean(exp(-u)/(1 + u^2)) 
                    }
                 )
set.seed(10)
anti_ethic <- replicate (2000, expr = 
                           { u <- runif (m/2) 
                           x1 <- exp(-u)/(1 + u^2) 
                           x2 <- exp(-(1 -u))/(1 + ((1 -u)^2)) 
                           mean(c (x1, x2)) 
                           }
                         )

print(c(mean(monte_carlo), var(monte_carlo)))
print(c(mean(anti_ethic), var(anti_ethic)))

approx_reduction = 100 * (var (monte_carlo) - var (anti_ethic)) / var (monte_carlo)
print(paste(round(approx_reduction,4), "%"))

```

A redu√ß√£o da vari√¢ncia entre o uso de Monte Carlo padr√£o e o uso de Monte Carlo com vari√°veis anti√©ticas foi de 96,09%.

##### c. Importance Sampling - Exerc√≠cio 5.14 do livro da Rizzo.

*Obtain a Monte Carlo estimate of* $$\int_{1}^{\infty} \frac{x^2}{\sqrt{2\pi}} e^{-x^2/2} \, dx
 $$ *by importance sampling.*

##### [Resposta]{.underline}

```{r q2c}
set.seed(1)
n <- 20000
imp_sampl_1 <- replicate(2000, expr = 
                   {
                     x <- sqrt(rchisq(n, 1)) + 1
                     f <- 2 * dnorm(x, 1)
                     g <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
                     mean (g/f)
                   }
)
set.seed(1)
imp_sampl_2 <- replicate(2000, expr = 
                   {
                     x <- rgamma(n, 3/2, 2) + 1
                     f <- dgamma(x - 1, 3/2, 2)
                     g <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
                     mean (g/f)
                   }
)
set.seed(1)
imp_sampl_3 <- replicate(2000, expr = 
                   {
                     x <- rexp(n, 1)
                     f <- dexp(x, 1)
                     g <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
                     mean (g/f)
                   }
)
set.seed(1)
imp_sampl_4 <- replicate(2000, expr = 
                   {
                     x <- rcauchy(n, 2, 1)
                     f <- dcauchy(x, 2, 1)
                     g <- x^2 * exp(-x^2/2)/sqrt(2 * pi)
                     mean (g/f)
                   }
)


print(c(mean(imp_sampl_1), mean(imp_sampl_2), mean(imp_sampl_3), mean(imp_sampl_4)))
print(c(var(imp_sampl_1), var(imp_sampl_2), var(imp_sampl_3), var(imp_sampl_4)))
print(var(imp_sampl_1)/var(imp_sampl_2))

```

No caso em tela, a 1a. fun√ß√£o produz um estimador mais eficiente do que a 2a. fun√ß√£o, uma vez que a vari√¢ncia do estimador da 1a. fun√ß√£o √© menor do que a vari√¢ncia do estimador da 2a. fun√ß√£o.

##### d. Stratified Importance Sampling - Exerc√≠cio 5.15 do livro da Rizzo. (N√£o esquece de usar os quant√≠s da fun√ß√£o import√¢ncia para definir os intervalos.)

*Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.*

##### [Resposta]{.underline}

O exemplo 5.13 traz a seguinte integral:

$$ \int_{0}^{1} \frac{e^{-x}}{1 + x^2} \, dx $$ com a fun√ß√£o import√¢ncia dada por:

$$ f(x) =  \frac{e^{-x}}{1 - e^{-1}}, 0 \lt x \lt 1,  $$ nos cinco subintervalos: $(j/5, (j + 1)/5), j = 0, 1, . . . , 4.$.

As vari√°veis no $j^{th}$ subintervalo s√£o geradas a partir da densidade:

$$
\frac{5e^{-x}}{1 - e^{-1}}, \quad \frac{j-1}{5} < x < \frac{j}{5}.
$$

Para encontrar a transforma√ß√£o inversa usada no c√≥digo, considere a fun√ß√£o de import√¢ncia dada por:

$$
f(x) = \frac{e^{-x}}{1 - e^{-1}} \quad \text{para } 0 < x < 1
$$

Primeiro, encontramos a fun√ß√£o de distribui√ß√£o acumulada (CDF) da fun√ß√£o de import√¢ncia ( f(x) ):

$$
F(x) = \int_{0}^{x} \frac{e^{-t}}{1 - e^{-1}} \, dt
$$

Integrando, obtemos:

$$
F(x) = \left[ \frac{-e^{-t}}{1 - e^{-1}} \right]_{0}^{x} = \frac{1 - e^{-x}}{1 - e^{-1}}
$$

Para obter a transforma√ß√£o inversa, resolvemos ( F(x) = u ) para ( x ):

$$
u = \frac{1 - e^{-x}}{1 - e^{-1}} => u(1-e^{-1}) = 1 - e^{-x} => e^{-x} = 1 - u(1-e^{-1}) =>
$$

$$
=> ln(e^{-x}) = ln(1 - u(1-e^{-1}) => x = -ln(1 - u(1-e^{-1})
$$

que √© a transforma√ß√£o inversa da fun√ß√£o de distribui√ß√£o acumulada (CDF) da distribui√ß√£o de import√¢ncia

$$
f(x) = \frac{e^{-x}}{1 - e^{-1}}
$$

```{r}
set.seed(1)
M <- 20000
k <- 5
m <- M/k
si_stratified <- numeric(k)
v_stratified <- numeric(k)
g <- function(x) exp(-x)/(1 + x^2)
f <- function(x) ((k * exp(-x))/(1 - exp(-1)))

for(j in 1:k) {
  print(paste("runif(m, (j - 1)/k, j/k)", m, (j-1)/k, j/k))
  u <- runif(m, (j - 1)/k, j/k)
  x <- -log(1 - (1 - exp(-1)) * u)
  gf <- g(x)/f(x)
  si_stratified[j] <- mean(gf)
  v_stratified[j] <- var(gf)
}

print(paste("A soma da m√©dia dos estratos para estimativa da integral √©: ",sum(si_stratified)))
print(paste("A m√©dia da vari√¢ncia dos estratos √©: ", mean(v_stratified)))
print(sqrt(mean(v_stratified)))

```

No caso do exemplo 5.10, se n√£o fosse usada a estratifica√ß√£o, os valores seriam:

```{r}
set.seed(1)
M <- 20000
si <- 0
v <- 0
g <- function(x) exp(-x)/(1 + x^2)
f <- function(x) (exp(-x)/(1 - exp(-1)))
u <- runif(M)
x <- -log(1 - (1 - exp(-1)) * u) 
gf <- g(x)/f(x)

si_n_stratified <- mean(gf)
v_n_stratified <- var(gf)

print(si_n_stratified)
print(v_n_stratified)
print(sqrt(v_n_stratified))
```

```{r}
print((v_n_stratified - mean(v_stratified))/v_n_stratified)
```

No exemplo calculado, a vari√¢ncia por amostragem estratificada foi reduzida para 99,82% do valor da vari√¢ncia por amostragem n√£o estratificada.

## 3) Infer√™ncia com Monte Carlo

##### a. MSE (EQM): Exerc√≠cio 6.1 do livro da Rizzo. (aproveite os c√≥digos para a Normal com contamina√ß√£o)

*Estimate the MSE of the level* $k$ trimmed means for random samples of size 20 generated from a standard Cauchy distribution. (The target parameter $\theta$ is the center or median; the expected value does not exist.) Summarize the estimates of MSE in a table for $k = 1,2,...,9$.

##### [Resposta:]{.underline}

A distribui√ß√£o de Cauchy √© uma distribui√ß√£o cont√≠nua com densidade de probabilidade dada por:

$$f(x; x_0, \gamma) = \frac{1}{\pi \gamma \left[1 + \left(\frac{x - x_0}{\gamma}\right)^2\right]}$$

Para a distribui√ß√£o de Cauchy padr√£o, temos $( x_0 = 0 )$ e $( \gamma = 1 )$:

$$f(x) = \frac{1}{\pi (1 + x^2)}$$

Trimmed mean (m√©dia podada) √© uma m√©dia calculada excluindo os valores extremos de uma amostra. Para uma amostra ordenada $X_{(1)}, X_{(2)}, \ldots, X_{(n)}$:

-   O n√≠vel de podagem \$ k \$ indica que os \$k\$ menores e os \$k\$ maiores valores s√£o exclu√≠dos.
-   A m√©dia podada √© ent√£o calculada sobre os \$ n - 2k \$ valores restantes.

O erro quadr√°tico m√©dio (MSE) √© uma medida de precis√£o de um estimador. Para um estimador $\hat{\theta}$ do par√¢metro $\theta$:

$\text{MSE}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \theta)^2]$

```{r}
set.seed(1)
n <- 20
K <- n/2 - 1 
m <- 1000
mse <- matrix(0, n/2, 2) 
trimmed.mse <- function(n, m, k) 
  {
    tmean <- numeric(m)
    for (i in 1:m) 
    {
      x <- sort(rcauchy(n))
      tmean[i] <- sum(x[(k + 1):(n - k)])/(n - 2 * k)
    }
    mse.est <- mean(tmean^2)
    se.mse <- sqrt(mean((tmean - mean(tmean))^2))/sqrt(m)
    return(c(mse.est, se.mse))
}

for (k in 0:K) 
{
  mse[k + 1, 1:2] <- trimmed.mse(n = n, m = m, k = k)
}

mse <- as.data.frame(cbind(0:K, mse))
names(mse) <- c("k", "m√©dia podada", "erro padr√£o")
print(mse)

```

A tabela de resultados mostra como o MSE da m√©dia podada varia com diferentes n√≠veis de podagem $k$. Conforme o n√≠vel de $k$ aumenta, excluem-se os valores mais extremos, melhorando a precis√£o do estimador, reduzindo o erro quadr√°tico m√©dio. No caso, a podagem ajuda a reduzir o efeito dos outliers na distribui√ß√£o de Cauchy, que afetam mais significativamente a m√©dia.

Entretanto, o $\text{MSE}$ e o $\text{erro padr√£o}$ atingem seus menores valores n√£o no m√°ximo valor de $k$, mas quando ele √© igual a 7.

##### b. Poder de testes: Exerc√≠cio 6.3 do livro da Rizzo. (aproveite os c√≥digos do Exemplo 6.9)

*Plot the power curves for the t-test in Example 6.9 for sample sizes 10, 20, 30, 40, and 50, but omit the standard error bars. Plot the curves on the same graph, each in a different color or different line type, and include a legend. Comment on the relation between power and sample size.*

##### [Resposta]{.underline}

```{r}
N <- c(10, 20, 30, 40, 50) 
m <- 1000
mu0 <- 500
sigma <- 100
mu <- c(seq(450, 650, 10)) #alternatives
M <- length(mu)
power <- matrix(0, M, 5)
for (j in 1:5) 
{
  n <- N[j]
  for (i in 1:M) 
  {
    mu1 <- mu[i]
    pvalues <- replicate(m, expr = 
    {
      #simulate under alternative mu1
      x <- rnorm(n, mean = mu1, sd = sigma)
      ttest <- t.test(x, alternative = "greater", mu = mu0)
      ttest$p.value
    })
    power[i, j] <- mean(pvalues <= .05)
  }
}
```

```{r}
plot(mu, power[, 1], type = "l", ylim = range(power), xlab = bquote(mu), ylab = "Poder do teste")
abline(v = mu0, lty = 3)
abline(h = 0.05, lty = 3)
for (j in 2:5) 
  {
    lines(mu, power[, j], col = j)
  }

legend("bottomright", inset = 0.02, legend = N, col = 1:5, lty = 1)

```

Quando o tamanho da amostra aumenta, a precis√£o das estimativas de par√¢metros tamb√©m aumenta. A m√©dia amostral se aproxima da verdadeira m√©dia populacional $ùúá_0$ , diminuindo a variabilidade da m√©dia amostral. Isso resulta em um maior poder estat√≠stico, permitindo detectar diferen√ßas menores entre a m√©dia amostral e $ùúá_0$. Portanto, quanto mais se aumentam as amostras, mais a curva do poder estat√≠stico se aproxima de $ùúá_0$, indicando uma maior capacidade de detectar desvios da hip√≥tese nula.

##### c. N√≠veis de confian√ßa: Exerc√≠cio 6.4 ou 6.5 do livro da Rizzo

*6.4 Suppose that* $X_1, . . . , X_n$ *are a random sample from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter* $\mu$*. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.*

##### [Resposta]{.underline}

Iremos gerar uma amostra de tamanho 50 a partir de uma distribui√ß√£o lognormal padr√£o (com m√©dia logar√≠tmica $ùúá = 0$ e desvio padr√£o logar√≠tmico $\sigma = 1$ ).

O intervalo de confian√ßa √© calculado utilizando-se a f√≥rmula:

$$
\left( \bar{y} - 1.96 \cdot \frac{s_y}{\sqrt{30}}, \bar{y} + 1.96 \cdot \frac{s_y}{\sqrt{30}} \right)
$$

```{r}
set.seed(1)
n <- 50
confid_interv <- replicate(20000, expr = 
  {
    x <- rlnorm(n, 0, 1)
    y <- log(x)
    ybar <- mean(y)
    se <- sd(y)/sqrt(n)
    ybar + se * qnorm(c(0.025, 0.975))
  })
L_confid_interv <- confid_interv[1, ]
U_confid_interv <- confid_interv[2, ]

print(paste("Quantidade de ICs que cont√©m o valor da m√©dia logar√≠tmica: ", sum(L_confid_interv < 0 & U_confid_interv > 0)))
print(paste("Propor√ß√£o de ICs que cont√™m o verdadeiro valor de mi: ", 100 * mean(L_confid_interv < 0 & U_confid_interv > 0),"%"))

  
  
```

Ao usarmos a simula√ß√£o Monte Carlo para validar a constru√ß√£o de intervalos de confian√ßa (IC) para a m√©dia logar√≠tmica $\mu$ de uma distribui√ß√£o lognormal, do total de 20.000 IC, temos 18.904 IC que cont√©m o valor da m√©dia logar√≠tmica ($\mu = 0$ para a distribui√ß√£o lognormal padr√£o).

Ainda, temos que a propor√ß√£o de IC que cont√™m o valor 0, pela simula√ß√£o, foi de 94,52%, um valor muito pr√≥ximo de 95%.
