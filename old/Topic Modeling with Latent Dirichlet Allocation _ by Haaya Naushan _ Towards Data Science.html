<!DOCTYPE html>
<!-- saved from url=(0090)https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8 -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="origin-trial" content="A/kargTFyk8MR5ueravczef/wIlTkbVk1qXQesp39nV+xNECPdLBVeYffxrM8TmZT6RArWGQVCJ0LRivD7glcAUAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZS5jb206NDQzIiwiZmVhdHVyZSI6IkRpc2FibGVUaGlyZFBhcnR5U3RvcmFnZVBhcnRpdGlvbmluZzIiLCJleHBpcnkiOjE3NDIzNDIzOTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9"><style type="text/css" id="alertifyCSS">.alertify-logs>*{padding:12px 24px;color:#fff;box-shadow:0 2px 5px 0 rgba(0,0,0,.2);border-radius:1px}.alertify-logs>*,.alertify-logs>.default{background:rgba(0,0,0,.8)}.alertify-logs>.error{background:rgba(244,67,54,.8)}.alertify-logs>.success{background:rgba(76,175,80,.9)}.alertify{position:fixed;background-color:rgba(0,0,0,.3);left:0;right:0;top:0;bottom:0;width:100%;height:100%;z-index:2}.alertify.hide{opacity:0;pointer-events:none}.alertify,.alertify.show{box-sizing:border-box;transition:all .33s cubic-bezier(.25,.8,.25,1)}.alertify,.alertify *{box-sizing:border-box}.alertify .dialog{padding:12px}.alertify .alert,.alertify .dialog{width:100%;margin:0 auto;position:relative;top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.alertify .alert>*,.alertify .dialog>*{width:400px;max-width:95%;margin:0 auto;text-align:center;padding:12px;background:#fff;box-shadow:0 2px 4px -1px rgba(0,0,0,.14),0 4px 5px 0 rgba(0,0,0,.098),0 1px 10px 0 rgba(0,0,0,.084)}.alertify .alert .msg,.alertify .dialog .msg{padding:12px;margin-bottom:12px;margin:0;text-align:left}.alertify .alert input:not(.form-control),.alertify .dialog input:not(.form-control){margin-bottom:15px;width:100%;font-size:100%;padding:12px}.alertify .alert input:not(.form-control):focus,.alertify .dialog input:not(.form-control):focus{outline-offset:-2px}.alertify .alert nav,.alertify .dialog nav{text-align:right}.alertify .alert nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button),.alertify .dialog nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button){background:transparent;box-sizing:border-box;color:rgba(0,0,0,.87);position:relative;outline:0;border:0;display:inline-block;-webkit-align-items:center;-ms-flex-align:center;-ms-grid-row-align:center;align-items:center;padding:0 6px;margin:6px 8px;line-height:36px;min-height:36px;white-space:nowrap;min-width:88px;text-align:center;text-transform:uppercase;font-size:14px;text-decoration:none;cursor:pointer;border:1px solid transparent;border-radius:2px}.alertify .alert nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button):active,.alertify .alert nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button):hover,.alertify .dialog nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button):active,.alertify .dialog nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button):hover{background-color:rgba(0,0,0,.05)}.alertify .alert nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button):focus,.alertify .dialog nav button:not(.btn):not(.pure-button):not(.md-button):not(.mdl-button):focus{border:1px solid rgba(0,0,0,.1)}.alertify .alert nav button.btn,.alertify .dialog nav button.btn{margin:6px 4px}.alertify-logs{position:fixed;z-index:1}.alertify-logs.bottom,.alertify-logs:not(.top){bottom:16px}.alertify-logs.left,.alertify-logs:not(.right){left:16px}.alertify-logs.left>*,.alertify-logs:not(.right)>*{float:left;-webkit-transform:translateZ(0);transform:translateZ(0);height:auto}.alertify-logs.left>.show,.alertify-logs:not(.right)>.show{left:0}.alertify-logs.left>*,.alertify-logs.left>.hide,.alertify-logs:not(.right)>*,.alertify-logs:not(.right)>.hide{left:-110%}.alertify-logs.right{right:16px}.alertify-logs.right>*{float:right;-webkit-transform:translateZ(0);transform:translateZ(0)}.alertify-logs.right>.show{right:0;opacity:1}.alertify-logs.right>*,.alertify-logs.right>.hide{right:-110%;opacity:0}.alertify-logs.top{top:0}.alertify-logs>*{box-sizing:border-box;transition:all .4s cubic-bezier(.25,.8,.25,1);position:relative;clear:both;-webkit-backface-visibility:hidden;backface-visibility:hidden;-webkit-perspective:1000;perspective:1000;max-height:0;margin:0;padding:0;overflow:hidden;opacity:0;pointer-events:none}.alertify-logs>.show{margin-top:12px;opacity:1;max-height:1000px;padding:12px;pointer-events:auto}</style><title>Topic Modeling with Latent Dirichlet Allocation | by Haaya Naushan | Towards Data Science</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-12-04T00:51:57.363Z"><meta data-rh="true" name="title" content="Topic Modeling with Latent Dirichlet Allocation | by Haaya Naushan | Towards Data Science"><meta data-rh="true" property="og:title" content="Topic Modeling with Latent Dirichlet Allocation"><meta data-rh="true" property="al:android:url" content="medium://p/e7ff75290f8"><meta data-rh="true" property="al:ios:url" content="medium://p/e7ff75290f8"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Topic modeling is a form of unsupervised machine learning that allows for efficient processing of large collections of data, while preserving the statistical relationships that are useful for tasks…"><meta data-rh="true" property="og:description" content="A practical exploration of the Natural Language Processing technique of Latent Dirichlet Allocation and its application to the task of…"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8"><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:1200/1*yHnDOAR2LBJJ6hHws00vWg.jpeg"><meta data-rh="true" property="article:author" content="https://haaya-naushan.medium.com"><meta data-rh="true" name="author" content="Haaya Naushan"><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" property="twitter:title" content="Topic Modeling with Latent Dirichlet Allocation"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/e7ff75290f8"><meta data-rh="true" property="twitter:description" content="A practical exploration of the Natural Language Processing technique of Latent Dirichlet Allocation and its application to the task of…"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:1200/1*yHnDOAR2LBJJ6hHws00vWg.jpeg"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="twitter:label1" content="Reading time"><meta data-rh="true" name="twitter:data1" content="18 min read"><link data-rh="true" rel="icon" href="https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:152:152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:120:120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:76:76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:60:60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"><link data-rh="true" rel="preconnect" href="https://glyph.medium.com/" crossorigin=""><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/unbound.css"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/unbound.css"><link data-rh="true" rel="author" href="https://haaya-naushan.medium.com/"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/e7ff75290f8"><script type="text/javascript" async="" charset="utf-8" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/recaptcha__pt.js" crossorigin="anonymous" integrity="sha384-RSRjcOqei1uHNq2nfM4X8hL/TvLWWMmcBHy75yNzvUeEX/BwGcZMSFeRHoMYYHkS"></script><script async="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/branch-latest.min.js"></script><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:1200\u002F1*yHnDOAR2LBJJ6hHws00vWg.jpeg"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8","dateCreated":"2020-12-02T23:24:19.018Z","datePublished":"2020-12-02T23:24:19.018Z","dateModified":"2023-06-15T14:05:38.069Z","headline":"Topic Modeling with Latent Dirichlet Allocation - Towards Data Science","name":"Topic Modeling with Latent Dirichlet Allocation - Towards Data Science","description":"Topic modeling is a form of unsupervised machine learning that allows for efficient processing of large collections of data, while preserving the statistical relationships that are useful for tasks…","identifier":"e7ff75290f8","author":{"@type":"Person","name":"Haaya Naushan","url":"https:\u002F\u002Fhaaya-naushan.medium.com"},"creator":["Haaya Naushan"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":192,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:384\u002F1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8"}</script><style type="text/css" data-fela-rehydration="546" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="546" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k1{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@-moz-keyframes k1{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@keyframes k1{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:sticky}.n{top:0}.o{z-index:500}.p{padding:0 24px}.q{align-items:center}.r{border-bottom:solid 1px #F2F2F2}.y{height:41px}.z{line-height:20px}.ab{display:flex}.ac{height:57px}.ae{flex:1 0 auto}.af{color:inherit}.ag{fill:inherit}.ah{font-size:inherit}.ai{border:inherit}.aj{font-family:inherit}.ak{letter-spacing:inherit}.al{font-weight:inherit}.am{padding:0}.an{margin:0}.ao{cursor:pointer}.ap:disabled{cursor:not-allowed}.aq:disabled{color:#6B6B6B}.ar:disabled{fill:#6B6B6B}.au path{fill:#242424}.av{height:27px}.aw{margin-left:16px}.ax{border:none}.ay{border-radius:20px}.az{width:240px}.ba{background:#F9F9F9}.bb path{fill:#6B6B6B}.bd{outline:none}.be{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bf{font-size:14px}.bg{width:100%}.bh{padding:10px 20px 10px 0}.bi{background-color:transparent}.bj{color:#242424}.bk::placeholder{color:#6B6B6B}.bl{display:inline-block}.bm{margin-left:12px}.bn{margin-right:12px}.bo{border-radius:4px}.bp{margin-left:24px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{justify-content:center}.cg{max-width:680px}.ch{min-width:0}.ci{animation:k1 1.2s ease-in-out infinite}.cj{height:100vh}.ck{margin-bottom:16px}.cl{margin-top:48px}.cm{align-items:flex-start}.cn{flex-direction:column}.co{justify-content:space-between}.cp{margin-bottom:24px}.cv{width:80%}.cw{background-color:#F2F2F2}.dc{height:44px}.dd{width:44px}.de{margin:auto 0}.df{margin-bottom:4px}.dg{height:16px}.dh{width:120px}.di{width:80px}.do{margin-bottom:8px}.dp{width:96%}.dq{width:98%}.dr{width:81%}.ds{margin-left:8px}.dt{color:#6B6B6B}.du{font-size:13px}.dv{height:100%}.eo{color:#FFFFFF}.ep{fill:#FFFFFF}.eq{background:rgba(102, 138, 170, 1)}.er{border-color:rgba(102, 138, 170, 1)}.ev:disabled{cursor:inherit !important}.ew:disabled{opacity:0.3}.ex:disabled:hover{background:rgba(102, 138, 170, 1)}.ey:disabled:hover{border-color:rgba(102, 138, 170, 1)}.ez{border-radius:99em}.fa{border-width:1px}.fb{border-style:solid}.fc{box-sizing:border-box}.fd{text-decoration:none}.fe{text-align:center}.fh{margin-right:32px}.fi{position:relative}.fj{fill:#6B6B6B}.fm{background:transparent}.fn svg{margin-left:4px}.fo svg{fill:#6B6B6B}.fq{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fr{position:absolute}.fy{margin:0 24px}.gc{background:rgba(255, 255, 255, 1)}.gd{border:1px solid #F2F2F2}.ge{box-shadow:0 1px 4px #F2F2F2}.gf{max-height:100vh}.gg{overflow-y:auto}.gh{left:0}.gi{top:calc(100vh + 100px)}.gj{bottom:calc(100vh + 100px)}.gk{width:10px}.gl{pointer-events:none}.gm{word-break:break-word}.gn{word-wrap:break-word}.go:after{display:block}.gp:after{content:""}.gq:after{clear:both}.gr{line-height:1.23}.gs{letter-spacing:0}.gt{font-style:normal}.gu{font-weight:700}.hp{margin-bottom:-0.27em}.hq{line-height:1.394}.il{align-items:baseline}.im{width:48px}.in{height:48px}.io{border:2px solid rgba(255, 255, 255, 1)}.ip{z-index:0}.iq{box-shadow:none}.ir{border:1px solid rgba(0, 0, 0, 0.05)}.is{margin-left:-12px}.it{width:28px}.iu{height:28px}.iv{z-index:1}.iw{width:24px}.ix{margin-bottom:2px}.iy{flex-wrap:nowrap}.iz{font-size:16px}.ja{line-height:24px}.jc{margin:0 8px}.jd{display:inline}.je{color:rgba(102, 138, 170, 1)}.jf{fill:rgba(102, 138, 170, 1)}.ji{flex:0 0 auto}.jl{flex-wrap:wrap}.jo{white-space:pre-wrap}.jp{margin-right:4px}.jq{overflow:hidden}.jr{max-height:20px}.js{text-overflow:ellipsis}.jt{display:-webkit-box}.ju{-webkit-line-clamp:1}.jv{-webkit-box-orient:vertical}.jw{word-break:break-all}.jy{padding-left:8px}.jz{padding-right:8px}.la> *{flex-shrink:0}.lb{overflow-x:scroll}.lc::-webkit-scrollbar{display:none}.ld{scrollbar-width:none}.le{-ms-overflow-style:none}.lf{width:74px}.lg{flex-direction:row}.lh{z-index:2}.lk{-webkit-user-select:none}.ll{border:0}.lm{fill:rgba(117, 117, 117, 1)}.lp{outline:0}.lq{user-select:none}.lr> svg{pointer-events:none}.ma{cursor:progress}.mb{margin-left:4px}.mc{margin-top:0px}.md{opacity:1}.me{padding:4px 0}.mh{width:16px}.mj{display:inline-flex}.mp{max-width:100%}.mq{padding:8px 2px}.mr svg{color:#6B6B6B}.ni{clear:both}.no{margin-left:auto}.np{margin-right:auto}.nq{max-width:3028px}.nw{padding-top:5px}.nx{padding-bottom:5px}.nz{cursor:zoom-in}.oa{z-index:auto}.oc{height:auto}.od{margin-top:10px}.oe{max-width:728px}.oh{line-height:1.58}.oi{letter-spacing:-0.004em}.oj{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.pc{margin-bottom:-0.46em}.pd{text-decoration:underline}.pe{line-height:1.12}.pf{letter-spacing:-0.022em}.pg{font-weight:600}.pz{margin-bottom:-0.28em}.qf{max-width:1952px}.qg{font-style:inherit}.qh{font-style:italic}.qi{max-width:2052px}.qj{max-width:2996px}.qk{max-width:2520px}.ql{line-height:1.18}.qz{margin-bottom:-0.31em}.ra{list-style-type:decimal}.rb{margin-left:30px}.rc{padding-left:0px}.ri{max-width:3244px}.rj{max-width:3160px}.rk{max-width:886px}.rl{margin-bottom:26px}.rm{margin-top:6px}.rn{margin-top:8px}.ro{margin-right:8px}.rp{padding:8px 16px}.rq{border-radius:100px}.rr{transition:background 300ms ease}.rt{white-space:nowrap}.ru{border-top:none}.sa{height:52px}.sb{max-height:52px}.sc{box-sizing:content-box}.sd{position:static}.sf{max-width:155px}.sl{margin-right:20px}.sr{align-items:flex-end}.ss{width:76px}.st{height:76px}.su{border:2px solid #F9F9F9}.sv{height:72px}.sw{width:72px}.sx{margin-left:-16px}.sy{width:36px}.sz{height:36px}.ta{width:auto}.tb{stroke:#F2F2F2}.tc{color:#F2F2F2}.td{fill:#F2F2F2}.te{background:#F2F2F2}.tf{border-color:#F2F2F2}.tl{font-weight:500}.tm{font-size:24px}.tn{line-height:30px}.to{letter-spacing:-0.016em}.tp{margin-top:16px}.tq{height:0px}.tr{border-bottom:solid 1px #E5E5E5}.tx{margin-top:72px}.ty{padding:24px 0}.tz{margin-bottom:0px}.ua{margin-right:16px}.as:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.at:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.es:hover{background:rgba(90, 118, 144, 1)}.et:hover{border-color:rgba(90, 118, 144, 1)}.eu:hover{cursor:pointer}.fk:hover{color:#242424}.fl:hover{fill:#242424}.fp:hover svg{fill:#242424}.fs:hover{background-color:rgba(0, 0, 0, 0.1)}.jb:hover{text-decoration:underline}.jg:hover:not(:disabled){color:rgba(90, 118, 144, 1)}.jh:hover:not(:disabled){fill:rgba(90, 118, 144, 1)}.lo:hover{fill:rgba(8, 8, 8, 1)}.mf:hover{fill:#000000}.mg:hover p{color:#000000}.mi:hover{color:#000000}.ms:hover svg{color:#000000}.rs:hover{background-color:#F2F2F2}.tg:hover{background:#F2F2F2}.th:hover{border-color:#F2F2F2}.ti:hover{cursor:wait}.tj:hover{color:#F2F2F2}.tk:hover{fill:#F2F2F2}.bc:focus-within path{fill:#242424}.ln:focus{fill:rgba(8, 8, 8, 1)}.mt:focus svg{color:#000000}.ob:focus{transform:scale(1.01)}.ls:active{border-style:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.bv{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ee{font-size:14px}.ef{line-height:20px}.el{font-size:13px}.em{padding:5px 12px}.fg{display:flex}.fx{margin-bottom:68px}.gb{max-width:680px}.hl{font-size:42px}.hm{margin-top:1.19em}.hn{line-height:52px}.ho{letter-spacing:-0.011em}.id{font-size:22px}.ie{margin-top:0.92em}.if{line-height:28px}.ik{align-items:center}.km{border-top:solid 1px #F2F2F2}.kn{border-bottom:solid 1px #F2F2F2}.ko{margin:32px 0 0}.kp{padding:3px 8px}.ky> *{margin-right:24px}.kz> :last-child{margin-right:0}.lz{margin-top:0px}.mo{margin:0}.nn{max-width:1192px}.nv{margin-top:56px}.oy{font-size:20px}.oz{margin-top:2.14em}.pa{line-height:32px}.pb{letter-spacing:-0.003em}.pv{font-size:24px}.pw{margin-top:1.95em}.px{line-height:30px}.py{letter-spacing:-0.016em}.qe{margin-top:0.94em}.qw{margin-top:1.72em}.qx{line-height:24px}.qy{letter-spacing:0}.rh{margin-top:1.14em}.rz{margin-bottom:88px}.sk{display:inline-block}.sq{padding-top:72px}.tw{margin-top:40px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.ly{margin-top:0px}.of{margin-left:auto}.og{text-align:center}.sj{display:inline-block}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.lx{margin-top:0px}.si{display:inline-block}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.lv{margin-top:0px}.lw{margin-right:0px}.sh{display:inline-block}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.s{display:flex}.t{justify-content:space-between}.br{width:24px}.cb{margin:0 24px}.cq{height:40px}.cx{margin-bottom:44px}.dj{margin-bottom:32px}.dw{font-size:13px}.dx{line-height:20px}.eg{padding:0px 8px 1px}.ft{margin-bottom:4px}.gv{font-size:32px}.gw{margin-top:1.01em}.gx{line-height:38px}.gy{letter-spacing:-0.014em}.hr{font-size:18px}.hs{margin-top:0.79em}.ht{line-height:24px}.ig{align-items:flex-start}.jj{flex-direction:column}.jm{margin-bottom:2px}.ka{margin:24px -24px 0}.kb{padding:0}.kq> *{margin-right:8px}.kr> :last-child{margin-right:24px}.li{margin-left:0px}.lt{margin-top:0px}.lu{margin-right:0px}.mk{margin:0}.mu{border:1px solid #F2F2F2}.mv{border-radius:99em}.mw{padding:0px 16px 0px 12px}.mx{height:38px}.my{align-items:center}.na svg{margin-right:8px}.nj{max-width:100%}.nr{margin-top:40px}.ok{margin-top:1.56em}.ol{line-height:28px}.om{letter-spacing:-0.003em}.ph{font-size:20px}.pi{margin-top:1.2em}.pj{letter-spacing:0}.qa{margin-top:0.67em}.qm{font-size:16px}.qn{margin-top:1.23em}.rd{margin-top:1.34em}.rv{margin-bottom:80px}.sg{display:inline-block}.sm{padding-top:48px}.ts{margin-top:32px}.mz:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.bu{width:64px}.ce{margin:0 64px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.ec{font-size:14px}.ed{line-height:20px}.ej{font-size:13px}.ek{padding:5px 12px}.ff{display:flex}.fw{margin-bottom:68px}.ga{max-width:680px}.hh{font-size:42px}.hi{margin-top:1.19em}.hj{line-height:52px}.hk{letter-spacing:-0.011em}.ia{font-size:22px}.ib{margin-top:0.92em}.ic{line-height:28px}.ij{align-items:center}.ki{border-top:solid 1px #F2F2F2}.kj{border-bottom:solid 1px #F2F2F2}.kk{margin:32px 0 0}.kl{padding:3px 8px}.kw> *{margin-right:24px}.kx> :last-child{margin-right:0}.mn{margin:0}.nm{max-width:1192px}.nu{margin-top:56px}.ou{font-size:20px}.ov{margin-top:2.14em}.ow{line-height:32px}.ox{letter-spacing:-0.003em}.pr{font-size:24px}.ps{margin-top:1.95em}.pt{line-height:30px}.pu{letter-spacing:-0.016em}.qd{margin-top:0.94em}.qt{margin-top:1.72em}.qu{line-height:24px}.qv{letter-spacing:0}.rg{margin-top:1.14em}.ry{margin-bottom:88px}.sp{padding-top:72px}.tv{margin-top:40px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.w{display:flex}.x{justify-content:space-between}.bt{width:64px}.cd{margin:0 48px}.cs{height:48px}.cz{margin-bottom:52px}.dl{margin-bottom:48px}.ea{font-size:13px}.eb{line-height:20px}.ei{padding:0px 8px 1px}.fv{margin-bottom:68px}.fz{max-width:680px}.hd{font-size:42px}.he{margin-top:1.19em}.hf{line-height:52px}.hg{letter-spacing:-0.011em}.hx{font-size:22px}.hy{margin-top:0.92em}.hz{line-height:28px}.ii{align-items:center}.ke{border-top:solid 1px #F2F2F2}.kf{border-bottom:solid 1px #F2F2F2}.kg{margin:32px 0 0}.kh{padding:3px 8px}.ku> *{margin-right:24px}.kv> :last-child{margin-right:0}.mm{margin:0}.nl{max-width:100%}.nt{margin-top:56px}.oq{font-size:20px}.or{margin-top:2.14em}.os{line-height:32px}.ot{letter-spacing:-0.003em}.pn{font-size:24px}.po{margin-top:1.95em}.pp{line-height:30px}.pq{letter-spacing:-0.016em}.qc{margin-top:0.94em}.qq{margin-top:1.72em}.qr{line-height:24px}.qs{letter-spacing:0}.rf{margin-top:1.14em}.rx{margin-bottom:88px}.so{padding-top:72px}.tu{margin-top:40px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.u{display:flex}.v{justify-content:space-between}.bs{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dy{font-size:13px}.dz{line-height:20px}.eh{padding:0px 8px 1px}.fu{margin-bottom:4px}.gz{font-size:32px}.ha{margin-top:1.01em}.hb{line-height:38px}.hc{letter-spacing:-0.014em}.hu{font-size:18px}.hv{margin-top:0.79em}.hw{line-height:24px}.ih{align-items:flex-start}.jk{flex-direction:column}.jn{margin-bottom:2px}.kc{margin:24px 0 0}.kd{padding:0}.ks> *{margin-right:8px}.kt> :last-child{margin-right:8px}.lj{margin-left:0px}.ml{margin:0}.nb{border:1px solid #F2F2F2}.nc{border-radius:99em}.nd{padding:0px 16px 0px 12px}.ne{height:38px}.nf{align-items:center}.nh svg{margin-right:8px}.nk{max-width:100%}.ns{margin-top:40px}.on{margin-top:1.56em}.oo{line-height:28px}.op{letter-spacing:-0.003em}.pk{font-size:20px}.pl{margin-top:1.2em}.pm{letter-spacing:0}.qb{margin-top:0.67em}.qo{font-size:16px}.qp{margin-top:1.23em}.re{margin-top:1.34em}.rw{margin-bottom:80px}.sn{padding-top:48px}.tt{margin-top:32px}.ng:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="print">.se{display:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.jx{max-height:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ny{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style><meta id="dcngeagmmhegagicpcmpinaoklddcgon"><script async="true" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/js" data-rh="true"></script><script data-rh="true">window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-7JY7T788PK');</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script><script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/enterprise.js" data-rh="true"></script><script type="text/javascript" async="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/_r"></script><script type="text/javascript" async="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/_r(1)"></script><script type="text/javascript" async="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/_r(2)"></script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="l c"><div class="l m n o c" style="transform: translateY(-57px);"><div class="p q r s t u v w x i d y z"><a class="dt ag du be ak b am an ao ap aq ar as at s u w i d q dv z" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe7ff75290f8&amp;%7Efeature=LoOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;source=---two_column_layout_nav----------------------------------" rel="noopener follow">Open in app<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" fill="none" viewBox="0 0 10 10" class="ds"><path fill="currentColor" d="M.985 8.485a.375.375 0 1 0 .53.53zM8.75 1.25h.375A.375.375 0 0 0 8.75.875zM8.375 6.5a.375.375 0 1 0 .75 0zM3.5.875a.375.375 0 1 0 0 .75zm-1.985 8.14 7.5-7.5-.53-.53-7.5 7.5zm6.86-7.765V6.5h.75V1.25zM3.5 1.625h5.25v-.75H3.5z"></path></svg></a><div class="ab q"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><a class="be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" data-testid="headerSignUpButton" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------" rel="noopener follow">Sign up</a></span></p><div class="aw l"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------" rel="noopener follow">Sign in</a></span></p></div></div></div><div class="p q r ab ac"><div class="ab q ae"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="120" height="26" fill="none" viewBox="0 0 120 26" class="au av"><path fill="#000" d="m29.57 1.404.036-.008V1.12h-7.27l-6.75 15.979-6.75-15.98H1.003v.278l.035.008c1.327.302 2 .752 2 2.374v18.993c0 1.623-.676 2.073-2.003 2.374L1 25.153v.279h5.315v-.278l-.035-.008c-1.327-.302-2-.751-2-2.374V4.88l8.67 20.552h.492l8.924-21.125V23.24c-.114 1.282-.782 1.677-1.983 1.95l-.036.009v.275h9.259V25.2l-.036-.008c-1.203-.274-1.886-.67-2-1.95l-.006-19.464h.006c0-1.622.674-2.072 2-2.374m4.23 12.582c.15-3.412 1.367-5.875 3.41-5.918.629.01 1.157.219 1.568.62.872.852 1.282 2.634 1.219 5.298zm-.092.962h10.85v-.046c-.03-2.61-.78-4.64-2.228-6.033-1.25-1.204-3.103-1.867-5.048-1.867h-.043c-1.01 0-2.248.246-3.13.693a7.3 7.3 0 0 0-2.623 2.086c-1.185 1.479-1.903 3.477-2.078 5.724a14 14 0 0 0-.04.755q-.007.292-.001.587C29.484 21.934 32.213 26 37.059 26c4.254 0 6.73-3.132 7.348-7.336l-.312-.11c-1.085 2.259-3.034 3.628-5.252 3.461-3.028-.228-5.347-3.32-5.137-7.066m23.122 6.893c-.356.85-1.099 1.319-2.094 1.319s-1.905-.689-2.552-1.939c-.694-1.342-1.06-3.24-1.06-5.487 0-4.678 1.445-7.704 3.68-7.704.937 0 1.674.468 2.026 1.284zm7.198 3.335c-1.327-.316-2-.787-2-2.492V0l-8.062 2.392v.293l.05-.004c1.111-.09 1.866.064 2.304.472.343.32.51.809.51 1.498v3.11C56.033 7.25 55.088 7 53.94 7c-2.326 0-4.453.987-5.986 2.779-1.599 1.867-2.444 4.42-2.444 7.38 0 5.287 2.584 8.84 6.43 8.84 2.25 0 4.06-1.242 4.888-3.336v2.811h7.233v-.29zM70.94 3.085c0-1.65-1.236-2.896-2.875-2.896-1.632 0-2.908 1.272-2.908 2.896s1.278 2.896 2.908 2.896c1.64 0 2.875-1.245 2.875-2.896m1.903 22.092c-1.327-.316-2-.787-2-2.492h-.006V7.055l-7.234 2.092v.284l.043.004c1.566.14 1.994.683 1.994 2.525v13.515h7.24v-.29zm18.536 0c-1.327-.316-2-.787-2-2.492V7.055L82.49 9.078v.285l.04.004c1.28.136 1.65.71 1.65 2.56v9.88c-.426.85-1.227 1.356-2.196 1.39-1.573 0-2.439-1.07-2.439-3.012V7.055l-7.234 2.092v.284l.044.004c1.565.14 1.994.683 1.994 2.525v8.362a9.4 9.4 0 0 0 .15 1.741l.13.57C75.243 24.845 76.848 26 79.362 26c2.129 0 3.996-1.328 4.818-3.405v2.885h7.233v-.291zm28.102.298v-.291l-.035-.009c-1.44-.334-2.001-.964-2.001-2.248V12.295C117.445 8.98 115.597 7 112.5 7c-2.257 0-4.16 1.314-4.893 3.36-.582-2.168-2.257-3.36-4.734-3.36-2.175 0-3.88 1.156-4.612 3.11V7.056l-7.233 2.006v.286l.043.004c1.547.138 1.994.697 1.994 2.492v13.631h6.75v-.29l-.037-.01c-1.148-.271-1.519-.767-1.519-2.04V10.95c.304-.715.917-1.562 2.127-1.562 1.504 0 2.266 1.05 2.266 3.116v12.972h6.751v-.29l-.035-.01c-1.149-.271-1.52-.767-1.52-2.04V12.294a7 7 0 0 0-.095-1.21c.322-.777.97-1.696 2.23-1.696 1.524 0 2.265 1.02 2.265 3.116v12.972z"></path></svg></a><div class="aw h"><div class="ab ax ay az ba q bb bc"><div class="bl" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><div class="bm bn ab"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="ax bd be bf z bg bh bi bj bk" placeholder="Search" value=""></div></div></div><div class="h k w ff fg"><div class="fh ab"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerWriteButton" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=---two_column_layout_nav-----------------------new_post_topnav-----------" rel="noopener follow"><div class="be b bf z dt fi fj ab q fk fl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Write"><path fill="currentColor" d="M14 4a.5.5 0 0 0 0-1zm7 6a.5.5 0 0 0-1 0zm-7-7H4v1h10zM3 4v16h1V4zm1 17h16v-1H4zm17-1V10h-1v10zm-1 1a1 1 0 0 0 1-1h-1zM3 20a1 1 0 0 0 1 1v-1zM4 3a1 1 0 0 0-1 1h1z"></path><path stroke="currentColor" d="m17.5 4.5-8.458 8.458a.25.25 0 0 0-.06.098l-.824 2.47a.25.25 0 0 0 .316.316l2.47-.823a.25.25 0 0 0 .098-.06L19.5 6.5m-2-2 2.323-2.323a.25.25 0 0 1 .354 0l1.646 1.646a.25.25 0 0 1 0 .354L19.5 6.5m-2-2 2 2"></path></svg><div class="ds l">Write</div></div></a></span></div></div><div class="k j i d"><div class="fh ab"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSearchButton" href="https://medium.com/search?source=---two_column_layout_nav----------------------------------" rel="noopener follow"><div class="be b bf z dt fi fj ab q fk fl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Search"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div></a></div></div><div class="fh h k j"><div class="ab q"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><a class="be b dw dx eg dy dz eh ea eb ei ej ed ek el ef em eo ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" data-testid="headerSignUpButton" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------" rel="noopener follow">Sign up</a></span></p><div class="aw l"><p class="be b dw dx dy dz ea eb ec ed ee ef dt"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerSignInButton" href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=post_page---two_column_layout_nav-----------------------global_nav-----------" rel="noopener follow">Sign in</a></span></p></div></div></div><div class="l" aria-hidden="false"><button class="ax fm am ab q ao fn fo fp" aria-label="user options menu" data-testid="headerUserIcon"><div class="l fi"><img alt="" class="l fc bx by bz cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_dmbNkD5D-u45r44go_cf0g.png" width="32" height="32" loading="lazy" role="presentation"><div class="fq bx l by bz fr n ax fs"></div></div></button></div></div></div><div class="l"><div class="se" role="dialog" aria-modal="true" tabindex="-1"><div class="ub uc bg dv ud ue uf ao ug gl uh" aria-hidden="true" role="presentation"></div><div class="ui ud uj uk ul ub dv fc um un uo md up uq ur us ut uu uv uw ux" aria-hidden="true"></div></div><div class="ft fu fv fw fx l"><div class="ab ca"><div class="ch bg fy fz ga gb"></div></div><article><div class="l"><div class="l"><span class="l"></span><section><div><div class="fr gh gi gj gk gl"></div><div class="no np oe fi"><div class="speechify-ignore l h g f e"><aside class="ul fr n" style="width: 396.5px;"><div class="rm mp fr agf rt bg"><p class="be b du z dt"><span class="bl mp rt jq js">Top highlight</span></p></div></aside></div></div><div class="gm gn go gp gq"><div class="ab ca"><div class="ch bg fy fz ga gb"><div><h1 id="ac27" class="pw-post-title gr gs gt be gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl hm hn ho hp bj" data-testid="storyTitle" data-selectable-paragraph=""><strong class="al">Topic Modeling with Latent Dirichlet Allocation</strong></h1></div><div><h2 id="691b" class="pw-subtitle-paragraph hq gs gt be b hr hs ht hu hv hw hx hy hz ia ib ic id ie if cp dt" data-selectable-paragraph="">A practical exploration of the Natural Language Processing technique of Latent Dirichlet Allocation and its application to the task of topic modeling.</h2><div><div class="speechify-ignore ab co"><div class="speechify-ignore bg l"><div class="ig ih ii ij ik ab"><div><div class="ab il"><a href="https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div><div class="bl" aria-hidden="false" aria-describedby="1" aria-labelledby="1"><div class="l im in bx io ip"><div class="l fi"><img alt="Haaya Naushan" class="l fc bx dc dd cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_TFVRDzBGIrkHhqgct4QfkQ.jpg" width="44" height="44" loading="lazy" data-testid="authorPhoto"><div class="iq bx l dc dd fr n ir fs"></div></div></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="is ab fi"><div><div class="bl" aria-hidden="false" aria-describedby="2" aria-labelledby="2"><div class="l it iu bx io iv"><div class="l fi"><img alt="Towards Data Science" class="l fc bx bq iw cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_CJe3891yB1A1mzMdqemkdg.jpg" width="24" height="24" loading="lazy" data-testid="publicationPhoto"><div class="iq bx l bq iw fr n ir fs"></div></div></div></div></div></div></a></div></div><div class="bm bg l"><div class="ab"><div style="flex:1"><span class="be b bf z bj"><div class="ix ab q"><div class="ab q iy"><div class="ab q"><div><div class="bl" aria-hidden="false" aria-describedby="3" aria-labelledby="3"><p class="be b iz ja bj"><a class="af ag ah ai aj ak al am an ao ap aq ar jb" data-testid="authorName" href="https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow">Haaya Naushan</a></p></div></div></div><span class="jc jd" aria-hidden="true"><span class="be b bf z dt">·</span></span><p class="be b iz ja dt"><span><a class="je jf ah ai aj ak al am an ao ap aq ar ew jg jh" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=post_page-68f801f1b50b----e7ff75290f8---------------------post_header-----------" rel="noopener follow">Follow</a></span></p></div></div></span></div></div><div class="l ji"><span class="be b bf z dt"><div class="ab cm jj jk jl"><div class="jm jn ab"><div class="be b bf z dt ab jo"><span class="jp l ji">Published in</span><div><div class="l" aria-hidden="false" aria-describedby="4" aria-labelledby="4"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b bf z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div><div class="h k"><span class="jc jd" aria-hidden="true"><span class="be b bf z dt">·</span></span></div></div><span class="be b bf z dt"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="jy jz l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="be b bf z dt">·</span></span></div><span data-testid="storyPublishDate">Dec 2, 2020</span></div></span></div></span></div></div></div><div class="ab co ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp"><div class="h k w ff fg q"><div class="lf l"><div class="ab q lg lh"><div class="pw-multi-vote-icon fi jp li lj lk"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerClapButton" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7ff75290f8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=-----e7ff75290f8---------------------clap_footer-----------" rel="noopener follow"><div><div class="bl" aria-hidden="false" aria-describedby="5" aria-labelledby="5"><div class="ll ao lm ln lo lp am lq lr ls lk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l lt lu lv lw lx ly lz"><div><div class="bl" aria-hidden="false" aria-describedby="98" aria-labelledby="98"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at xu mi">241<span class="l h g f sj sk"></span></button></p></div></div></div></div></div><div><div class="bl" aria-hidden="false" aria-describedby="6" aria-labelledby="6"><button class="ao ll md me ab q fj mf mg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="mc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="be b du z dt"><span class="pw-responses-count mb mc">3</span></p></button></div></div></div><div class="ab q kq kr ks kt ku kv kw kx ky kz la lb lc ld le"><div class="mh k j i d"></div><div class="h k"><div><div class="bl" aria-hidden="false" aria-describedby="7" aria-labelledby="7"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="headerBookmarkButton" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7ff75290f8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=-----e7ff75290f8---------------------bookmark_footer-----------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div><div class="fc mj cm"><div class="l ae"><div class="ab ca"><div class="mk ml mm mn mo mp ch bg"><div class="ab"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3De7ff75290f8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=-----e7ff75290f8---------------------post_audio_button-----------" rel="noopener follow"><div><div class="bl" aria-hidden="false" aria-describedby="80" aria-labelledby="80"><button aria-label="Listen" data-testid="audioPlayButton" class="af fj ah ai aj ak al mq an ao ap ew mr ms mg mt mu mv mw mx s my mz na nb nc nd ne u nf ng nh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"></path></svg><div class="j i d"><p class="be b bf z dt">Listen</p></div></button></div></div></a></span></div></div></div></div></div><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false" aria-describedby="9" aria-labelledby="9"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af fj ah ai aj ak al mq an ao ap ew mr ms mg mt mu mv mw mx s my mz na nb nc nd ne u nf ng nh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg><div class="j i d"><p class="be b bf z dt">Share</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="ni"><div class="ab ca"><div class="mk nj ml nk mm nl ce nm cf nn ch bg"><figure class="nr ns nt nu nv ni nw nx paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np nq"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:2000/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:2000/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 2000w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 1000px"><img alt="" class="bg mp oc c" width="1000" height="406" loading="eager" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_yHnDOAR2LBJJ6hHws00vWg.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">The LDA model graphically represented with plate notation. Image by Author.</figcaption></figure></div></div></div><div class="ab ca"><div class="ch bg fy fz ga gb"><p id="6d4b" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Topic modeling is a form of <a class="af pd" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank">unsupervised machine learning</a> that allows for efficient processing of large collections of data, while preserving the statistical relationships that are useful for tasks such as classification or summarization. The goal of topic modeling is to uncover latent variables that govern the semantics of a document, these latent variables representing abstract topics. Currently, the most popular technique for topic modeling is Latent Dirichlet Allocation (LDA), and this model can be used effectively on a variety of document types such as collections of news articles, policy documents, social media posts or tweets.</p><p id="64f9" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">This article will necessarily and briefly mention precursive topic modeling techniques, such as Latent Semantic Indexing (LSI, also referred to interchangeably as Latent Semantic Analysis/LSA) and probabilistic Latent Semantic Indexing (pLSI). The main focus will be a discussion of the LDA model, with an emphasis on understanding the role of hyperparameters and the challenge of inference. Next, I offer a practical introduction to implementation, covering dataset requirements, fine-tuning, and evaluation. Lastly, I conclude with a discussion of the limitations of the LDA model.</p><h1 id="e31f" class="pe pf gt be pg ph pi ht pj pk pl hw pm pn po pp pq pr ps pt pu pv pw px py pz bj" data-selectable-paragraph=""><strong class="al">Foundation of modern Topic modeling</strong></h1><p id="27f8" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">A brief history and a theoretical understanding of the foundational techniques that preceded LDA will explain the importance of the improvements made by this modern technique. Back in the 1980’s the field of <a class="af pd" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">information retrieval</a> produced a text representation scheme called<a class="af pd" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> term frequency–inverse document frequency (tf-idf)</a> for applying a numerical statistic to a word that would be representative of its importance within a document (Salton and McGill, 1983).</p><p id="e04d" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Tf-idf vectorization is where a normalized term frequency (count of the number of occurrences of a term within a document) is compared to the normalized inverse document frequency (count of the number of occurrences of a term within a corpus) on a log scale. This results in a term-by-document weight matrix that is unfortunately very sparse, noisy and redundant. The process to calculate the tf-idf vector for any word in a document can be represented by the equation below.</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np qf"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxZnBpfi45tp_8AluCMW_A.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*dxZnBpfi45tp_8AluCMW_A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*dxZnBpfi45tp_8AluCMW_A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*dxZnBpfi45tp_8AluCMW_A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*dxZnBpfi45tp_8AluCMW_A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*dxZnBpfi45tp_8AluCMW_A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*dxZnBpfi45tp_8AluCMW_A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*dxZnBpfi45tp_8AluCMW_A.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="315" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_dxZnBpfi45tp_8AluCMW_A.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph=""><em class="qg">Figure 1. Equation to calculate a tf-idf vector for a term </em>w<em class="qg">ᵢ,ⱼ. Image by Author.</em></figcaption></figure><h1 id="9bbf" class="pe pf gt be pg ph pi ht pj pk pl hw pm pn po pp pq pr ps pt pu pv pw px py pz bj" data-selectable-paragraph=""><strong class="al">Latent Semantic Indexing</strong></h1><p id="f741" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">Using tf-idf as a <a class="af pd" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">dimensionality reduction</a> technique identifies discriminative words for a collection of documents; however, it does not expose the inter- or intra- document statistical structure. Therefore, <a class="af pd" href="https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9" rel="noopener ugc nofollow" target="_blank">Latent Semantic Indexing (Deerwester et al., 1990)</a> was introduced as a way of overcoming the deficits of tf-idf. By using <a class="af pd" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">singular value decomposition (SVD)</a> to accomplish dimensionality reduction of the term-document matrix, it is possible to identify the linear subspace within the tf-idf features that captures most of the variance in a collection of documents, allowing LSI to be a generative rather than discriminative process.</p><p id="7ab2" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The SVD process can be visualized by the diagram below, where term-document matrix <em class="qh">A</em> is factored into a product of three matrices: <em class="qh">U</em>, <em class="qh">S</em>, and <em class="qh">V</em><strong class="oj gu">T</strong>. The rows of <em class="qh">U</em> represent document vectors in terms of topics and the rows of <em class="qh">V</em> represent term vectors in terms of topics.</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np qi"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyeNINhenigBdJt4ArmLIA.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*VyeNINhenigBdJt4ArmLIA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*VyeNINhenigBdJt4ArmLIA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*VyeNINhenigBdJt4ArmLIA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*VyeNINhenigBdJt4ArmLIA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*VyeNINhenigBdJt4ArmLIA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*VyeNINhenigBdJt4ArmLIA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*VyeNINhenigBdJt4ArmLIA.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="321" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_VyeNINhenigBdJt4ArmLIA.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 2. Singular value decomposition of term-document matrix <em class="qg">A</em>, where <em class="qg">U</em> is an orthogonal document-topic matrix, <em class="qg">S</em> is a diagonal matrix of singular values of <em class="qg">A</em>, and <em class="qg">V</em> is the transpose of the orthogonal and represents a term-topic matrix. Image by Author.</figcaption></figure><p id="a186" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">LSI, however, requires a special type of SVD, specifically a <a class="af pd" href="https://en.wikipedia.org/wiki/Low-rank_approximation" rel="noopener ugc nofollow" target="_blank">low-rank approximation</a> of the matrix A is required for efficiency, therefore truncation based on the <a class="af pd" href="https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_Frobenius_norm)" rel="noopener ugc nofollow" target="_blank">Eckart-Young Theorem</a> (specifically the proof for the Frobenius norm) is used to build an approximate matrix. In application, a value <em class="qh">k</em> is the hyperparameter used to represent the number of topics desired, and <a class="af pd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank">truncated SVD</a> is used to select only the <em class="qh">k</em> columns of <em class="qh">U</em> and <em class="qh">V</em>. The selected singular values from the diagonal <em class="qh">S</em> matrix are represented by “σ<em class="qh">ₗ</em>…σ<em class="qh">ₖ</em>”, where σ<em class="qh">ₖ </em>≥ 0 and σ<em class="qh">ₗ</em> has the highest importance. Truncated SVD of matrix A, can be visualized as follows:</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np qj"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="231" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_kNtvMOUrzhgBaeT9kUQ_Wg.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 3. Latent Semantic Indexing, where term-document matrix A is transformed by truncated singular value decomposition. Image by Author.</figcaption></figure><p id="ba97" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The derived features of LSI are linear combinations of the original tf-idf features, thereby capturing some aspects of linguistic notions of synonymy and <a class="af pd" href="https://en.wikipedia.org/wiki/Polysemy" rel="noopener ugc nofollow" target="_blank">polysemy</a>. Nonetheless, LSI unfortunately requires a very large collection of documents and terms, in addition to lacking interpretability with regards to topic content and sentiment.</p><h1 id="3915" class="pe pf gt be pg ph pi ht pj pk pl hw pm pn po pp pq pr ps pt pu pv pw px py pz bj" data-selectable-paragraph=""><strong class="al">Probabilistic Latent Semantic Indexing</strong></h1><p id="3cc4" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">Instead of relying solely on truncated SVD, a probabilistic model of LSI was introduced as an alternative, referred to as pLSI (<a class="af pd" href="https://www.researchgate.net/publication/2941307_Probabilistic_Latent_Semantic_Indexing" rel="noopener ugc nofollow" target="_blank">Hoffman, 1999</a>). Each word in a document is sampled from a mixture model, where the mixture components are multinomial random variables that can be viewed as representative of topics. Hence, each word is generated from a single topic, and different words within a document can be generated from different topics. Essentially, each document is reduced to a probability distribution on a fixed set of topics. This probabilistic approach can be modeled by a set of equivalent equations representing the joint probability of a document with a word, P(D,W). Furthermore, as seen in the diagram below, these equations can be derived from the truncated SVD model of LSI.</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np qk"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-9XsHgwFA1YxCQpXKJj15Q.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="265" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_-9XsHgwFA1YxCQpXKJj15Q.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 4. Pair of equivalent joint probability equations, where the generative process starting with a topic P(<em class="qg">z</em>) is representative of a probabilistic model of matrix A from LSI. Image by Author.</figcaption></figure><p id="7e99" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">In the set of equations above for P(D,W), the joint probability parameters are multinomial distributions that can be trained by an <a class="af pd" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">expectation-maximization algorithm (EM algorithm)</a> for inference of parameter estimates which depend on unobserved, latent variables. Importantly, pLSI only applies a probabilistic treatment to topics and words, not documents. This leads to two problems, firstly the number of parameters for pLSI grows linearly with the size of the corpus, so it is prone to overfitting. Secondly, there are no parameters to model P(D), so it is not evident how probability would be assigned to a new document. These issues led to the development of the LDA model, which allowed for better generalization and will be discussed in the following section.</p><h1 id="cf4f" class="pe pf gt be pg ph pi ht pj pk pl hw pm pn po pp pq pr ps pt pu pv pw px py pz bj" data-selectable-paragraph=""><strong class="al">Latent Dirichlet Allocation (LDA)</strong></h1><p id="d93d" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">LDA has roots in evolutionary biology; back in<a class="af pd" href="https://www.genetics.org/content/155/2/945" rel="noopener ugc nofollow" target="_blank"> 2000 researchers developed this model for the study of population genetics</a>. A few years later, LDA was applied to the field of machine learning by<a class="af pd" href="https://dl.acm.org/doi/10.5555/944919.944937" rel="noopener ugc nofollow" target="_blank"> Blei et al., 2003</a>, a group that includes the renowned Andrew Ng. <mark class="aeu aev ao">LDA is a generative probabilistic model, specifically it is a three-level </mark><mark class="aeu aev ao"><a class="af pd" href="https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling" rel="noopener ugc nofollow" target="_blank">hierarchical Bayesian model</a></mark><mark class="aeu aev ao">, for a collection of discrete data (such as a text corpora).</mark> LDA can be thought of as a Bayesian version of pLSI, that overcomes the weakness of the latter and thus allows for better generalization.</p><p id="24af" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The theoretical underpinnings of LDA rely on exploiting the concepts of <a class="af pd" href="https://en.wikipedia.org/wiki/Exchangeable_random_variables" rel="noopener ugc nofollow" target="_blank">exchangeability</a> with the <a class="af pd" href="https://en.wikipedia.org/wiki/De_Finetti%27s_theorem" rel="noopener ugc nofollow" target="_blank">de Finetti theorem</a> (1990). Exchangeability is a major simplifying assumption of text processing that allows for computationally-efficient methods. Both LSI and pLSI and based on the fundamental probability assumption described by the “<a class="af pd" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">bag-of-words</a>” method whereby the order of words in a document can be ignored. This assumption of exchangeability extends to the treatment of documents, where one can assume that the specific order of documents in a corpus is not an important consideration.</p><p id="017b" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">According to de Finetti’s theorem, exchangeable observations are conditionally independent relative to a latent variable, therefore an <a class="af pd" href="https://en.wikipedia.org/wiki/Probability_interpretations#Logical,_epistemic,_and_inductive_probability" rel="noopener ugc nofollow" target="_blank">epistemic probability</a> can be assigned to the latent variable. Furthermore, any collection of exchangeable random variables, also referred to as an exchangeable sequence of <a class="af pd" href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="noopener ugc nofollow" target="_blank">Bernoulli random variables</a>, has a representation as a “mixture” distribution, specifically a mixture of sequences of <a class="af pd" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">independent and identically distributed (<em class="qh">i.i.d.</em>)</a> Bernoulli random variables. The implication of a <a class="af pd" href="https://en.wikipedia.org/wiki/Mixture_model" rel="noopener ugc nofollow" target="_blank">mixture model</a> is the possibility of probabilistically representing the presence of sub-populations within an overall population without requiring an observed dataset to identify the sub-populations. Essentially, utilizing de Finetti’s theorem, it is possible to capture significant intra-document statistical structure via the <a class="af pd" href="https://en.wikipedia.org/wiki/Mixture_distribution" rel="noopener ugc nofollow" target="_blank">mixture distribution</a>.</p><h2 id="005d" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Unpacking the LDA model</strong></h2><p id="b40e" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">The innovation of LDA is in using Dirichlet <a class="af pd" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank">priors</a> for document-topic and term-topic distributions, thereby allowing for <a class="af pd" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank">Bayesian inference</a> over a three-level hierarchical model, the third layer being the distinguishing feature in a comparison to a simple Dirichlet multinomial clustering model. With regards to Bayesian inference, <a class="af pd" href="https://en.wikipedia.org/wiki/Plate_notation" rel="noopener ugc nofollow" target="_blank">plate notation</a> is an intuitive method of graphically representing variables that repeat; a “plate” (ie. box) is used to represent replicates, and edges denote conditional dependencies. As seen in the diagram below, the outer plate represents documents, and the inner plate represents repeated choices of topics and words within a document.</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np nq"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*yHnDOAR2LBJJ6hHws00vWg.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="284" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_yHnDOAR2LBJJ6hHws00vWg(1).jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 5. Probabilistic graphical representation of the LDA model with plate notation. Image by Author.</figcaption></figure><p id="999f" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Dirichlet distributions allow for probability distribution sampling over a <a class="af pd" href="https://en.wikipedia.org/wiki/Simplex#Probability" rel="noopener ugc nofollow" target="_blank">probability simplex</a> in which all the numbers add up to 1, and these numbers represent probabilities over <em class="qh">K</em> distinct categories. A <em class="qh">K</em>-dimensional Dirichlet distribution has <em class="qh">k</em>-parameters and represents uncertainty as a probability distribution. The Dirichlet prior parameters 𝛂 and 𝛃 are corpus-level parameters that are sampled once in the process of generating a corpus, and parameter 𝚹<em class="qh">ₘ</em><strong class="oj gu"> </strong>is a document-level variable that is sampled once per document. Variables <em class="qh">zₘₙ</em><strong class="oj gu"> </strong>and<strong class="oj gu"> </strong><em class="qh">wₘₙ</em><strong class="oj gu"> </strong>are word-level variables that are sampled once for each word in each document. Lastly, 𝛗<em class="qh">ₖ</em> represents the word probability distribution for a topic <em class="qh">k</em>.</p><h2 id="7c18" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Procedure and corpus probability</strong></h2><p id="249a" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">With the LDA model, documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. This generative process for each document within a corpus can be written simply as follows:</p><p id="dbb4" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Draw each topic parameter 𝛃<em class="qh">ₖ </em>~ Dirichlet (𝛗), for <em class="qh">k</em> 𝞊 {1…K}</p><p id="475d" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">For each document:</p><ol class=""><li id="bde0" class="oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc ra rb rc bj" data-selectable-paragraph="">Choose the topic distribution 𝜭<em class="qh">ₘ</em> ~ Dirichlet (𝜶)</li><li id="c150" class="oh oi gt oj b hr rd ol om hu re oo op oq rf os ot ou rg ow ox oy rh pa pb pc ra rb rc bj" data-selectable-paragraph="">For each of the <em class="qh">N</em> words <em class="qh">wₙ</em> :</li></ol><p id="6379" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">a) Choose a topic <em class="qh">zₘₙ</em> ~ Multinomial(𝜭<em class="qh">ₘ</em>)</p><p id="383f" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">b) Choose a word <em class="qh">wₙ </em>~ Multinomial(𝛃<em class="qh">ₖ</em>)</p><p id="f76f" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The probability of a corpus (<em class="qh">D</em>) is the result of taking the product of the marginal probabilities of single documents, and the marginal distribution for a single document is obtained by integrating over 𝜭 and summing over <em class="qh">z</em> topics.</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np ri"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*iQzGRURGlGbLyPjkjeJUuA.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="108" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_iQzGRURGlGbLyPjkjeJUuA.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 6. From the LDA model, the probability of a corpus <em class="qg">D </em>is modeled by the probability equation P(D|𝜶,𝛃). Image by Author.</figcaption></figure><p id="07ed" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Understanding the importance of Dirichlet distributions in LDA requires an understanding of <a class="af pd" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">Bayes’ Theorem</a> which states that, if the prior is Dirichlet distributed (𝛂, 𝛃) and the likelihood is multinomial distributed (z<em class="qh">ₘₙ</em><strong class="oj gu">, </strong>w<em class="qh">ₘₙ</em><strong class="oj gu">)</strong>, the posterior will be Dirichlet distributed and can therefore be computed. Nevertheless, the posterior distribution is intractable for exact inference, and there are several approximating inference algorithms such as <a class="af pd" href="https://en.wikipedia.org/wiki/Laplace%27s_method" rel="noopener ugc nofollow" target="_blank">Laplace approximation</a>, <a class="af pd" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">Markov chain Monte Carlo</a> (eg. <a class="af pd" href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="noopener ugc nofollow" target="_blank">Gibbs sampling</a>) and my choice of a <a class="af pd" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank">variational Bayes algorithm</a> which will be discussed later in the implementation section covering inference.</p><h1 id="675e" class="pe pf gt be pg ph pi ht pj pk pl hw pm pn po pp pq pr ps pt pu pv pw px py pz bj" data-selectable-paragraph=""><strong class="al">Implementing the LDA model</strong></h1><p id="d4ce" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">A reasonably large dataset is required to build a LDA model. Minimum necessary size is dependent on the characteristics and average length of the documents. Generally, the larger the dataset the better the results, due to an increase in observations. For example, when working with news articles, the minimum number of articles required for modeling is 600; however, results improve when the dataset includes more than 1,000 articles. For tweets, the minimum size depends on the heterogeneity of the conversation, and since tweets have a short sequence length, the minimum dataset size is significantly larger than what is required for news articles. A dataset of 5,000 to 10,000 tweets is sufficient for modelling, and results improve at a slower rate compared to news articles when the dataset grows. For policy documents, which have a longer average length compared to news articles, fewer samples are required to build a model; minimum size depends on the scope of the collection.</p><p id="7ac4" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The following four sections will cover the details necessary to successfully implement LDA for topic modeling of text data. The first section provides a description of select hyperparameters of the LDA model, including the Dirichlet priors. Secondly, it is important to consider the problem of inference; specifically, I focus on the solution of a variational Bayes algorithm. Following that, I provide an explanation of how to heuristically fine-tune the Dirichlet prior parameters with empirical experiments (ie. without relying on an EM algorithm for estimations, the method used in the introductory paper by Blei et al., 2003). Lastly, a practical introduction to a method for model evaluation that relies on a <a class="af pd" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank">t-distributed stochastic neighbor embedding (t-SNE)</a> visualization and perplexity scores.</p><h2 id="9539" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Hyperparameters of the LDA model</strong></h2><p id="932d" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">There are several Python libraries with LDA modules. Currently, I prefer using <a class="af pd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank">Sci-Kit Learn</a> (sklearn), though <a class="af pd" href="https://radimrehurek.com/gensim/models/ldamodel.html" rel="noopener ugc nofollow" target="_blank">Gensim</a> is a very popular choice with a multicore option for parallelization of LDA. Regardless of the choice of package, the hyperparameters remain the same, so the following discussion will be general in nature.</p><p id="68ea" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">When implementing LDA it is necessary to fine-tune the hyperparameters, specifically the number of topics (<em class="qh">k</em>), the number of features (<em class="qh">V</em>, ie. fixed vocabulary size), and the 𝛂 and 𝛃 Dirichlet prior parameters. It is possible to<a class="af pd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank"> fine-tune other parameters such as the number of iterations, the learning method (batch or online), the learning offset, perplexity tolerance, and others</a>. For the sake of practicality, however, I will focus on the four parameters first mentioned. In my experience, working with a variety of datasets ranging from tweets (short and informal) to policy documents (long and formal), I have found that the best approach is to choose hyperparameters heuristically and then refine them with empirical experiments.</p><h2 id="30e9" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Number of topics (<em class="qg">k</em>)</strong></h2><p id="5853" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">Often, the most important hyperparameter is the number of topics, the choice of which depends on the characteristics and size of the dataset. For example, the larger the dataset the greater the number of topics, only if the dataset is representative of a diverse collection. However, a collection of a few thousand scientific articles on a particular subject, might not contain more topics if several thousand similar articles are added to the initial dataset.</p><p id="efed" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The heuristic approach is to leverage knowledge of the content of the dataset to estimate a probable target, and make adjustments based on model evaluation and dimensionality reduction based visualizations. The optimal number of topics can be determined by calculating <a class="af pd" href="https://radimrehurek.com/gensim/models/coherencemodel.html" rel="noopener ugc nofollow" target="_blank">topic coherence </a>scores over a range of topic numbers and <a class="af pd" rel="noopener" target="_blank" href="https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0">plotting the resultant topic coherence trend</a>. However, this is a computationally expensive task (ie. very time-intensive), therefore it is far more efficient to heuristically estimate a starting value for <em class="qh">k</em> and use model evaluation techniques to empirically guide adjustments.</p><h2 id="46f7" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Number of features (<em class="qg">V</em>)</strong></h2><p id="cd8f" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">The same approach can be used for choosing the number of features, which is equivalent to setting a fixed size for the vocabulary. The greater the number of features, the longer the LDA model will take to train; however, a sufficiently-sized vocabulary is necessary to capture the most important words for clustering of topics. Generally, setting the number of features to 10,000 is a good starting point for most models. This value needs to be fine-tuned depending on the size of the dataset and the amount of diversity of the words in the collection.</p><p id="fc65" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Importantly, increasing the number of features has diminishing returns on clustering accuracy. For example, if the total vocabulary for a dataset is 20,000 words, setting the number of features to 10,000 will definitely capture the vast majority of important words. On the other hand, assuming a total vocabulary of 100,000 words, increasing the number of features to 15 or 20,000 is enough to capture the majority of important words, since ordering the 100,000 words by tf-idf vectorization values will result in only a small percent being feature relevant.</p><h2 id="f6f5" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">The 𝛂 and 𝛃 hyperparameters</strong></h2><p id="9593" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">Most LDA models assume <a class="af pd" href="https://en.wikipedia.org/wiki/Symmetric_probability_distribution" rel="noopener ugc nofollow" target="_blank">symmetric distribution</a>, and the 𝛂 and 𝛃 parameters act as prior to the posterior calculation. This assumption of symmetry would mean that each topic is evenly distributed throughout a document, whereas for an asymmetric distribution (as measured by <a class="af pd" href="https://en.wikipedia.org/wiki/Skewness" rel="noopener ugc nofollow" target="_blank">skewness</a>) certain topics would be favoured over others. As Dirichlet prior concentration parameters, 𝛂 and 𝛃, are representative of document-topic density and topic-word density respectively.</p><p id="4b60" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">The 𝛂 parameter will specify prior beliefs about topic sparsity and uniformity; visualized as a matrix each row is a document and each column is a topic. With a high 𝛂 value, documents are assumed to contain more topics. Essentially, this means that each document is likely to contain a mix of many topics and not a single topic specifically. Conversely, a low 𝛂 value assumes that a document will contain a mixture of just a few or a single topic. This happens because as the value of 𝛂<em class="qh"> </em>decreases, sparsity increases such that, when the distribution is sampled, most values will be zero or close to zero.</p><p id="73f4" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Additionally, if the distributions are asymmetrical, a high 𝛂 value results in a more specific topic distribution per document. Initially, the 𝛂 parameter can be set to a real number value divided by the number of topics, and the results should reveal a sense of the sparsity and symmetry of the distribution. Therefore, the heuristic approach for choosing an 𝛂 value, is to estimate the topical sparsity of each document on average. Subsequent adjustments should be determined by model evaluation and then tested empirically.</p><p id="47aa" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">As mentioned, the 𝛃 parameter represents topic-word density, and it is a matrix where each row represents a topic and each column represents a word. The 𝛃 parameter will specify prior beliefs about word sparsity and uniformity within topics, adjusting for bias that certain topics will favour certain words. With a high 𝛃 value, topics are assumed to be made up of most words in the fixed-sized vocabulary and this results in a more specific word mixture for each topic.</p><p id="5a2a" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Conversely, with a low 𝛃 value, a topic may contain a mixture of just a few of the words in the fixed-sized vocabulary. Furthermore, if the distribution is asymmetrical, a high 𝛃 value will result in a more specific word distribution. The topics, however, will be more similar in terms of words contained. Generally, it is sufficient to set the 𝛃 value to 0.01 which is the value commonly used when the word distribution is sparse (usually true); however, it may be necessary to adjust this parameter.</p><h2 id="911d" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Variational Bayesian Inference</strong></h2><p id="925f" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">By definition, <a class="af pd" href="https://en.wikipedia.org/wiki/Bayesian_inference" rel="noopener ugc nofollow" target="_blank">Bayesian inference</a> derives the <a class="af pd" href="https://en.wikipedia.org/wiki/Posterior_probability" rel="noopener ugc nofollow" target="_blank">posterior probability</a> as a consequence of two antecedents: a<a class="af pd" href="https://en.wikipedia.org/wiki/Prior_probability" rel="noopener ugc nofollow" target="_blank"> prior probability</a> and a <a class="af pd" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">likelihood function</a> derived from a statistical model for the observed data. With regards to LDA, the problem is how to compute the posterior distribution of the hidden variables, given a document. Normalizing the posterior distribution by marginalizing over the hidden variables results in a function which is intractable, due to the coupling of 𝚹 and 𝛃 in summation over latent topics. This joint probability can be modeled with plate notation as seen in Figure 5, and expressed with the following equation:</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np rj"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="156" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_ZDOV4InS_AhvFeKcQUsy_Q.jpg"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 7. Joint probability equation for the posterior distribution of a LDA model. Image by Author.</figcaption></figure><p id="5001" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">In this form, it is clear that integrating over 𝚹, 𝛃 is intractable, and as mentioned earlier, this makes exact inference of the posterior distribution very difficult. One solution is to use variational inference, specifically <a class="af pd" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank">variational Bayesian method</a>s that allow for approximating intractable integrals arising from Bayesian inference. Variational inference uses <a class="af pd" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" rel="noopener ugc nofollow" target="_blank">Jensen’s inequality</a> to obtain an adjustable lower bound on the log likelihood; therefore, variational parameters are chosen by an optimization procedure aimed at finding the tightest possible lower bound. The optimizing values of the variational parameters are found by minimizing the <a class="af pd" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">Kullback-Leibler (KL) divergence</a> between the variational distribution and the true posterior distribution. I recommend this <a class="af pd" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" rel="noopener ugc nofollow" target="_blank">blog post about KL divergence</a> for an understanding of the role this dissimilarity measure plays in Bayesian inference.</p><p id="22ce" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">A variational Bayes algorithm provides a locally-optimal exact-analytical solution to an approximation of the posterior distribution, in other words, it is an extension of the EM algorithm. The advantage of this approach over other options is the speed; therefore, I choose to use SciKit-Learn’s LDA module which by default relies on a variational Bayes algorithm for inference. A slower, but nonetheless popular alternative is to use the <a class="af pd" href="http://mallet.cs.umass.edu/" rel="noopener ugc nofollow" target="_blank">Java-based library MALLET</a>, which offers an optimized version of <a class="af pd" href="https://dl.acm.org/doi/abs/10.1145/1401890.1401960" rel="noopener ugc nofollow" target="_blank">collapsed Gibbs sampling</a>, that can also be accessed with a <a class="af pd" href="https://radimrehurek.com/gensim/models/wrappers/ldamallet.html" rel="noopener ugc nofollow" target="_blank">Python wrapper through Gensim</a>.</p><h2 id="7576" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Fine tuning the 𝛂 and 𝛃 hyperparameters</strong></h2><p id="e4f4" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">Based on my experience, generalized for working with any dataset, the guidelines for a heuristic approach are as follows:</p><ol class=""><li id="916f" class="oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc ra rb rc bj" data-selectable-paragraph="">Given knowledge of the topics, is it expected that the distribution of topics in each document will be sparse, such that each document contains only a few topics? If yes, then choose an 𝛂 &lt; 1</li><li id="1553" class="oh oi gt oj b hr rd ol om hu re oo op oq rf os ot ou rg ow ox oy rh pa pb pc ra rb rc bj" data-selectable-paragraph="">Given knowledge of the total vocabulary, is it expected that the distribution of words in each topic will be sparse, such that certain topics favour certain words? If yes, then choose a 𝛃 &lt; 1</li></ol><p id="0dd9" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Alternately, if knowledge of the dataset is limited or if the distribution is asymmetrical, it is possible to empirically rely on model evaluation to inform fine tuning of the 𝛂 and 𝛃 parameters. This is accomplished by calculating <a class="af pd" href="https://en.wikipedia.org/wiki/Perplexity" rel="noopener ugc nofollow" target="_blank">perplexity scores</a> and adjusting for sparsity. The general procedure can be described by the following steps:</p><ol class=""><li id="3c23" class="oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc ra rb rc bj" data-selectable-paragraph="">Choose an 𝛂<em class="qh">ₘ</em> value from [0.05, 0.1, 0.5, 1, 5, 10, 50]</li><li id="0851" class="oh oi gt oj b hr rd ol om hu re oo op oq rf os ot ou rg ow ox oy rh pa pb pc ra rb rc bj" data-selectable-paragraph="">Choose a 𝛃<em class="qh">ₘ</em> value from [0.01, 0.05, 0.1, 0.5, 1, 5, 10]</li><li id="761c" class="oh oi gt oj b hr rd ol om hu re oo op oq rf os ot ou rg ow ox oy rh pa pb pc ra rb rc bj" data-selectable-paragraph="">Train the LDA model with 𝛂<em class="qh">ₘ</em> and 𝛃<em class="qh">ₘ</em>, while keeping all other parameters constant</li><li id="551a" class="oh oi gt oj b hr rd ol om hu re oo op oq rf os ot ou rg ow ox oy rh pa pb pc ra rb rc bj" data-selectable-paragraph="">Calculate model perplexity score on holdout data</li><li id="4417" class="oh oi gt oj b hr rd ol om hu re oo op oq rf os ot ou rg ow ox oy rh pa pb pc ra rb rc bj" data-selectable-paragraph="">Chose (𝛂<em class="qh">ₘ</em>, 𝛃<em class="qh">ₘ</em>) pair with the minimum perplexity score</li></ol><p id="cc6c" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Personally, I favour combining the heuristic approach with empirical experiments. Calculating model perplexity scores is a method of model evaluation, which will be discussed further in the following section.</p><h2 id="8d55" class="ql pf gt be pg qm qn dx pj qo qp dz pm oq qq qr qs ou qt qu qv oy qw qx qy qz bj" data-selectable-paragraph=""><strong class="al">Model Evaluation</strong></h2><p id="58e6" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">The challenge with many machine learning models, including the LDA model is how to interact with the high-dimensional data in a meaningful way that is interpretable for humans. Therefore, my primary method of evaluation is to use <a class="af pd" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank">t-distributed Stochastic Neighbor Embedding (t-SNE)</a> as a tool to visualize the high-dimensional data. This dimensionality reduction technique was introduced by Laurens van der Maaten and Geoffrey Hinton in 2008. The best resource I have found for implementing t-SNE is the <a class="af pd" href="https://lvdmaaten.github.io/tsne/" rel="noopener ugc nofollow" target="_blank">personal blog of Laurens van der Maaten</a>; the FAQ section on the t-SNE page in particular, offers valuable tips for understanding the visualization.</p><p id="5c85" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">Essentially, the t-SNE technique works to convert similarities between data points to joint probabilities, and then tries to minimize the KL divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. The dimensionality of the LDA model is determined by the number of features set during training (usually a minimum of 10,000); therefore, t-SNE can be used to reduce the dimensions to a 2-D embedding that offers a visualization of the clustering determined by LDA. To interpret the t-SNE, a simple visual evaluation of the clusters offers insight, as does the KL divergence score, for which the value closest to 0 is optimal. In the image below, created with the Python plotting library Bokeh and a dataset of 67,000 tweets, the differently coloured clusters represent the abstract topics, and positioning is determined by the dimensionality reduction algorithm.</p><figure class="nr ns nt nu nv ni no np paragraph-image"><div role="button" tabindex="0" class="ny nz fi oa bg ob"><div class="no np rk"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pGBGt7y662YNbbwGccqvkQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*pGBGt7y662YNbbwGccqvkQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pGBGt7y662YNbbwGccqvkQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pGBGt7y662YNbbwGccqvkQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pGBGt7y662YNbbwGccqvkQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pGBGt7y662YNbbwGccqvkQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pGBGt7y662YNbbwGccqvkQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pGBGt7y662YNbbwGccqvkQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bg mp oc c" width="700" height="691" loading="lazy" role="presentation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_pGBGt7y662YNbbwGccqvkQ.png"></picture></div></div><figcaption class="od fe oe no np of og be b bf z dt" data-selectable-paragraph="">Figure 8. Example t-SNE plot. Image by Author.</figcaption></figure><p id="1660" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">As mentioned earlier, model perplexity scores can be calculated to evaluate the effect of various hyperparameters for empirical testing. I use <a class="af pd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity" rel="noopener ugc nofollow" target="_blank">sklearn to calculate perplexity</a>, and <a class="af pd" rel="noopener" target="_blank" href="https://towardsdatascience.com/perplexity-in-language-models-87a196019a94">this blog post</a> provides an overview of how to assess perplexity in language models. When building a LDA model I prefer to set the perplexity tolerance to 0.1 and I keep this value constant so as to better utilize t-SNE visualizations. It is important to note that t-SNE has a non-convex objective function, where the objective function is minimized using a gradient descent optimization that is initiated randomly. Therefore, different initializations will result in different solutions, so it is possible (and often advisable) to run t-SNE multiple times with the same data and perplexity, and then choose the visualization with the lowest KL divergence.</p><h1 id="260a" class="pe pf gt be pg ph pi ht pj pk pl hw pm pn po pp pq pr ps pt pu pv pw px py pz bj" data-selectable-paragraph="">Limitations of LDA</h1><p id="cb21" class="pw-post-body-paragraph oh oi gt oj b hr qa ol om hu qb oo op oq qc os ot ou qd ow ox oy qe pa pb pc gm bj" data-selectable-paragraph="">Practically, an assessment of the LDA model reveals a few weaknesses, namely the necessity of a fixed <em class="qh">k</em> value, the inability of Dirichlet distributions in capturing correlations, the static nature does not show the evolution of topics over time, and lastly the simplifying “bag-of-words” exchangeability assumption. Of these limitations, none are sufficient to abandon this topic modeling method, but an awareness is necessary to understand the boundaries of results.</p><p id="0f41" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">In my experience, LDA can be used consistently and successfully to model text collections of news articles and policy documents, yet the results can be mixed for unconventional datasets, such as collections of tweets which are short and informal. Generally, a dataset is unsuitable for topic modeling if the length of the documents is too short, the data set is too small, or if there are too many topics within a collection (eg. book). In the past, ad hoc heuristics have been successfully employed to preprocess documents, such as aggregating tweets into longer “documents” (<a class="af pd" href="https://dl.acm.org/doi/10.1145/1964858.1964870" rel="noopener ugc nofollow" target="_blank">Hong and Davison, 2010</a>). These measures, however, can be blind since many of the common assumptions of limitations are not theoretically justified; for example, the deficiency in handling shorter documents has not been explained by theory.</p><p id="c414" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">In a paper from 2014 by <a class="af pd" href="https://dl.acm.org/doi/10.5555/3044805.3044828" rel="noopener ugc nofollow" target="_blank">Tang et al.</a> an attempt was made to understand the limiting factors of LDA with posterior contraction analysis. It was found that Liebig’s law of the minimum is applicable to LDA, whereby the scarcest resource acts as the limiting factor. Of the four guidelines proposed, the most important one is regarding the number of documents (<em class="qh">M</em>); a sufficiently sized dataset is absolutely necessary. Once a viable <em class="qh">M</em> value is achieved, further increasing <em class="qh">M</em> may not significantly improve performance, unless the document length is also suitably increased. Lastly, when a large number of topics (<em class="qh">K</em>) is used to fit an LDA model, the statistical inference may become inescapably inefficient. This is because the convergence rate deteriorates quickly to a non-parametric rate, depending on the number of topics used to fit the LDA model. Therefore, in practice it is very important to avoid selecting an overly large <em class="qh">k</em> value.</p><p id="0584" class="pw-post-body-paragraph oh oi gt oj b hr ok ol om hu on oo op oq or os ot ou ov ow ox oy oz pa pb pc gm bj" data-selectable-paragraph="">In conclusion, I believe that an awareness of the LDA model’s deficiencies along with practical guidelines for choosing datasets and hyperparameters will allow for successful implementation of this method for topic modeling. I welcome all feedback, whether questions or comments, so please feel free to connect with me on <a class="af pd" href="https://www.linkedin.com/in/haaya-naushan-a4b5b61a5/" rel="noopener ugc nofollow" target="_blank">Linkedin</a>.</p></div></div></div></div></section></div></div></article></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="rl rm ab jl"><div class="rn ab"><a class="ro ax am ao" href="https://medium.com/tag/machine-learning?source=post_page-----e7ff75290f8---------------machine_learning-----------------" rel="noopener follow"><div class="rp fi cw rq gd rr rs be b bf z bj rt">Machine Learning</div></a></div><div class="rn ab"><a class="ro ax am ao" href="https://medium.com/tag/nlp?source=post_page-----e7ff75290f8---------------nlp-----------------" rel="noopener follow"><div class="rp fi cw rq gd rr rs be b bf z bj rt">NLP</div></a></div><div class="rn ab"><a class="ro ax am ao" href="https://medium.com/tag/topic-modeling?source=post_page-----e7ff75290f8---------------topic_modeling-----------------" rel="noopener follow"><div class="rp fi cw rq gd rr rs be b bf z bj rt">Topic Modeling</div></a></div></div></div></div><div class="l"></div><footer class="ru rv rw rx ry rz sa sb sc ab q sd iv c"><div class="l ae"><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="ab co se"><div class="ab q lg"><div class="sf l"><span class="l sg sh si e d"><div class="ab q lg lh"><div class="pw-multi-vote-icon fi jp li lj lk"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7ff75290f8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=-----e7ff75290f8---------------------clap_footer-----------" rel="noopener follow"><div><div class="bl" aria-hidden="false" aria-describedby="10" aria-labelledby="10"><div class="ll ao lm ln lo lp am lq lr ls lk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l lt lu lv lw lx ly lz"><div><div class="bl" aria-hidden="false" aria-describedby="104" aria-labelledby="104"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at xu mi">241<span class="l h g f sj sk"></span></button></p></div></div></div></div></span><span class="l h g f sj sk"><div class="ab q lg lh"><div class="pw-multi-vote-icon fi jp li lj lk"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerClapButton" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7ff75290f8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=-----e7ff75290f8---------------------clap_footer-----------" rel="noopener follow"><div><div class="bl" aria-hidden="false" aria-describedby="11" aria-labelledby="11"><div class="ll ao lm ln lo lp am lq lr ls lk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></div></div></div></a></span></div><div class="pw-multi-vote-count l lt lu lv lw lx ly lz"><div><div class="bl" aria-hidden="false" aria-describedby="106" aria-labelledby="106"><p class="be b du z dt"><button class="af ag ah ai aj ak al am an ao ap aq ar as at xu mi">241</button></p></div></div></div></div></span></div><div class="bp ab"><div><div class="bl" aria-hidden="false" aria-describedby="12" aria-labelledby="12"><button class="ao ll md me ab q fj mf mg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="mc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="be b bf z dt"><span class="pw-responses-count mb mc">3</span></p></button></div></div></div></div><div class="ab q"><div class="sl l ji"><div><div class="bl" aria-hidden="false" aria-describedby="13" aria-labelledby="13"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" data-testid="footerBookmarkButton" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7ff75290f8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;source=--------------------------bookmark_footer-----------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div><div class="sl l ji"><div class="bl" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bl" aria-hidden="false" aria-describedby="14" aria-labelledby="14"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="af fj ah ai aj ak al mq an ao ap ew mr ms mg mt"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></footer><div class="sm sn so sp sq l bw"><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="ck ab sr co"><div class="ab il"><a href="https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="l ss st bx su ip"><div class="l fi"><img alt="Haaya Naushan" class="l fc bx sv sw cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_TFVRDzBGIrkHhqgct4QfkQ(1).jpg" width="72" height="72" loading="lazy"><div class="iq bx l sv sw fr n ir fs"></div></div></div></a><a href="https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="sx ab fi"><div><div class="bl" aria-hidden="false" aria-describedby="15" aria-labelledby="15"><div class="l sy sz bx su iv"><div class="l fi"><img alt="Towards Data Science" class="l fc bx by bz cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_CJe3891yB1A1mzMdqemkdg(1).jpg" width="32" height="32" loading="lazy"><div class="iq bx l by bz fr n ir fs"></div></div></div></div></div></div></a></div><div class="j i d"><div class="ab"><span><a class="be b bf z eo rp ep eq er es et eu ev ew ex ey ez ta fa fb fc bl fd fe" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=post_page-68f801f1b50b----e7ff75290f8---------------------follow_profile-----------" rel="noopener follow">Follow</a></span><div class="ds l"><div><div><div class="bl" aria-hidden="false" aria-describedby="249" aria-labelledby="249"><div class="l"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e411f1cc489&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;newsletterV3=68f801f1b50b&amp;newsletterV3Id=4e411f1cc489&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=-----e7ff75290f8---------------------subscribe_user-----------" rel="noopener follow"><button class="be b bf z eo am ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" aria-label="Subscribe"><svg xmlns="http://www.w3.org/2000/svg" width="38" height="38" fill="none" viewBox="0 0 38 38" class="xs sz sy"><rect width="0.5" height="6.5" x="26.25" y="9.25" rx="0.25"></rect><rect width="0.5" height="6.5" x="29.75" y="12.25" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5 19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class="ab cm co"><div class="l"><div class="ab q"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab q" href="https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><h2 class="pw-author-name be tl tm tn to bj"><span class="gm jz">Written by <!-- -->Haaya Naushan</span></h2></a></div><div class="rn ab"><div class="l ji"><span class="pw-follower-count be b bf z bj"><a class="af ag ah ai aj ak al am an ao ap aq ar jb" href="https://haaya-naushan.medium.com/followers?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow">1K Followers</a></span></div><div class="be b bf z jq jr js ab ju jv jw jx dt jo"><span class="jc l" aria-hidden="true"><span class="be b bf z dt">·</span></span><span class="l ji">Writer for </span><div><div class="l" aria-hidden="false" aria-describedby="17" aria-labelledby="17"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b bf z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div></div><div class="tp l"><p class="be b bf z bj"><span class="gm">Data Scientist enthusiastic about machine learning, social justice, video games and philosophy.</span></p></div></div><div class="h k"><div class="ab"><span><a class="be b bf z eo rp ep eq er es et eu ev ew ex ey ez ta fa fb fc bl fd fe" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F68f801f1b50b&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=post_page-68f801f1b50b----e7ff75290f8---------------------follow_profile-----------" rel="noopener follow">Follow</a></span><div class="ds l"><div><div><div class="bl" aria-hidden="false" aria-describedby="251" aria-labelledby="251"><div class="l"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e411f1cc489&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8&amp;newsletterV3=68f801f1b50b&amp;newsletterV3Id=4e411f1cc489&amp;user=Haaya+Naushan&amp;userId=68f801f1b50b&amp;source=-----e7ff75290f8---------------------subscribe_user-----------" rel="noopener follow"><button class="be b bf z eo am ep eq er es et eu ev ew ex ey ez fa fb fc bl fd fe" aria-label="Subscribe"><svg xmlns="http://www.w3.org/2000/svg" width="38" height="38" fill="none" viewBox="0 0 38 38" class="xs sz sy"><rect width="0.5" height="6.5" x="26.25" y="9.25" rx="0.25"></rect><rect width="0.5" height="6.5" x="29.75" y="12.25" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5 19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div></div></div><div class="tq bg tr ts tt tu tv tw"></div></div></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="xv xw l"><h2 class="be tl iz z gs bj">More from Haaya Naushan and Towards Data Science</h2></div><div class="wr ab lg jl xx xy xz ya yb yc yd ye yf yg yh yi yj yk yl"><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://towardsdatascience.com/causal-machine-learning-for-econometrics-causal-forests-5ab3aec825a7" tabindex="0"><div class="zx"><div aria-label="Causal Machine Learning for Econometrics: Causal Forests"><div class="zz aba abb abc abd"><img alt="Causal Machine Learning for Econometrics: Causal Forests" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_vKNHsDLevjizZgRThbng-A.jpg" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="135" aria-labelledby="135"><a tabindex="-1" href="https://haaya-naushan.medium.com/?source=author_recirc-----e7ff75290f8----0---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><div class="l fi"><img alt="Haaya Naushan" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_TFVRDzBGIrkHhqgct4QfkQ(2).jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="136" aria-labelledby="136"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://haaya-naushan.medium.com/?source=author_recirc-----e7ff75290f8----0---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Haaya Naushan</p></a></div></div></div><div class="abs l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="137" aria-labelledby="137"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://towardsdatascience.com/?source=author_recirc-----e7ff75290f8----0---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://towardsdatascience.com/causal-machine-learning-for-econometrics-causal-forests-5ab3aec825a7?source=author_recirc-----e7ff75290f8----0---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Causal Machine Learning for Econometrics: Causal Forests</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">Introduction to causal machine learning for econometrics, including a code tutorial on estimating the CATE with a causal forest using…</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><span>Apr 16, 2021</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" rel="noopener follow" href="https://towardsdatascience.com/causal-machine-learning-for-econometrics-causal-forests-5ab3aec825a7?source=author_recirc-----e7ff75290f8----0---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="204" aria-labelledby="204"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">520</span></div></div></div></div><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="138" aria-labelledby="138"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" viewBox="0 0 16 16"><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span style="margin-left: 4px;">13</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="139" aria-labelledby="139"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ab3aec825a7&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcausal-machine-learning-for-econometrics-causal-forests-5ab3aec825a7&amp;source=-----e7ff75290f8----0-----------------bookmark_preview----6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://towardsdatascience.com/17-advanced-rag-techniques-to-turn-your-rag-app-prototype-into-a-production-ready-solution-5a048e36cdc8" tabindex="0"><div class="zx"><div aria-label="17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution"><div class="zz aba abb abc abd"><img alt="17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_DaXfsffAhnAzQMHH7nZ4ng.png" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="140" aria-labelledby="140"><a tabindex="-1" href="https://dmnkplzr.medium.com/?source=author_recirc-----e7ff75290f8----1---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><div class="l fi"><img alt="Dominik Polzer" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_KqpicOFO7jh7FXGjoJ2Bcg.jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="141" aria-labelledby="141"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://dmnkplzr.medium.com/?source=author_recirc-----e7ff75290f8----1---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Dominik Polzer</p></a></div></div></div><div class="abs l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="142" aria-labelledby="142"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://towardsdatascience.com/?source=author_recirc-----e7ff75290f8----1---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://towardsdatascience.com/17-advanced-rag-techniques-to-turn-your-rag-app-prototype-into-a-production-ready-solution-5a048e36cdc8?source=author_recirc-----e7ff75290f8----1---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div title="17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution"><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready Solution</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">A collection of RAG techniques to help you develop your RAG app into something robust that will last</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><div class="acz sc ab"><div class="bl" aria-hidden="false" aria-describedby="143" aria-labelledby="143"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="144" aria-labelledby="144"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></button></div></div><span>Jun 26</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" rel="noopener follow" href="https://towardsdatascience.com/17-advanced-rag-techniques-to-turn-your-rag-app-prototype-into-a-production-ready-solution-5a048e36cdc8?source=author_recirc-----e7ff75290f8----1---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="206" aria-labelledby="206"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">2K</span></div></div></div></div><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="145" aria-labelledby="145"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" viewBox="0 0 16 16"><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span style="margin-left: 4px;">20</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="146" aria-labelledby="146"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a048e36cdc8&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2F17-advanced-rag-techniques-to-turn-your-rag-app-prototype-into-a-production-ready-solution-5a048e36cdc8&amp;source=-----e7ff75290f8----1-----------------bookmark_preview----6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://towardsdatascience.com/genai-with-python-rag-with-llm-complete-tutorial-c276dda6707b" tabindex="0"><div class="zx"><div aria-label="GenAI with Python: RAG with LLM (Complete Tutorial)"><div class="zz aba abb abc abd"><img alt="GenAI with Python: RAG with LLM (Complete Tutorial)" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_AVyb1zVcuR-ke_kg.jpg" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="147" aria-labelledby="147"><a tabindex="-1" href="https://maurodp.medium.com/?source=author_recirc-----e7ff75290f8----2---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><div class="l fi"><img alt="Mauro Di Pietro" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/2_vdOG1aNO6cMj0IYIQuKxDQ.jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="148" aria-labelledby="148"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://maurodp.medium.com/?source=author_recirc-----e7ff75290f8----2---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Mauro Di Pietro</p></a></div></div></div><div class="abs l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="149" aria-labelledby="149"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://towardsdatascience.com/?source=author_recirc-----e7ff75290f8----2---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://towardsdatascience.com/genai-with-python-rag-with-llm-complete-tutorial-c276dda6707b?source=author_recirc-----e7ff75290f8----2---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">GenAI with Python: RAG with LLM (Complete Tutorial)</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">Build your own ChatGPT with multimodal data and run it on your laptop without GPU</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><div class="acz sc ab"><div class="bl" aria-hidden="false" aria-describedby="150" aria-labelledby="150"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="151" aria-labelledby="151"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></button></div></div><span>Jun 28</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" rel="noopener follow" href="https://towardsdatascience.com/genai-with-python-rag-with-llm-complete-tutorial-c276dda6707b?source=author_recirc-----e7ff75290f8----2---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="208" aria-labelledby="208"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">759</span></div></div></div></div><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="152" aria-labelledby="152"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" viewBox="0 0 16 16"><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span style="margin-left: 4px;">14</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="153" aria-labelledby="153"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc276dda6707b&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenai-with-python-rag-with-llm-complete-tutorial-c276dda6707b&amp;source=-----e7ff75290f8----2-----------------bookmark_preview----6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://towardsdatascience.com/multivariate-autoregressive-models-and-impulse-response-analysis-cb5ead9b2b68" tabindex="0"><div class="zx"><div aria-label="Multivariate Autoregressive Models and Impulse Response Analysis"><div class="zz aba abb abc abd"><img alt="Multivariate Autoregressive Models and Impulse Response Analysis" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_ZCFCPs7_ckkeFAuWJIOPKQ.png" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="154" aria-labelledby="154"><a tabindex="-1" href="https://haaya-naushan.medium.com/?source=author_recirc-----e7ff75290f8----3---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><div class="l fi"><img alt="Haaya Naushan" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_TFVRDzBGIrkHhqgct4QfkQ(2).jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="155" aria-labelledby="155"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://haaya-naushan.medium.com/?source=author_recirc-----e7ff75290f8----3---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Haaya Naushan</p></a></div></div></div><div class="abs l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="156" aria-labelledby="156"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://towardsdatascience.com/?source=author_recirc-----e7ff75290f8----3---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://towardsdatascience.com/multivariate-autoregressive-models-and-impulse-response-analysis-cb5ead9b2b68?source=author_recirc-----e7ff75290f8----3---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Multivariate Autoregressive Models and Impulse Response Analysis</h2></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><span>May 27, 2021</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" rel="noopener follow" href="https://towardsdatascience.com/multivariate-autoregressive-models-and-impulse-response-analysis-cb5ead9b2b68?source=author_recirc-----e7ff75290f8----3---------------------6b9f5610_1102_492e_9723_60e67b9d11dc-------"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="210" aria-labelledby="210"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">94</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="157" aria-labelledby="157"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb5ead9b2b68&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultivariate-autoregressive-models-and-impulse-response-analysis-cb5ead9b2b68&amp;source=-----e7ff75290f8----3-----------------bookmark_preview----6b9f5610_1102_492e_9723_60e67b9d11dc-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span></div></div></div></div></div></div></article></div></div></div><div class="tq bg tr dj dk ada adb adc"></div><div class="ab jj jk add ade adf"><a class="be b bf z bj rp adg adh adi mi mf xi eu ev ew adj adk adl ez wt wu adm adn ado fa fb fc bl fd fe" href="https://haaya-naushan.medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="l fe">See all from Haaya Naushan</div></a><div class="adp adq adr ads adt adu adv adw adx lz l"><a class="be b bf z bj rp adg adh adi mi mf xi eu ev ew adj adk adl ez wt wu adm adn ado fa fb fc bl fd fe" href="https://towardsdatascience.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="l fe">See all from Towards Data Science</div></a></div></div></div></div><div class="tq bg tr ady adz aea aeb aec"></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="aed aee l"><h2 class="be tl ph ht pj pk hw pm pn pp pq pr pt pu pv px py bj">Recommended from Medium</h2><div class="nr ns nt nu nv l"><div class="wr ab lg jl xx xy xz ya yb yc yd ye yf yg yh yi yj yk yl"><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://towardsdatascience.com/topic-modelling-with-berttopic-in-python-8a80d529de34" tabindex="0"><div class="zx"><div aria-label="Topic Modelling with BERTtopic in Python"><div class="zz aba abb abc abd"><img alt="Topic Modelling with BERTtopic in Python" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_3Sqr6LH1agLPsrR4kOl5ZA.jpg" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="158" aria-labelledby="158"><a tabindex="-1" href="https://petrkorab.medium.com/?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="l fi"><img alt="Petr Korab" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_ZLl1xeZ1_beSk7LfvranlA.jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="159" aria-labelledby="159"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://petrkorab.medium.com/?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Petr Korab</p></a></div></div></div><div class="abs l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="160" aria-labelledby="160"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://towardsdatascience.com/?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Towards Data Science</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" rel="noopener follow" href="https://towardsdatascience.com/topic-modelling-with-berttopic-in-python-8a80d529de34?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Topic Modelling with BERTtopic in Python</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">Hands-on tutorial on modeling political statements with a state-of-the-art transformer-based topic model</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><div class="acz sc ab"><div class="bl" aria-hidden="false" aria-describedby="161" aria-labelledby="161"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="162" aria-labelledby="162"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></button></div></div><span>Apr 1</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" rel="noopener follow" href="https://towardsdatascience.com/topic-modelling-with-berttopic-in-python-8a80d529de34?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="211" aria-labelledby="211"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">296</span></div></div></div></div><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="163" aria-labelledby="163"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" viewBox="0 0 16 16"><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span style="margin-left: 4px;">1</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="164" aria-labelledby="164"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a80d529de34&amp;operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftopic-modelling-with-berttopic-in-python-8a80d529de34&amp;source=-----e7ff75290f8----0-----------------bookmark_preview----967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://datasciencenerd.us/topic-modeling-and-network-analysis-using-bertopic-and-networkx-6ab9c4796db5" tabindex="0"><div class="zx"><div aria-label="Topic modeling and network analysis using BERTopic and NetworkX"><div class="zz aba abb abc abd"><img alt="Topic modeling and network analysis using BERTopic and NetworkX" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_MCuNo3TUup-EausL.jpg" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="165" aria-labelledby="165"><a tabindex="-1" href="https://medium.com/@ednalyn.dedios?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="l fi"><img alt="Ednalyn C. De Dios" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/2_Axcg_Bxyvj7M6sf9sX7V_w.jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="166" aria-labelledby="166"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://medium.com/@ednalyn.dedios?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Ednalyn C. De Dios</p></a></div></div></div><div class="abs l"><p class="be b du z dt">in</p></div><div class="l"><div><div class="l" aria-hidden="false" aria-describedby="167" aria-labelledby="167"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://datasciencenerd.us/?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Data Science Nerd</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://datasciencenerd.us/topic-modeling-and-network-analysis-using-bertopic-and-networkx-6ab9c4796db5?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Topic modeling and network analysis using BERTopic and NetworkX</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">A quick and dirty guide to uncovering themes in domestic violence studies using machine learning</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><div class="acz sc ab"><div class="bl" aria-hidden="false" aria-describedby="168" aria-labelledby="168"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="169" aria-labelledby="169"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></button></div></div><span>Jun 29</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" href="https://datasciencenerd.us/topic-modeling-and-network-analysis-using-bertopic-and-networkx-6ab9c4796db5?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="213" aria-labelledby="213"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">80</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="170" aria-labelledby="170"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6ab9c4796db5&amp;operation=register&amp;redirect=https%3A%2F%2Fdatasciencenerd.us%2Ftopic-modeling-and-network-analysis-using-bertopic-and-networkx-6ab9c4796db5&amp;source=-----e7ff75290f8----1-----------------bookmark_preview----967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span></div></div></div></div></div></div></article></div></div></div></div><div class="tq bg tr aef"></div><h2 class="be tl iz z gs bj">Lists</h2><div class="aeg l"><div class="cm ab lg jl xx xy xz ya yb yc yd ye yf yg yh yi yj yk yl"><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@ben.putney/list/predictive-modeling-w-python-e3668ea008e1?source=read_next_recirc-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="aem aen jq ab ji fi"><div class="fi abe aei bw aej"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_r4yjMpEmqzHCUvWC.jpg" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi abe aei bw lh aek"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_bv2KUVNLi2sFNjBTdoBmWw.png" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi abe bw iv ael"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_zsngbTOmFCy6sUCx.jpg" width="48" height="48" loading="lazy" role="presentation"></div></div></div><div class="aw l"><h2 class="be tl iz z jq acs js jt act jv jx gs bj">Predictive Modeling w/ Python</h2><div class="be b du z dt ab aeh">20 stories<span class="jc l" aria-hidden="true"><span class="be b bf z dt">·</span></span>1403 saves</div></div></a></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=read_next_recirc-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="aem aen jq ab ji fi"><div class="fi abe aei bw aej"><div class="abe in jq l"><img alt="Principal Component Analysis for ML" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_swd_PY6vTCyPnsgBYoFZfA.png" width="48" height="48" loading="lazy"></div></div><div class="fi abe aei bw lh aek"><div class="abe in jq l"><img alt="Time Series Analysis" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_8sSAHftNwd_RNJ3k4VA0pA.png" width="48" height="48" loading="lazy"></div></div><div class="fi abe bw iv ael"><div class="abe in jq l"><img alt="deep learning cheatsheet for beginner" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_uNyD4yNMH-DnOel1wzxOOA.png" width="48" height="48" loading="lazy"></div></div></div><div class="aw l"><h2 class="be tl iz z jq acs js jt act jv jx gs bj">Practical Guides to Machine Learning</h2><div class="be b du z dt ab aeh">10 stories<span class="jc l" aria-hidden="true"><span class="be b bf z dt">·</span></span>1702 saves</div></div></a></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@AMGAS14/list/natural-language-processing-0a856388a93a?source=read_next_recirc-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="aem aen jq ab ji fi"><div class="fi abe aei bw aej"><div class="abe in jq l"><img alt="A llama" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_xid8UNevYAC7CryWCE1ItA.jpg" width="48" height="48" loading="lazy"></div></div><div class="fi abe aei bw lh aek"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_1EBrOa4eUilLiU5Yy_D6SQ.png" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi abe bw iv ael"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_StC6eQ27083tIT73-Qzwqw.png" width="48" height="48" loading="lazy" role="presentation"></div></div></div><div class="aw l"><h2 class="be tl iz z jq acs js jt act jv jx gs bj">Natural Language Processing</h2><div class="be b du z dt ab aeh">1603 stories<span class="jc l" aria-hidden="true"><span class="be b bf z dt">·</span></span>1157 saves</div></div></a></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><a class="af ag ah ai aj ak al am an ao ap aq ar as at ab ck cm" href="https://medium.com/@MediumStaff/list/the-new-chatbots-chatgpt-bard-and-beyond-5969c7449b7f?source=read_next_recirc-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="aem aen jq ab ji fi"><div class="fi abe aei bw aej"><div class="abe in jq l"><img alt="Image by vectorjuice on FreePik" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_3OsUtsnlTx9Svm4c.jpg" width="48" height="48" loading="lazy"></div></div><div class="fi abe aei bw lh aek"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_IPZF1hcDWwpPqOz2vL7NxQ.png" width="48" height="48" loading="lazy" role="presentation"></div></div><div class="fi abe bw iv ael"><div class="abe in jq l"><img alt="" class="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_0fHUKyg3xtpNWpop35PR4g.png" width="48" height="48" loading="lazy" role="presentation"></div></div></div><div class="aw l"><h2 class="be tl iz z jq acs js jt act jv jx gs bj">The New Chatbots: ChatGPT, Bard, and Beyond</h2><div class="be b du z dt ab aeh">12 stories<span class="jc l" aria-hidden="true"><span class="be b bf z dt">·</span></span>428 saves</div></div></a></div></div></div><div class="tq bg tr adq dj ads dk aeo aep aeq aer aes aet"></div><div class="wr ab lg jl xx xy xz ya yb yc yd ye yf yg yh yi yj yk yl"><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://medium.com/@kappei/a-novel-approach-to-topic-modeling-using-large-language-models-llms-648c131393d2" tabindex="0"><div class="zx"><div aria-label="A Novel Approach to Topic Modeling Using Large Language Models (LLMs)"><div class="zz aba abb abc abd"><img alt="A Novel Approach to Topic Modeling Using Large Language Models (LLMs)" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_mosxQ0_LIHH6La9n5e7B7A.png" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="171" aria-labelledby="171"><a tabindex="-1" href="https://medium.com/@kappei?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="l fi"><img alt="Nakano Kappei" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_VaD1NsrY7B-BmjiO.jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="172" aria-labelledby="172"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://medium.com/@kappei?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Nakano Kappei</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@kappei/a-novel-approach-to-topic-modeling-using-large-language-models-llms-648c131393d2?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">A Novel Approach to Topic Modeling Using Large Language Models (LLMs)</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">Introduction</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><span>Apr 14</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" href="https://medium.com/@kappei/a-novel-approach-to-topic-modeling-using-large-language-models-llms-648c131393d2?source=read_next_recirc-----e7ff75290f8----0---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="214" aria-labelledby="214"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">2</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="173" aria-labelledby="173"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F648c131393d2&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40kappei%2Fa-novel-approach-to-topic-modeling-using-large-language-models-llms-648c131393d2&amp;source=-----e7ff75290f8----0-----------------bookmark_preview----967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://ydong029.medium.com/topic-summarization-and-categorization-with-gpt-717a40130d25" tabindex="0"><div class="zx"><div aria-label="Topic Summarization and Categorization with GPT"><div class="zz aba abb abc abd"><img alt="Topic Summarization and Categorization with GPT" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_a-n05ocnx9xp2OZcEBYOWw.png" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="174" aria-labelledby="174"><a tabindex="-1" href="https://ydong029.medium.com/?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="l fi"><img alt="Yu Dong" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_29MqzRR7m5m93unBH1o7lw.png" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="175" aria-labelledby="175"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://ydong029.medium.com/?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Yu Dong</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://ydong029.medium.com/topic-summarization-and-categorization-with-gpt-717a40130d25?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Topic Summarization and Categorization with GPT</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">Use GPT-3.5 API for text analytics to categorize and summarize data science blog posts.</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><div class="acz sc ab"><div class="bl" aria-hidden="false" aria-describedby="176" aria-labelledby="176"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="177" aria-labelledby="177"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></button></div></div><span>May 25</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" href="https://ydong029.medium.com/topic-summarization-and-categorization-with-gpt-717a40130d25?source=read_next_recirc-----e7ff75290f8----1---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="215" aria-labelledby="215"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">73</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="178" aria-labelledby="178"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F717a40130d25&amp;operation=register&amp;redirect=https%3A%2F%2Fydong029.medium.com%2Ftopic-summarization-and-categorization-with-gpt-717a40130d25&amp;source=-----e7ff75290f8----1-----------------bookmark_preview----967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://medium.com/@danushidk507/topic-modelling-with-bertopic-249095144555" tabindex="0"><div class="zx"><div aria-label="Topic Modelling with BERTopic"><div class="zz aba abb abc abd"><img alt="Topic Modelling with BERTopic" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_UI0pbfQ89oI3ycTU.png" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="179" aria-labelledby="179"><a tabindex="-1" href="https://medium.com/@danushidk507?source=read_next_recirc-----e7ff75290f8----2---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="l fi"><img alt="DhanushKumar" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_nV_t7yjnnON8MBQAh4VeeQ.jpg" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="180" aria-labelledby="180"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://medium.com/@danushidk507?source=read_next_recirc-----e7ff75290f8----2---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">DhanushKumar</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@danushidk507/topic-modelling-with-bertopic-249095144555?source=read_next_recirc-----e7ff75290f8----2---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Topic Modelling with BERTopic</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">BERTopic is a topic modeling technique that leverages BERT (Bidirectional Encoder Representations from Transformers), a powerful language…</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><span>Jul 1</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" href="https://medium.com/@danushidk507/topic-modelling-with-bertopic-249095144555?source=read_next_recirc-----e7ff75290f8----2---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="216" aria-labelledby="216"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">2</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="181" aria-labelledby="181"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F249095144555&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40danushidk507%2Ftopic-modelling-with-bertopic-249095144555&amp;source=-----e7ff75290f8----2-----------------bookmark_preview----967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span><div class="j i d"><div class="tq bg tr mc"></div></div></div></div></div></div></div></div></article></div></div><div class="ym yn yo nj yp yq yr nk ys yt yu yv yw yx yy yz za zb zc zd ze"><div class="zf zg zh zi zj dv l"><article class="dv"><div class="dv sc l"><div class="bg dv"><div class="dv l"><div class="fi dv zk zl zm zn zo zp zq zr zs zt zu zv zw" role="link" data-href="https://medium.com/@bnjmn_marie/generate-synthetic-data-from-scratch-to-fine-tune-llms-6a0a67128a15" tabindex="0"><div class="zx"><div aria-label="Generate Synthetic Data from Scratch to Fine-tune LLMs"><div class="zz aba abb abc abd"><img alt="Generate Synthetic Data from Scratch to Fine-tune LLMs" class="bg abe abf abg abh bw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/0_ULO9lZuV3HqObN_q.jpg" loading="lazy"></div></div></div><div class="zy ab ca cn"><div class="ab cn abi bg abj abk abl abm"><div class="vv vx abn abo abp ab q"><div class="ro l"><div><div class="l" aria-hidden="false" aria-describedby="182" aria-labelledby="182"><a tabindex="-1" href="https://medium.com/@bnjmn_marie?source=read_next_recirc-----e7ff75290f8----3---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="l fi"><img alt="Benjamin Marie" class="l fc bx abq abr cw" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/1_sifLT7ybERpQ7SnaPwBDBQ.png" width="20" height="20" loading="lazy"><div class="fq bx l abq abr fr n ax fs"></div></div></a></div></div></div><div class="abs l"><div><div class="l" aria-hidden="false" aria-describedby="183" aria-labelledby="183"><a class="af ag ah ai aj ak al am an ao ap aq ar jb ab q" href="https://medium.com/@bnjmn_marie?source=read_next_recirc-----e7ff75290f8----3---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><p class="be b du z jq jr js jt ju jv jw jx bj">Benjamin Marie</p></a></div></div></div></div><div class="abt l abu abv abw abx aby gm"><div class="abz aca acb acc acd ace acf acg"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@bnjmn_marie/generate-synthetic-data-from-scratch-to-fine-tune-llms-6a0a67128a15?source=read_next_recirc-----e7ff75290f8----3---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div title=""><h2 class="be gu ph ht ach aci pj pk hw acj ack pm oq qr acl acm qs ou qu acn aco qv oy qx acp acq qy jq js jt jv jx bj">Generate Synthetic Data from Scratch to Fine-tune LLMs</h2></div><div class="acr l"><h3 class="be b iz z jq acs js jt act jv jx dt">GLAN (Generalized instruction-tuning for Large lANguage models)</h3></div></a></div></div><span class="be b du z dt"><div class="in ab co ae"><div class="ab q"><div class="acz sc ab"><div class="bl" aria-hidden="false" aria-describedby="184" aria-labelledby="184"><button class="l ax ao am" aria-label="Member-only story"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="185" aria-labelledby="185"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></button></div></div><span>Feb 27</span><div class=""><div class="fi acu dg ab q"><div class="fr ug acv ab q"><div class="aw dg di l cw"></div></div><a class="fr md acv ab q" tabindex="-1" href="https://medium.com/@bnjmn_marie/generate-synthetic-data-from-scratch-to-fine-tune-llms-6a0a67128a15?source=read_next_recirc-----e7ff75290f8----3---------------------967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="217" aria-labelledby="217"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 16 16"><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span style="margin-left: 4px;">185</span></div></div></div></div><div class="aw l"><div><div class="ab" aria-hidden="false" aria-describedby="186" aria-labelledby="186"><div class="ab q fi"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" viewBox="0 0 16 16"><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span style="margin-left: 4px;">1</span></div></div></div></div></a></div></div></div><div class="ab q acw acx"><div class=""><div><div class="bl" aria-hidden="false" aria-describedby="187" aria-labelledby="187"><span><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6a0a67128a15&amp;operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2F%40bnjmn_marie%2Fgenerate-synthetic-data-from-scratch-to-fine-tune-llms-6a0a67128a15&amp;source=-----e7ff75290f8----3-----------------bookmark_preview----967d9a00_7519_4941_883d_6c5b8b3d4df8-------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25" class="dt mi acy" aria-label="Add to list bookmark button"><path fill="currentColor" d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .805.396L12.5 17l5.695 4.396A.5.5 0 0 0 19 21v-8.5a.5.5 0 0 0-1 0v7.485l-5.195-4.012a.5.5 0 0 0-.61 0L7 19.985z"></path></svg></a></span></div></div></div></div></div></span></div></div></div></div></div></div></article></div></div></div><div class="tq bg tr dj dk ada adb adc"></div><a class="be b bf z bj rp adg adh adi mi mf xi eu ev ew adj adk adl ez wt wu adm adn ado fa fb fc bl fd fe" href="https://medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><div class="l fe">See more recommendations</div></a></div></div></div><div class="h k j"><div class="tq bg tr tx"></div><div class="ab ca"><div class="ch bg fy fz ga gb"><div class="ty ab lg jl"><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://help.medium.com/hc/en-us?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Help</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.statuspage.io/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Status</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/about?autoplay=1&amp;source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">About</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Careers</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://towardsdatascience.com/pressinquiries@medium.com?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Press</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://blog.medium.com/?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Blog</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Privacy</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Terms</p></a></div><div class="tz ua l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://speechify.com/medium?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Text to speech</p></a></div><div class="tz l"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/business?source=post_page-----e7ff75290f8--------------------------------" rel="noopener follow"><p class="be b du z dt">Teams</p></a></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20240726-180941-e64b9be4a9"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-e7ff75290f8","user-68f801f1b50b","collection-7f60cf5620c9"],"serverVariantState":"44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":true,"vary":[],"lohpSummerUpsellEnabled":true},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"viewerIsBot":false},"debug":{"requestId":"ab4446da-cc68-4f9e-9fb5-4b9efd07339a","hybridDevServices":[],"originalSpanCarrier":{}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"partnerProgram":{"selectedCountryCode":null},"queryString":"","currentHash":""},"config":{"nodeEnv":"production","version":"main-20240726-180941-e64b9be4a9","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20240726-180941-e64b9be4a9","commit":"e64b9be4a9d457ebe2c661d1181bedde1dddb868"}},"datacenter":"us"},"googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","viewer":null,"collectionByDomainOrSlug({\"domainOrSlug\":\"towardsdatascience.com\"})":{"__ref":"Collection:7f60cf5620c9"},"postResult({\"id\":\"e7ff75290f8\"})":{"__ref":"Post:e7ff75290f8"}},"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png":{"__typename":"ImageMetadata","id":"1*VzTUkfeGymHP4Bvav-T-lA.png"},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","favicon":{"__ref":"ImageMetadata:1*VzTUkfeGymHP4Bvav-T-lA.png"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"googleAnalyticsId":null,"editors":[{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:7e12c71dfa81"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:e6ad8abedec9"}},{"__typename":"CollectionMastheadUserItem","user":{"__ref":"User:895063a310f4"}}],"name":"Towards Data Science","avatar":{"__ref":"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg"},"domain":"towardsdatascience.com","slug":"towards-data-science","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","subscriberCount":710150,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_4e1c48e2b02f"},"twitterUsername":"TDataScience","facebookPageId":null,"logo":{"__ref":"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png"}},"User:7e12c71dfa81":{"__typename":"User","id":"7e12c71dfa81"},"User:e6ad8abedec9":{"__typename":"User","id":"e6ad8abedec9"},"User:895063a310f4":{"__typename":"User","id":"895063a310f4"},"ImageMetadata:1*CJe3891yB1A1mzMdqemkdg.jpeg":{"__typename":"ImageMetadata","id":"1*CJe3891yB1A1mzMdqemkdg.jpeg"},"LinkedAccounts:68f801f1b50b":{"__typename":"LinkedAccounts","mastodon":null,"id":"68f801f1b50b"},"UserViewerEdge:userId:68f801f1b50b-viewerId:lo_4e1c48e2b02f":{"__typename":"UserViewerEdge","id":"userId:68f801f1b50b-viewerId:lo_4e1c48e2b02f","isFollowing":false,"isUser":false,"isMuting":false},"NewsletterV3:4e411f1cc489":{"__typename":"NewsletterV3","id":"4e411f1cc489","type":"NEWSLETTER_TYPE_AUTHOR","slug":"68f801f1b50b","name":"68f801f1b50b","collection":null,"user":{"__ref":"User:68f801f1b50b"}},"User:68f801f1b50b":{"__typename":"User","id":"68f801f1b50b","name":"Haaya Naushan","username":"haaya-naushan","newsletterV3":{"__ref":"NewsletterV3:4e411f1cc489"},"linkedAccounts":{"__ref":"LinkedAccounts:68f801f1b50b"},"isSuspended":false,"imageId":"1*TFVRDzBGIrkHhqgct4QfkQ.jpeg","mediumMemberAt":0,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":1048},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"haaya-naushan.medium.com"}},"hasSubdomain":true,"bio":"Data Scientist enthusiastic about machine learning, social justice, video games and philosophy.","isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:68f801f1b50b-viewerId:lo_4e1c48e2b02f"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"allowNotes":true,"twitterScreenName":"","membership":null},"Topic:1eca0103fff3":{"__typename":"Topic","slug":"machine-learning","id":"1eca0103fff3","name":"Machine Learning"},"Paragraph:5f742fd77aa_0":{"__typename":"Paragraph","id":"5f742fd77aa_0","name":"ac27","type":"H3","href":null,"layout":null,"metadata":null,"text":"Topic Modeling with Latent Dirichlet Allocation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_1":{"__typename":"Paragraph","id":"5f742fd77aa_1","name":"691b","type":"H4","href":null,"layout":null,"metadata":null,"text":"A practical exploration of the Natural Language Processing technique of Latent Dirichlet Allocation and its application to the task of topic modeling.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*yHnDOAR2LBJJ6hHws00vWg.jpeg":{"__typename":"ImageMetadata","id":"1*yHnDOAR2LBJJ6hHws00vWg.jpeg","originalHeight":1228,"originalWidth":3028,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_2":{"__typename":"Paragraph","id":"5f742fd77aa_2","name":"be52","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*yHnDOAR2LBJJ6hHws00vWg.jpeg"},"text":"The LDA model graphically represented with plate notation. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_3":{"__typename":"Paragraph","id":"5f742fd77aa_3","name":"6d4b","type":"P","href":null,"layout":null,"metadata":null,"text":"Topic modeling is a form of unsupervised machine learning that allows for efficient processing of large collections of data, while preserving the statistical relationships that are useful for tasks such as classification or summarization. The goal of topic modeling is to uncover latent variables that govern the semantics of a document, these latent variables representing abstract topics. Currently, the most popular technique for topic modeling is Latent Dirichlet Allocation (LDA), and this model can be used effectively on a variety of document types such as collections of news articles, policy documents, social media posts or tweets.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":28,"end":57,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FUnsupervised_learning","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_4":{"__typename":"Paragraph","id":"5f742fd77aa_4","name":"64f9","type":"P","href":null,"layout":null,"metadata":null,"text":"This article will necessarily and briefly mention precursive topic modeling techniques, such as Latent Semantic Indexing (LSI, also referred to interchangeably as Latent Semantic Analysis\u002FLSA) and probabilistic Latent Semantic Indexing (pLSI). The main focus will be a discussion of the LDA model, with an emphasis on understanding the role of hyperparameters and the challenge of inference. Next, I offer a practical introduction to implementation, covering dataset requirements, fine-tuning, and evaluation. Lastly, I conclude with a discussion of the limitations of the LDA model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_5":{"__typename":"Paragraph","id":"5f742fd77aa_5","name":"e31f","type":"H3","href":null,"layout":null,"metadata":null,"text":"Foundation of modern Topic modeling","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_6":{"__typename":"Paragraph","id":"5f742fd77aa_6","name":"27f8","type":"P","href":null,"layout":null,"metadata":null,"text":"A brief history and a theoretical understanding of the foundational techniques that preceded LDA will explain the importance of the improvements made by this modern technique. Back in the 1980’s the field of information retrieval produced a text representation scheme called term frequency–inverse document frequency (tf-idf) for applying a numerical statistic to a word that would be representative of its importance within a document (Salton and McGill, 1983).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":208,"end":229,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FInformation_retrieval","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":274,"end":325,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTf%E2%80%93idf","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_7":{"__typename":"Paragraph","id":"5f742fd77aa_7","name":"e04d","type":"P","href":null,"layout":null,"metadata":null,"text":"Tf-idf vectorization is where a normalized term frequency (count of the number of occurrences of a term within a document) is compared to the normalized inverse document frequency (count of the number of occurrences of a term within a corpus) on a log scale. This results in a term-by-document weight matrix that is unfortunately very sparse, noisy and redundant. The process to calculate the tf-idf vector for any word in a document can be represented by the equation below.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*dxZnBpfi45tp_8AluCMW_A.jpeg":{"__typename":"ImageMetadata","id":"1*dxZnBpfi45tp_8AluCMW_A.jpeg","originalHeight":876,"originalWidth":1952,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_8":{"__typename":"Paragraph","id":"5f742fd77aa_8","name":"011e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*dxZnBpfi45tp_8AluCMW_A.jpeg"},"text":"Figure 1. Equation to calculate a tf-idf vector for a term wᵢ,ⱼ. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":0,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":60,"end":81,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_9":{"__typename":"Paragraph","id":"5f742fd77aa_9","name":"9bbf","type":"H3","href":null,"layout":null,"metadata":null,"text":"Latent Semantic Indexing","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_10":{"__typename":"Paragraph","id":"5f742fd77aa_10","name":"f741","type":"P","href":null,"layout":null,"metadata":null,"text":"Using tf-idf as a dimensionality reduction technique identifies discriminative words for a collection of documents; however, it does not expose the inter- or intra- document statistical structure. Therefore, Latent Semantic Indexing (Deerwester et al., 1990) was introduced as a way of overcoming the deficits of tf-idf. By using singular value decomposition (SVD) to accomplish dimensionality reduction of the term-document matrix, it is possible to identify the linear subspace within the tf-idf features that captures most of the variance in a collection of documents, allowing LSI to be a generative rather than discriminative process.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":18,"end":42,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDimensionality_reduction","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":208,"end":258,"href":"https:\u002F\u002Fasistdl.onlinelibrary.wiley.com\u002Fdoi\u002Fabs\u002F10.1002\u002F%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":330,"end":364,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSingular_value_decomposition","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_11":{"__typename":"Paragraph","id":"5f742fd77aa_11","name":"7ab2","type":"P","href":null,"layout":null,"metadata":null,"text":"The SVD process can be visualized by the diagram below, where term-document matrix A is factored into a product of three matrices: U, S, and VT. The rows of U represent document vectors in terms of topics and the rows of V represent term vectors in terms of topics.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":142,"end":143,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":83,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":131,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":134,"end":135,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":141,"end":142,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":157,"end":158,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":221,"end":222,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*VyeNINhenigBdJt4ArmLIA.jpeg":{"__typename":"ImageMetadata","id":"1*VyeNINhenigBdJt4ArmLIA.jpeg","originalHeight":940,"originalWidth":2052,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_12":{"__typename":"Paragraph","id":"5f742fd77aa_12","name":"4b0a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*VyeNINhenigBdJt4ArmLIA.jpeg"},"text":"Figure 2. Singular value decomposition of term-document matrix A, where U is an orthogonal document-topic matrix, S is a diagonal matrix of singular values of A, and V is the transpose of the orthogonal and represents a term-topic matrix. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":63,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":72,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":114,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":159,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":166,"end":167,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_13":{"__typename":"Paragraph","id":"5f742fd77aa_13","name":"a186","type":"P","href":null,"layout":null,"metadata":null,"text":"LSI, however, requires a special type of SVD, specifically a low-rank approximation of the matrix A is required for efficiency, therefore truncation based on the Eckart-Young Theorem (specifically the proof for the Frobenius norm) is used to build an approximate matrix. In application, a value k is the hyperparameter used to represent the number of topics desired, and truncated SVD is used to select only the k columns of U and V. The selected singular values from the diagonal S matrix are represented by “σₗ…σₖ”, where σₖ ≥ 0 and σₗ has the highest importance. Truncated SVD of matrix A, can be visualized as follows:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":61,"end":83,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLow-rank_approximation","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":162,"end":182,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLow-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_Frobenius_norm)","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":371,"end":384,"href":"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.decomposition.TruncatedSVD.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":295,"end":296,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":412,"end":413,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":425,"end":426,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":431,"end":432,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":481,"end":482,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":511,"end":512,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":514,"end":515,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":525,"end":527,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":536,"end":537,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg":{"__typename":"ImageMetadata","id":"1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg","originalHeight":988,"originalWidth":2996,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_14":{"__typename":"Paragraph","id":"5f742fd77aa_14","name":"da32","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*kNtvMOUrzhgBaeT9kUQ_Wg.jpeg"},"text":"Figure 3. Latent Semantic Indexing, where term-document matrix A is transformed by truncated singular value decomposition. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_15":{"__typename":"Paragraph","id":"5f742fd77aa_15","name":"ba97","type":"P","href":null,"layout":null,"metadata":null,"text":"The derived features of LSI are linear combinations of the original tf-idf features, thereby capturing some aspects of linguistic notions of synonymy and polysemy. Nonetheless, LSI unfortunately requires a very large collection of documents and terms, in addition to lacking interpretability with regards to topic content and sentiment.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":154,"end":162,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPolysemy","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_16":{"__typename":"Paragraph","id":"5f742fd77aa_16","name":"3915","type":"H3","href":null,"layout":null,"metadata":null,"text":"Probabilistic Latent Semantic Indexing","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_17":{"__typename":"Paragraph","id":"5f742fd77aa_17","name":"3cc4","type":"P","href":null,"layout":null,"metadata":null,"text":"Instead of relying solely on truncated SVD, a probabilistic model of LSI was introduced as an alternative, referred to as pLSI (Hoffman, 1999). Each word in a document is sampled from a mixture model, where the mixture components are multinomial random variables that can be viewed as representative of topics. Hence, each word is generated from a single topic, and different words within a document can be generated from different topics. Essentially, each document is reduced to a probability distribution on a fixed set of topics. This probabilistic approach can be modeled by a set of equivalent equations representing the joint probability of a document with a word, P(D,W). Furthermore, as seen in the diagram below, these equations can be derived from the truncated SVD model of LSI.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":128,"end":141,"href":"https:\u002F\u002Fwww.researchgate.net\u002Fpublication\u002F2941307_Probabilistic_Latent_Semantic_Indexing","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-9XsHgwFA1YxCQpXKJj15Q.jpeg":{"__typename":"ImageMetadata","id":"1*-9XsHgwFA1YxCQpXKJj15Q.jpeg","originalHeight":952,"originalWidth":2520,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_18":{"__typename":"Paragraph","id":"5f742fd77aa_18","name":"ece2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*-9XsHgwFA1YxCQpXKJj15Q.jpeg"},"text":"Figure 4. Pair of equivalent joint probability equations, where the generative process starting with a topic P(z) is representative of a probabilistic model of matrix A from LSI. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":111,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_19":{"__typename":"Paragraph","id":"5f742fd77aa_19","name":"7e99","type":"P","href":null,"layout":null,"metadata":null,"text":"In the set of equations above for P(D,W), the joint probability parameters are multinomial distributions that can be trained by an expectation-maximization algorithm (EM algorithm) for inference of parameter estimates which depend on unobserved, latent variables. Importantly, pLSI only applies a probabilistic treatment to topics and words, not documents. This leads to two problems, firstly the number of parameters for pLSI grows linearly with the size of the corpus, so it is prone to overfitting. Secondly, there are no parameters to model P(D), so it is not evident how probability would be assigned to a new document. These issues led to the development of the LDA model, which allowed for better generalization and will be discussed in the following section.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":131,"end":180,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExpectation%E2%80%93maximization_algorithm","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_20":{"__typename":"Paragraph","id":"5f742fd77aa_20","name":"cf4f","type":"H3","href":null,"layout":null,"metadata":null,"text":"Latent Dirichlet Allocation (LDA)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_21":{"__typename":"Paragraph","id":"5f742fd77aa_21","name":"d93d","type":"P","href":null,"layout":null,"metadata":null,"text":"LDA has roots in evolutionary biology; back in 2000 researchers developed this model for the study of population genetics. A few years later, LDA was applied to the field of machine learning by Blei et al., 2003, a group that includes the renowned Andrew Ng. LDA is a generative probabilistic model, specifically it is a three-level hierarchical Bayesian model, for a collection of discrete data (such as a text corpora). LDA can be thought of as a Bayesian version of pLSI, that overcomes the weakness of the latter and thus allows for better generalization.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":46,"end":121,"href":"https:\u002F\u002Fwww.genetics.org\u002Fcontent\u002F155\u002F2\u002F945","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":193,"end":211,"href":"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.5555\u002F944919.944937","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":333,"end":360,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBayesian_hierarchical_modeling","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_22":{"__typename":"Paragraph","id":"5f742fd77aa_22","name":"24af","type":"P","href":null,"layout":null,"metadata":null,"text":"The theoretical underpinnings of LDA rely on exploiting the concepts of exchangeability with the de Finetti theorem (1990). Exchangeability is a major simplifying assumption of text processing that allows for computationally-efficient methods. Both LSI and pLSI and based on the fundamental probability assumption described by the “bag-of-words” method whereby the order of words in a document can be ignored. This assumption of exchangeability extends to the treatment of documents, where one can assume that the specific order of documents in a corpus is not an important consideration.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":72,"end":87,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExchangeable_random_variables","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":97,"end":115,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDe_Finetti%27s_theorem","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":332,"end":344,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBag-of-words_model","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_23":{"__typename":"Paragraph","id":"5f742fd77aa_23","name":"017b","type":"P","href":null,"layout":null,"metadata":null,"text":"According to de Finetti’s theorem, exchangeable observations are conditionally independent relative to a latent variable, therefore an epistemic probability can be assigned to the latent variable. Furthermore, any collection of exchangeable random variables, also referred to as an exchangeable sequence of Bernoulli random variables, has a representation as a “mixture” distribution, specifically a mixture of sequences of independent and identically distributed (i.i.d.) Bernoulli random variables. The implication of a mixture model is the possibility of probabilistically representing the presence of sub-populations within an overall population without requiring an observed dataset to identify the sub-populations. Essentially, utilizing de Finetti’s theorem, it is possible to capture significant intra-document statistical structure via the mixture distribution.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":135,"end":156,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FProbability_interpretations#Logical,_epistemic,_and_inductive_probability","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":307,"end":333,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBernoulli_distribution","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":424,"end":472,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FIndependent_and_identically_distributed_random_variables","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":522,"end":535,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMixture_model","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":849,"end":869,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMixture_distribution","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":465,"end":471,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_24":{"__typename":"Paragraph","id":"5f742fd77aa_24","name":"005d","type":"H4","href":null,"layout":null,"metadata":null,"text":"Unpacking the LDA model","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_25":{"__typename":"Paragraph","id":"5f742fd77aa_25","name":"b40e","type":"P","href":null,"layout":null,"metadata":null,"text":"The innovation of LDA is in using Dirichlet priors for document-topic and term-topic distributions, thereby allowing for Bayesian inference over a three-level hierarchical model, the third layer being the distinguishing feature in a comparison to a simple Dirichlet multinomial clustering model. With regards to Bayesian inference, plate notation is an intuitive method of graphically representing variables that repeat; a “plate” (ie. box) is used to represent replicates, and edges denote conditional dependencies. As seen in the diagram below, the outer plate represents documents, and the inner plate represents repeated choices of topics and words within a document.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":44,"end":50,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPrior_probability","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":121,"end":139,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBayesian_inference","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":332,"end":346,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPlate_notation","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_26":{"__typename":"Paragraph","id":"5f742fd77aa_26","name":"1504","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*yHnDOAR2LBJJ6hHws00vWg.jpeg"},"text":"Figure 5. Probabilistic graphical representation of the LDA model with plate notation. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_27":{"__typename":"Paragraph","id":"5f742fd77aa_27","name":"999f","type":"P","href":null,"layout":null,"metadata":null,"text":"Dirichlet distributions allow for probability distribution sampling over a probability simplex in which all the numbers add up to 1, and these numbers represent probabilities over K distinct categories. A K-dimensional Dirichlet distribution has k-parameters and represents uncertainty as a probability distribution. The Dirichlet prior parameters 𝛂 and 𝛃 are corpus-level parameters that are sampled once in the process of generating a corpus, and parameter 𝚹ₘ is a document-level variable that is sampled once per document. Variables zₘₙ and wₘₙ are word-level variables that are sampled once for each word in each document. Lastly, 𝛗ₖ represents the word probability distribution for a topic k.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":75,"end":94,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSimplex#Probability","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":464,"end":465,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":542,"end":543,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":546,"end":547,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":550,"end":551,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":180,"end":181,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":205,"end":206,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":246,"end":247,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":463,"end":464,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":539,"end":542,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":547,"end":550,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":640,"end":641,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":699,"end":700,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_28":{"__typename":"Paragraph","id":"5f742fd77aa_28","name":"7c18","type":"H4","href":null,"layout":null,"metadata":null,"text":"Procedure and corpus probability","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_29":{"__typename":"Paragraph","id":"5f742fd77aa_29","name":"249a","type":"P","href":null,"layout":null,"metadata":null,"text":"With the LDA model, documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. This generative process for each document within a corpus can be written simply as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_30":{"__typename":"Paragraph","id":"5f742fd77aa_30","name":"dbb4","type":"P","href":null,"layout":null,"metadata":null,"text":"Draw each topic parameter 𝛃ₖ ~ Dirichlet (𝛗), for k 𝞊 {1…K}","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":28,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":52,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_31":{"__typename":"Paragraph","id":"5f742fd77aa_31","name":"475d","type":"P","href":null,"layout":null,"metadata":null,"text":"For each document:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_32":{"__typename":"Paragraph","id":"5f742fd77aa_32","name":"bde0","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Choose the topic distribution 𝜭ₘ ~ Dirichlet (𝜶)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":32,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_33":{"__typename":"Paragraph","id":"5f742fd77aa_33","name":"c150","type":"OLI","href":null,"layout":null,"metadata":null,"text":"For each of the N words wₙ :","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":16,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":24,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_34":{"__typename":"Paragraph","id":"5f742fd77aa_34","name":"6379","type":"P","href":null,"layout":null,"metadata":null,"text":"a) Choose a topic zₘₙ ~ Multinomial(𝜭ₘ)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":18,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":38,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_35":{"__typename":"Paragraph","id":"5f742fd77aa_35","name":"383f","type":"P","href":null,"layout":null,"metadata":null,"text":"b) Choose a word wₙ ~ Multinomial(𝛃ₖ)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":17,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":36,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_36":{"__typename":"Paragraph","id":"5f742fd77aa_36","name":"f76f","type":"P","href":null,"layout":null,"metadata":null,"text":"The probability of a corpus (D) is the result of taking the product of the marginal probabilities of single documents, and the marginal distribution for a single document is obtained by integrating over 𝜭 and summing over z topics.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":29,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":223,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iQzGRURGlGbLyPjkjeJUuA.jpeg":{"__typename":"ImageMetadata","id":"1*iQzGRURGlGbLyPjkjeJUuA.jpeg","originalHeight":500,"originalWidth":3244,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_37":{"__typename":"Paragraph","id":"5f742fd77aa_37","name":"f297","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*iQzGRURGlGbLyPjkjeJUuA.jpeg"},"text":"Figure 6. From the LDA model, the probability of a corpus D is modeled by the probability equation P(D|𝜶,𝛃). Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":58,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_38":{"__typename":"Paragraph","id":"5f742fd77aa_38","name":"07ed","type":"P","href":null,"layout":null,"metadata":null,"text":"Understanding the importance of Dirichlet distributions in LDA requires an understanding of Bayes’ Theorem which states that, if the prior is Dirichlet distributed (𝛂, 𝛃) and the likelihood is multinomial distributed (zₘₙ, wₘₙ), the posterior will be Dirichlet distributed and can therefore be computed. Nevertheless, the posterior distribution is intractable for exact inference, and there are several approximating inference algorithms such as Laplace approximation, Markov chain Monte Carlo (eg. Gibbs sampling) and my choice of a variational Bayes algorithm which will be discussed later in the implementation section covering inference.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":92,"end":106,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBayes%27_theorem","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":448,"end":469,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLaplace%27s_method","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":471,"end":495,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMarkov_chain_Monte_Carlo","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":501,"end":515,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGibbs_sampling","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":536,"end":563,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FVariational_Bayesian_methods","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":223,"end":225,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":228,"end":229,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":221,"end":223,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":226,"end":228,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_39":{"__typename":"Paragraph","id":"5f742fd77aa_39","name":"675e","type":"H3","href":null,"layout":null,"metadata":null,"text":"Implementing the LDA model","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_40":{"__typename":"Paragraph","id":"5f742fd77aa_40","name":"d4ce","type":"P","href":null,"layout":null,"metadata":null,"text":"A reasonably large dataset is required to build a LDA model. Minimum necessary size is dependent on the characteristics and average length of the documents. Generally, the larger the dataset the better the results, due to an increase in observations. For example, when working with news articles, the minimum number of articles required for modeling is 600; however, results improve when the dataset includes more than 1,000 articles. For tweets, the minimum size depends on the heterogeneity of the conversation, and since tweets have a short sequence length, the minimum dataset size is significantly larger than what is required for news articles. A dataset of 5,000 to 10,000 tweets is sufficient for modelling, and results improve at a slower rate compared to news articles when the dataset grows. For policy documents, which have a longer average length compared to news articles, fewer samples are required to build a model; minimum size depends on the scope of the collection.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_41":{"__typename":"Paragraph","id":"5f742fd77aa_41","name":"7ac4","type":"P","href":null,"layout":null,"metadata":null,"text":"The following four sections will cover the details necessary to successfully implement LDA for topic modeling of text data. The first section provides a description of select hyperparameters of the LDA model, including the Dirichlet priors. Secondly, it is important to consider the problem of inference; specifically, I focus on the solution of a variational Bayes algorithm. Following that, I provide an explanation of how to heuristically fine-tune the Dirichlet prior parameters with empirical experiments (ie. without relying on an EM algorithm for estimations, the method used in the introductory paper by Blei et al., 2003). Lastly, a practical introduction to a method for model evaluation that relies on a t-distributed stochastic neighbor embedding (t-SNE) visualization and perplexity scores.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":715,"end":766,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FT-distributed_stochastic_neighbor_embedding","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_42":{"__typename":"Paragraph","id":"5f742fd77aa_42","name":"9539","type":"H4","href":null,"layout":null,"metadata":null,"text":"Hyperparameters of the LDA model","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_43":{"__typename":"Paragraph","id":"5f742fd77aa_43","name":"932d","type":"P","href":null,"layout":null,"metadata":null,"text":"There are several Python libraries with LDA modules. Currently, I prefer using Sci-Kit Learn (sklearn), though Gensim is a very popular choice with a multicore option for parallelization of LDA. Regardless of the choice of package, the hyperparameters remain the same, so the following discussion will be general in nature.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":79,"end":92,"href":"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.decomposition.LatentDirichletAllocation.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":111,"end":117,"href":"https:\u002F\u002Fradimrehurek.com\u002Fgensim\u002Fmodels\u002Fldamodel.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_44":{"__typename":"Paragraph","id":"5f742fd77aa_44","name":"68ea","type":"P","href":null,"layout":null,"metadata":null,"text":"When implementing LDA it is necessary to fine-tune the hyperparameters, specifically the number of topics (k), the number of features (V, ie. fixed vocabulary size), and the 𝛂 and 𝛃 Dirichlet prior parameters. It is possible to fine-tune other parameters such as the number of iterations, the learning method (batch or online), the learning offset, perplexity tolerance, and others. For the sake of practicality, however, I will focus on the four parameters first mentioned. In my experience, working with a variety of datasets ranging from tweets (short and informal) to policy documents (long and formal), I have found that the best approach is to choose hyperparameters heuristically and then refine them with empirical experiments.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":229,"end":383,"href":"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.decomposition.LatentDirichletAllocation.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":107,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":135,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_45":{"__typename":"Paragraph","id":"5f742fd77aa_45","name":"30e9","type":"H4","href":null,"layout":null,"metadata":null,"text":"Number of topics (k)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":18,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_46":{"__typename":"Paragraph","id":"5f742fd77aa_46","name":"5853","type":"P","href":null,"layout":null,"metadata":null,"text":"Often, the most important hyperparameter is the number of topics, the choice of which depends on the characteristics and size of the dataset. For example, the larger the dataset the greater the number of topics, only if the dataset is representative of a diverse collection. However, a collection of a few thousand scientific articles on a particular subject, might not contain more topics if several thousand similar articles are added to the initial dataset.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_47":{"__typename":"Paragraph","id":"5f742fd77aa_47","name":"efed","type":"P","href":null,"layout":null,"metadata":null,"text":"The heuristic approach is to leverage knowledge of the content of the dataset to estimate a probable target, and make adjustments based on model evaluation and dimensionality reduction based visualizations. The optimal number of topics can be determined by calculating topic coherence scores over a range of topic numbers and plotting the resultant topic coherence trend. However, this is a computationally expensive task (ie. very time-intensive), therefore it is far more efficient to heuristically estimate a starting value for k and use model evaluation techniques to empirically guide adjustments.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":269,"end":285,"href":"https:\u002F\u002Fradimrehurek.com\u002Fgensim\u002Fmodels\u002Fcoherencemodel.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":326,"end":370,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fevaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":531,"end":532,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_48":{"__typename":"Paragraph","id":"5f742fd77aa_48","name":"46f7","type":"H4","href":null,"layout":null,"metadata":null,"text":"Number of features (V)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":20,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_49":{"__typename":"Paragraph","id":"5f742fd77aa_49","name":"cd8f","type":"P","href":null,"layout":null,"metadata":null,"text":"The same approach can be used for choosing the number of features, which is equivalent to setting a fixed size for the vocabulary. The greater the number of features, the longer the LDA model will take to train; however, a sufficiently-sized vocabulary is necessary to capture the most important words for clustering of topics. Generally, setting the number of features to 10,000 is a good starting point for most models. This value needs to be fine-tuned depending on the size of the dataset and the amount of diversity of the words in the collection.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_50":{"__typename":"Paragraph","id":"5f742fd77aa_50","name":"fc65","type":"P","href":null,"layout":null,"metadata":null,"text":"Importantly, increasing the number of features has diminishing returns on clustering accuracy. For example, if the total vocabulary for a dataset is 20,000 words, setting the number of features to 10,000 will definitely capture the vast majority of important words. On the other hand, assuming a total vocabulary of 100,000 words, increasing the number of features to 15 or 20,000 is enough to capture the majority of important words, since ordering the 100,000 words by tf-idf vectorization values will result in only a small percent being feature relevant.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_51":{"__typename":"Paragraph","id":"5f742fd77aa_51","name":"f6f5","type":"H4","href":null,"layout":null,"metadata":null,"text":"The 𝛂 and 𝛃 hyperparameters","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_52":{"__typename":"Paragraph","id":"5f742fd77aa_52","name":"9593","type":"P","href":null,"layout":null,"metadata":null,"text":"Most LDA models assume symmetric distribution, and the 𝛂 and 𝛃 parameters act as prior to the posterior calculation. This assumption of symmetry would mean that each topic is evenly distributed throughout a document, whereas for an asymmetric distribution (as measured by skewness) certain topics would be favoured over others. As Dirichlet prior concentration parameters, 𝛂 and 𝛃, are representative of document-topic density and topic-word density respectively.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":23,"end":45,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSymmetric_probability_distribution","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":274,"end":282,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSkewness","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_53":{"__typename":"Paragraph","id":"5f742fd77aa_53","name":"4b60","type":"P","href":null,"layout":null,"metadata":null,"text":"The 𝛂 parameter will specify prior beliefs about topic sparsity and uniformity; visualized as a matrix each row is a document and each column is a topic. With a high 𝛂 value, documents are assumed to contain more topics. Essentially, this means that each document is likely to contain a mix of many topics and not a single topic specifically. Conversely, a low 𝛂 value assumes that a document will contain a mixture of just a few or a single topic. This happens because as the value of 𝛂 decreases, sparsity increases such that, when the distribution is sampled, most values will be zero or close to zero.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":491,"end":492,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_54":{"__typename":"Paragraph","id":"5f742fd77aa_54","name":"73f4","type":"P","href":null,"layout":null,"metadata":null,"text":"Additionally, if the distributions are asymmetrical, a high 𝛂 value results in a more specific topic distribution per document. Initially, the 𝛂 parameter can be set to a real number value divided by the number of topics, and the results should reveal a sense of the sparsity and symmetry of the distribution. Therefore, the heuristic approach for choosing an 𝛂 value, is to estimate the topical sparsity of each document on average. Subsequent adjustments should be determined by model evaluation and then tested empirically.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_55":{"__typename":"Paragraph","id":"5f742fd77aa_55","name":"47aa","type":"P","href":null,"layout":null,"metadata":null,"text":"As mentioned, the 𝛃 parameter represents topic-word density, and it is a matrix where each row represents a topic and each column represents a word. The 𝛃 parameter will specify prior beliefs about word sparsity and uniformity within topics, adjusting for bias that certain topics will favour certain words. With a high 𝛃 value, topics are assumed to be made up of most words in the fixed-sized vocabulary and this results in a more specific word mixture for each topic.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_56":{"__typename":"Paragraph","id":"5f742fd77aa_56","name":"5a2a","type":"P","href":null,"layout":null,"metadata":null,"text":"Conversely, with a low 𝛃 value, a topic may contain a mixture of just a few of the words in the fixed-sized vocabulary. Furthermore, if the distribution is asymmetrical, a high 𝛃 value will result in a more specific word distribution. The topics, however, will be more similar in terms of words contained. Generally, it is sufficient to set the 𝛃 value to 0.01 which is the value commonly used when the word distribution is sparse (usually true); however, it may be necessary to adjust this parameter.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_57":{"__typename":"Paragraph","id":"5f742fd77aa_57","name":"911d","type":"H4","href":null,"layout":null,"metadata":null,"text":"Variational Bayesian Inference","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_58":{"__typename":"Paragraph","id":"5f742fd77aa_58","name":"925f","type":"P","href":null,"layout":null,"metadata":null,"text":"By definition, Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a likelihood function derived from a statistical model for the observed data. With regards to LDA, the problem is how to compute the posterior distribution of the hidden variables, given a document. Normalizing the posterior distribution by marginalizing over the hidden variables results in a function which is intractable, due to the coupling of 𝚹 and 𝛃 in summation over latent topics. This joint probability can be modeled with plate notation as seen in Figure 5, and expressed with the following equation:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":15,"end":33,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBayesian_inference","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":46,"end":67,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPosterior_probability","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":106,"end":124,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPrior_probability","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":131,"end":150,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLikelihood_function","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg":{"__typename":"ImageMetadata","id":"1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg","originalHeight":700,"originalWidth":3160,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_59":{"__typename":"Paragraph","id":"5f742fd77aa_59","name":"9085","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ZDOV4InS_AhvFeKcQUsy_Q.jpeg"},"text":"Figure 7. Joint probability equation for the posterior distribution of a LDA model. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_60":{"__typename":"Paragraph","id":"5f742fd77aa_60","name":"5001","type":"P","href":null,"layout":null,"metadata":null,"text":"In this form, it is clear that integrating over 𝚹, 𝛃 is intractable, and as mentioned earlier, this makes exact inference of the posterior distribution very difficult. One solution is to use variational inference, specifically variational Bayesian methods that allow for approximating intractable integrals arising from Bayesian inference. Variational inference uses Jensen’s inequality to obtain an adjustable lower bound on the log likelihood; therefore, variational parameters are chosen by an optimization procedure aimed at finding the tightest possible lower bound. The optimizing values of the variational parameters are found by minimizing the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior distribution. I recommend this blog post about KL divergence for an understanding of the role this dissimilarity measure plays in Bayesian inference.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":229,"end":256,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FVariational_Bayesian_methods","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":369,"end":388,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FJensen%27s_inequality","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":654,"end":686,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FKullback%E2%80%93Leibler_divergence","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":778,"end":807,"href":"https:\u002F\u002Fwww.countbayesie.com\u002Fblog\u002F2017\u002F5\u002F9\u002Fkullback-leibler-divergence-explained","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_61":{"__typename":"Paragraph","id":"5f742fd77aa_61","name":"22ce","type":"P","href":null,"layout":null,"metadata":null,"text":"A variational Bayes algorithm provides a locally-optimal exact-analytical solution to an approximation of the posterior distribution, in other words, it is an extension of the EM algorithm. The advantage of this approach over other options is the speed; therefore, I choose to use SciKit-Learn’s LDA module which by default relies on a variational Bayes algorithm for inference. A slower, but nonetheless popular alternative is to use the Java-based library MALLET, which offers an optimized version of collapsed Gibbs sampling, that can also be accessed with a Python wrapper through Gensim.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":439,"end":464,"href":"http:\u002F\u002Fmallet.cs.umass.edu\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":503,"end":527,"href":"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F1401890.1401960","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":562,"end":591,"href":"https:\u002F\u002Fradimrehurek.com\u002Fgensim\u002Fmodels\u002Fwrappers\u002Fldamallet.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_62":{"__typename":"Paragraph","id":"5f742fd77aa_62","name":"7576","type":"H4","href":null,"layout":null,"metadata":null,"text":"Fine tuning the 𝛂 and 𝛃 hyperparameters","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_63":{"__typename":"Paragraph","id":"5f742fd77aa_63","name":"e4f4","type":"P","href":null,"layout":null,"metadata":null,"text":"Based on my experience, generalized for working with any dataset, the guidelines for a heuristic approach are as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_64":{"__typename":"Paragraph","id":"5f742fd77aa_64","name":"916f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Given knowledge of the topics, is it expected that the distribution of topics in each document will be sparse, such that each document contains only a few topics? If yes, then choose an 𝛂 \u003C 1","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_65":{"__typename":"Paragraph","id":"5f742fd77aa_65","name":"1553","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Given knowledge of the total vocabulary, is it expected that the distribution of words in each topic will be sparse, such that certain topics favour certain words? If yes, then choose a 𝛃 \u003C 1","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_66":{"__typename":"Paragraph","id":"5f742fd77aa_66","name":"0dd9","type":"P","href":null,"layout":null,"metadata":null,"text":"Alternately, if knowledge of the dataset is limited or if the distribution is asymmetrical, it is possible to empirically rely on model evaluation to inform fine tuning of the 𝛂 and 𝛃 parameters. This is accomplished by calculating perplexity scores and adjusting for sparsity. The general procedure can be described by the following steps:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":234,"end":251,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FPerplexity","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_67":{"__typename":"Paragraph","id":"5f742fd77aa_67","name":"3c23","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Choose an 𝛂ₘ value from [0.05, 0.1, 0.5, 1, 5, 10, 50]","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":12,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_68":{"__typename":"Paragraph","id":"5f742fd77aa_68","name":"0851","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Choose a 𝛃ₘ value from [0.01, 0.05, 0.1, 0.5, 1, 5, 10]","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":11,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_69":{"__typename":"Paragraph","id":"5f742fd77aa_69","name":"761c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Train the LDA model with 𝛂ₘ and 𝛃ₘ, while keeping all other parameters constant","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":27,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":35,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_70":{"__typename":"Paragraph","id":"5f742fd77aa_70","name":"551a","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Calculate model perplexity score on holdout data","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_71":{"__typename":"Paragraph","id":"5f742fd77aa_71","name":"4417","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Chose (𝛂ₘ, 𝛃ₘ) pair with the minimum perplexity score","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":9,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":14,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_72":{"__typename":"Paragraph","id":"5f742fd77aa_72","name":"cc6c","type":"P","href":null,"layout":null,"metadata":null,"text":"Personally, I favour combining the heuristic approach with empirical experiments. Calculating model perplexity scores is a method of model evaluation, which will be discussed further in the following section.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_73":{"__typename":"Paragraph","id":"5f742fd77aa_73","name":"8d55","type":"H4","href":null,"layout":null,"metadata":null,"text":"Model Evaluation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_74":{"__typename":"Paragraph","id":"5f742fd77aa_74","name":"58e6","type":"P","href":null,"layout":null,"metadata":null,"text":"The challenge with many machine learning models, including the LDA model is how to interact with the high-dimensional data in a meaningful way that is interpretable for humans. Therefore, my primary method of evaluation is to use t-distributed Stochastic Neighbor Embedding (t-SNE) as a tool to visualize the high-dimensional data. This dimensionality reduction technique was introduced by Laurens van der Maaten and Geoffrey Hinton in 2008. The best resource I have found for implementing t-SNE is the personal blog of Laurens van der Maaten; the FAQ section on the t-SNE page in particular, offers valuable tips for understanding the visualization.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":230,"end":281,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FT-distributed_stochastic_neighbor_embedding","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":503,"end":542,"href":"https:\u002F\u002Flvdmaaten.github.io\u002Ftsne\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_75":{"__typename":"Paragraph","id":"5f742fd77aa_75","name":"5c85","type":"P","href":null,"layout":null,"metadata":null,"text":"Essentially, the t-SNE technique works to convert similarities between data points to joint probabilities, and then tries to minimize the KL divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. The dimensionality of the LDA model is determined by the number of features set during training (usually a minimum of 10,000); therefore, t-SNE can be used to reduce the dimensions to a 2-D embedding that offers a visualization of the clustering determined by LDA. To interpret the t-SNE, a simple visual evaluation of the clusters offers insight, as does the KL divergence score, for which the value closest to 0 is optimal. In the image below, created with the Python plotting library Bokeh and a dataset of 67,000 tweets, the differently coloured clusters represent the abstract topics, and positioning is determined by the dimensionality reduction algorithm.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pGBGt7y662YNbbwGccqvkQ.png":{"__typename":"ImageMetadata","id":"1*pGBGt7y662YNbbwGccqvkQ.png","originalHeight":874,"originalWidth":886,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:5f742fd77aa_76":{"__typename":"Paragraph","id":"5f742fd77aa_76","name":"b8b2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*pGBGt7y662YNbbwGccqvkQ.png"},"text":"Figure 8. Example t-SNE plot. Image by Author.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_77":{"__typename":"Paragraph","id":"5f742fd77aa_77","name":"1660","type":"P","href":null,"layout":null,"metadata":null,"text":"As mentioned earlier, model perplexity scores can be calculated to evaluate the effect of various hyperparameters for empirical testing. I use sklearn to calculate perplexity, and this blog post provides an overview of how to assess perplexity in language models. When building a LDA model I prefer to set the perplexity tolerance to 0.1 and I keep this value constant so as to better utilize t-SNE visualizations. It is important to note that t-SNE has a non-convex objective function, where the objective function is minimized using a gradient descent optimization that is initiated randomly. Therefore, different initializations will result in different solutions, so it is possible (and often advisable) to run t-SNE multiple times with the same data and perplexity, and then choose the visualization with the lowest KL divergence.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":143,"end":174,"href":"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.perplexity","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":180,"end":194,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fperplexity-in-language-models-87a196019a94","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_78":{"__typename":"Paragraph","id":"5f742fd77aa_78","name":"260a","type":"H3","href":null,"layout":null,"metadata":null,"text":"Limitations of LDA","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_79":{"__typename":"Paragraph","id":"5f742fd77aa_79","name":"cb21","type":"P","href":null,"layout":null,"metadata":null,"text":"Practically, an assessment of the LDA model reveals a few weaknesses, namely the necessity of a fixed k value, the inability of Dirichlet distributions in capturing correlations, the static nature does not show the evolution of topics over time, and lastly the simplifying “bag-of-words” exchangeability assumption. Of these limitations, none are sufficient to abandon this topic modeling method, but an awareness is necessary to understand the boundaries of results.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":102,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_80":{"__typename":"Paragraph","id":"5f742fd77aa_80","name":"0f41","type":"P","href":null,"layout":null,"metadata":null,"text":"In my experience, LDA can be used consistently and successfully to model text collections of news articles and policy documents, yet the results can be mixed for unconventional datasets, such as collections of tweets which are short and informal. Generally, a dataset is unsuitable for topic modeling if the length of the documents is too short, the data set is too small, or if there are too many topics within a collection (eg. book). In the past, ad hoc heuristics have been successfully employed to preprocess documents, such as aggregating tweets into longer “documents” (Hong and Davison, 2010). These measures, however, can be blind since many of the common assumptions of limitations are not theoretically justified; for example, the deficiency in handling shorter documents has not been explained by theory.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":577,"end":599,"href":"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F1964858.1964870","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_81":{"__typename":"Paragraph","id":"5f742fd77aa_81","name":"c414","type":"P","href":null,"layout":null,"metadata":null,"text":"In a paper from 2014 by Tang et al. an attempt was made to understand the limiting factors of LDA with posterior contraction analysis. It was found that Liebig’s law of the minimum is applicable to LDA, whereby the scarcest resource acts as the limiting factor. Of the four guidelines proposed, the most important one is regarding the number of documents (M); a sufficiently sized dataset is absolutely necessary. Once a viable M value is achieved, further increasing M may not significantly improve performance, unless the document length is also suitably increased. Lastly, when a large number of topics (K) is used to fit an LDA model, the statistical inference may become inescapably inefficient. This is because the convergence rate deteriorates quickly to a non-parametric rate, depending on the number of topics used to fit the LDA model. Therefore, in practice it is very important to avoid selecting an overly large k value.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":24,"end":35,"href":"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.5555\u002F3044805.3044828","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":356,"end":357,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":428,"end":429,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":468,"end":469,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":607,"end":608,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":925,"end":926,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:5f742fd77aa_82":{"__typename":"Paragraph","id":"5f742fd77aa_82","name":"0584","type":"P","href":null,"layout":null,"metadata":null,"text":"In conclusion, I believe that an awareness of the LDA model’s deficiencies along with practical guidelines for choosing datasets and hyperparameters will allow for successful implementation of this method for topic modeling. I welcome all feedback, whether questions or comments, so please feel free to connect with me on Linkedin.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":322,"end":330,"href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fhaaya-naushan-a4b5b61a5\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_4e1c48e2b02f":{"__typename":"CollectionViewerEdge","id":"collectionId:7f60cf5620c9-viewerId:lo_4e1c48e2b02f","isEditor":false,"isMuting":false},"ImageMetadata:1*cFFKn8rFH4ZndmaYeAs6iQ.png":{"__typename":"ImageMetadata","id":"1*cFFKn8rFH4ZndmaYeAs6iQ.png","originalWidth":2381,"originalHeight":743},"Tag:machine-learning":{"__typename":"Tag","id":"machine-learning","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:nlp":{"__typename":"Tag","id":"nlp","displayTitle":"NLP","normalizedTagSlug":"nlp"},"Tag:topic-modeling":{"__typename":"Tag","id":"topic-modeling","displayTitle":"Topic Modeling","normalizedTagSlug":"topic-modeling"},"Post:e7ff75290f8":{"__typename":"Post","id":"e7ff75290f8","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"a039","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:5f742fd77aa_0"},{"__ref":"Paragraph:5f742fd77aa_1"},{"__ref":"Paragraph:5f742fd77aa_2"},{"__ref":"Paragraph:5f742fd77aa_3"},{"__ref":"Paragraph:5f742fd77aa_4"},{"__ref":"Paragraph:5f742fd77aa_5"},{"__ref":"Paragraph:5f742fd77aa_6"},{"__ref":"Paragraph:5f742fd77aa_7"},{"__ref":"Paragraph:5f742fd77aa_8"},{"__ref":"Paragraph:5f742fd77aa_9"},{"__ref":"Paragraph:5f742fd77aa_10"},{"__ref":"Paragraph:5f742fd77aa_11"},{"__ref":"Paragraph:5f742fd77aa_12"},{"__ref":"Paragraph:5f742fd77aa_13"},{"__ref":"Paragraph:5f742fd77aa_14"},{"__ref":"Paragraph:5f742fd77aa_15"},{"__ref":"Paragraph:5f742fd77aa_16"},{"__ref":"Paragraph:5f742fd77aa_17"},{"__ref":"Paragraph:5f742fd77aa_18"},{"__ref":"Paragraph:5f742fd77aa_19"},{"__ref":"Paragraph:5f742fd77aa_20"},{"__ref":"Paragraph:5f742fd77aa_21"},{"__ref":"Paragraph:5f742fd77aa_22"},{"__ref":"Paragraph:5f742fd77aa_23"},{"__ref":"Paragraph:5f742fd77aa_24"},{"__ref":"Paragraph:5f742fd77aa_25"},{"__ref":"Paragraph:5f742fd77aa_26"},{"__ref":"Paragraph:5f742fd77aa_27"},{"__ref":"Paragraph:5f742fd77aa_28"},{"__ref":"Paragraph:5f742fd77aa_29"},{"__ref":"Paragraph:5f742fd77aa_30"},{"__ref":"Paragraph:5f742fd77aa_31"},{"__ref":"Paragraph:5f742fd77aa_32"},{"__ref":"Paragraph:5f742fd77aa_33"},{"__ref":"Paragraph:5f742fd77aa_34"},{"__ref":"Paragraph:5f742fd77aa_35"},{"__ref":"Paragraph:5f742fd77aa_36"},{"__ref":"Paragraph:5f742fd77aa_37"},{"__ref":"Paragraph:5f742fd77aa_38"},{"__ref":"Paragraph:5f742fd77aa_39"},{"__ref":"Paragraph:5f742fd77aa_40"},{"__ref":"Paragraph:5f742fd77aa_41"},{"__ref":"Paragraph:5f742fd77aa_42"},{"__ref":"Paragraph:5f742fd77aa_43"},{"__ref":"Paragraph:5f742fd77aa_44"},{"__ref":"Paragraph:5f742fd77aa_45"},{"__ref":"Paragraph:5f742fd77aa_46"},{"__ref":"Paragraph:5f742fd77aa_47"},{"__ref":"Paragraph:5f742fd77aa_48"},{"__ref":"Paragraph:5f742fd77aa_49"},{"__ref":"Paragraph:5f742fd77aa_50"},{"__ref":"Paragraph:5f742fd77aa_51"},{"__ref":"Paragraph:5f742fd77aa_52"},{"__ref":"Paragraph:5f742fd77aa_53"},{"__ref":"Paragraph:5f742fd77aa_54"},{"__ref":"Paragraph:5f742fd77aa_55"},{"__ref":"Paragraph:5f742fd77aa_56"},{"__ref":"Paragraph:5f742fd77aa_57"},{"__ref":"Paragraph:5f742fd77aa_58"},{"__ref":"Paragraph:5f742fd77aa_59"},{"__ref":"Paragraph:5f742fd77aa_60"},{"__ref":"Paragraph:5f742fd77aa_61"},{"__ref":"Paragraph:5f742fd77aa_62"},{"__ref":"Paragraph:5f742fd77aa_63"},{"__ref":"Paragraph:5f742fd77aa_64"},{"__ref":"Paragraph:5f742fd77aa_65"},{"__ref":"Paragraph:5f742fd77aa_66"},{"__ref":"Paragraph:5f742fd77aa_67"},{"__ref":"Paragraph:5f742fd77aa_68"},{"__ref":"Paragraph:5f742fd77aa_69"},{"__ref":"Paragraph:5f742fd77aa_70"},{"__ref":"Paragraph:5f742fd77aa_71"},{"__ref":"Paragraph:5f742fd77aa_72"},{"__ref":"Paragraph:5f742fd77aa_73"},{"__ref":"Paragraph:5f742fd77aa_74"},{"__ref":"Paragraph:5f742fd77aa_75"},{"__ref":"Paragraph:5f742fd77aa_76"},{"__ref":"Paragraph:5f742fd77aa_77"},{"__ref":"Paragraph:5f742fd77aa_78"},{"__ref":"Paragraph:5f742fd77aa_79"},{"__ref":"Paragraph:5f742fd77aa_80"},{"__ref":"Paragraph:5f742fd77aa_81"},{"__ref":"Paragraph:5f742fd77aa_82"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:68f801f1b50b"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Ftopic-modeling-with-latent-dirichlet-allocation-e7ff75290f8","primaryTopic":{"__ref":"Topic:1eca0103fff3"},"topics":[{"__typename":"Topic","slug":"artificial-intelligence"},{"__typename":"Topic","slug":"machine-learning"},{"__typename":"Topic","slug":"data-science"}],"isPublished":true,"latestPublishedVersion":"5f742fd77aa","visibility":"PUBLIC","postResponses":{"__typename":"PostResponses","count":3},"clapCount":241,"allowResponses":true,"isLimitedState":false,"title":"Topic Modeling with Latent Dirichlet Allocation","isSeries":false,"sequence":null,"uniqueSlug":"topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8","socialTitle":"","socialDek":"","noIndex":null,"canonicalUrl":"","metaDescription":"","latestPublishedAt":1607043117363,"readingTime":17.271698113207545,"previewContent":{"__typename":"PreviewContent","subtitle":"A practical exploration of the Natural Language Processing technique of Latent Dirichlet Allocation and its application to the task of…"},"previewImage":{"__ref":"ImageMetadata:1*yHnDOAR2LBJJ6hHws00vWg.jpeg"},"isShortform":false,"seoTitle":"","firstPublishedAt":1606951459018,"updatedAt":1686837938069,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:nlp"},{"__ref":"Tag:topic-modeling"}],"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":4259,"layerCake":3}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"MISS"}}</script><script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/manifest.6f47ed20.js"></script><script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/4900.53bf9e04.js"></script><script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/main.aac0440a.js"></script><script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/instrumentation.d9108df7.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/reporting.ff22a7a5.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/9120.5df29668.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/5049.d1ead72d.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/4810.988332a1.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6618.db187378.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/382.a98f5384.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/9148.3242ff58.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/9977.b539ef71.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/5025.b8a5ab3b.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/5250.fc15c18c.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6349.02c5ee3e.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/3801.7d014b43.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/2648.716eed4d.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/8594.9eac1902.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6003.d14e9f7d.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6636.82e49556.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/3735.3535ed24.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/4300.dc9e14c6.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6546.0f97e7cb.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6834.f2d3924e.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6858.454b4e14.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/2420.0330d157.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/5832.97239afc.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/8980.6c8ff2c1.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/2859.a9a624d0.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/6040.6ceb7f43.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/4391.3e417aeb.chunk.js"></script>
<script src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/PostPage.MainContent.190e2c44.chunk.js"></script><script>window.main();</script><script defer="" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon="{&quot;rayId&quot;:&quot;8a9e58ccdb3ea160&quot;,&quot;serverTiming&quot;:{&quot;name&quot;:{&quot;cfL4&quot;:true}},&quot;version&quot;:&quot;2024.7.0&quot;,&quot;token&quot;:&quot;0b5f665943484354a59c39c6833f7078&quot;}" crossorigin="anonymous"></script>
<iframe id="DZfvuPDZ" frameborder="0" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/translateSandbox(1).html" style="width: 0px; height: 0px; display: none;"></iframe><div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe title="reCAPTCHA" width="256" height="60" role="presentation" name="a-6xgfhya44n5x" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/anchor.html"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;" src="./Topic Modeling with Latent Dirichlet Allocation _ by Haaya Naushan _ Towards Data Science_files/saved_resource(1).html"></iframe></div></body></html>